{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zd1H8cDcvaHr",
    "outputId": "f39b22da-e568-4310-c94a-93de13ab5e55"
   },
   "outputs": [],
   "source": [
    "# # Google Colab specific code for mounting Google Drive\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "# # Define the directory path on your Google Drive\n",
    "# # Replace 'Your_directory' with the actual directory\n",
    "# directory = '/content/drive/My Drive/Colab Notebooks/ML4GST/'\n",
    "\n",
    "# # Now use this directory for reading and writing data\n",
    "# data_template_filename = directory + \"dataset.txt\"\n",
    "# gst_dir = directory + \"test_gst_dir\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0RJyTecyWOVa",
    "outputId": "852b7777-2a72-44cb-8d44-81a04f25f58d",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: C:\\Users\\ALAN\\Documents\\ML4GST\n"
     ]
    }
   ],
   "source": [
    "# Change the working directory to the desired path\n",
    "os.chdir(r\"C:\\Users\\ALAN\\Documents\\ML4GST\")\n",
    "\n",
    "# Verify that the working directory has been changed\n",
    "print(\"Current working directory:\", os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vDzCiN2Ru6ky",
    "outputId": "b076112b-7037-4547-e000-9d17dd4d693a"
   },
   "outputs": [],
   "source": [
    "# pip install pygsti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZLOeLti-u9tg",
    "outputId": "47bc21b0-1ba7-43cf-cbf0-7c8900b75490"
   },
   "outputs": [],
   "source": [
    "# import pygsti\n",
    "# import pygsti.algorithms.fiducialselection as fidsel\n",
    "# import pygsti.algorithms.germselection as germsel\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "\n",
    "# # Define the Pauli Transfer Matrices for the gates\n",
    "# # I = np.array([[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]])\n",
    "# # X_pi_4 = np.array([[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, np.cos(np.pi/4), -np.sin(np.pi/4)], [0, 0, np.sin(np.pi/4), np.cos(np.pi/4)]])\n",
    "# # Y_pi_2 = np.array([[1, 0, 0, 0], [0, np.cos(np.pi/2), 0, np.sin(np.pi/2)], [0, 0, 1, 0], [0, -np.sin(np.pi/2), 0, np.cos(np.pi/2)]])\n",
    "\n",
    "# # Create the explicit model\n",
    "# ideal_target_model = pygsti.models.create_explicit_model_from_expressions(\n",
    "#     [('Q0',)], ['Gi', 'Gx', 'Gy'],\n",
    "#     [\"I(Q0)\", \"X(pi/4,Q0)\", \"Y(pi/2,Q0)\"])\n",
    "\n",
    "# class MyXPi2Operator(pygsti.modelmembers.operations.DenseOperator):\n",
    "#     def __init__(self):\n",
    "#         #initialize with no noise\n",
    "#         super(MyXPi2Operator,self).__init__(np.identity(4,'d'), 'pp', \"densitymx\") # this is *super*-operator, so \"densitymx\"\n",
    "#         self.from_vector([0.0, 0.1])\n",
    "\n",
    "#     @property\n",
    "#     def num_params(self):\n",
    "#         return 2 # we have two parameters\n",
    "\n",
    "#     def to_vector(self):\n",
    "#         return np.array([self.depol_amt, self.over_rotation],'d') #our parameter vector\n",
    "\n",
    "#     def from_vector(self, v, close=False, dirty_value=True):\n",
    "#         #initialize from parameter vector v\n",
    "#         self.depol_amt = v[0]\n",
    "#         self.over_rotation = v[1]\n",
    "\n",
    "#         # print(f'depol_amt: {self.depol_amt}, over_rotation: {self.over_rotation}')\n",
    "\n",
    "#         theta = (np.pi/4 + self.over_rotation)/2\n",
    "#         a = 1.0-self.depol_amt\n",
    "#         b = a*2*np.cos(theta)*np.sin(theta)\n",
    "#         c = a*(np.sin(theta)**2 - np.cos(theta)**2)\n",
    "\n",
    "#         # print(f'a: {a}, b: {b}, c: {c}')\n",
    "\n",
    "#         # ._ptr is a member of DenseOperator and is a numpy array that is\n",
    "#         # the dense Pauli transfer matrix of this operator\n",
    "#         # Technical note: use [:,:] instead of direct assignment so id of self._ptr doesn't change\n",
    "#         self._ptr[:,:] = np.array([[1,   0,   0,   0],\n",
    "#                                   [0,   a,   0,   0],\n",
    "#                                   [0,   0,   c,  -b],\n",
    "#                                   [0,   0,   b,   c]],'d')\n",
    "#         self.dirty = dirty_value  # mark that parameter vector may have changed\n",
    "\n",
    "#     def transform(self, S):\n",
    "#         # Update self with inverse(S) * self * S (used in gauge optimization)\n",
    "#         raise NotImplementedError(\"MyXPi2Operator cannot be transformed!\")\n",
    "\n",
    "# import copy\n",
    "# target_model = copy.deepcopy(ideal_target_model)\n",
    "# target_model.operations[('Gx')] = MyXPi2Operator()\n",
    "# print('target_model: \\n', target_model)\n",
    "# print('ideal_target_model: \\n', ideal_target_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F4rvB8JHcipz",
    "outputId": "0c701123-b036-447b-af62-279e54d5acf6"
   },
   "outputs": [],
   "source": [
    "# # Automatic selection of fiducials and germs using \"laissez-faire\" method\n",
    "# prepFiducials, measFiducials = fidsel.find_fiducials(ideal_target_model)\n",
    "# germs = germsel.find_germs(ideal_target_model, seed = 1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Dtc8CpOEjL9R",
    "outputId": "8756bd8b-a691-4584-dc0b-cf94e1fa20a8"
   },
   "outputs": [],
   "source": [
    "# print(f'prepFiducials: {prepFiducials} \\n measFiducials: {measFiducials} \\n germs: {germs}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "uI80q6AIip7Z"
   },
   "outputs": [],
   "source": [
    "# # Generate a list of circuits using the long-sequence gate set tomography (LSGST) method\n",
    "# maxLengths = [2**n for n in range(5)]\n",
    "\n",
    "# listOfExperiments = pygsti.circuits.create_lsgst_circuits(\n",
    "#     target_model, prepFiducials, measFiducials, germs, maxLengths)\n",
    "\n",
    "# # Simulate the probability outcomes of these circuits\n",
    "# ds = pygsti.data.simulate_data(target_model, listOfExperiments, num_samples=1000,\n",
    "#                                             sample_error=\"binomial\", seed=1234)\n",
    "# # print(ds)\n",
    "\n",
    "# pygsti.io.write_dataset(\"Custom_1Q_XYI_dataset_abc.txt\", ds, outcome_label_order=['0','1'])\n",
    "\n",
    "# # Convert the probabilities to a DataFrame and save to a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "6XTVMibkMAOZ",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ALAN\\anaconda3\\envs\\ML4GST\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ALAN\\anaconda3\\envs\\ML4GST\\Lib\\site-packages\\tensorflow_probability\\python\\internal\\backend\\numpy\\_utils.py:48: The name tf.logging.TaskLevelStatusMessage is deprecated. Please use tf.compat.v1.logging.TaskLevelStatusMessage instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ALAN\\anaconda3\\envs\\ML4GST\\Lib\\site-packages\\tensorflow_probability\\python\\internal\\backend\\numpy\\_utils.py:48: The name tf.control_flow_v2_enabled is deprecated. Please use tf.compat.v1.control_flow_v2_enabled instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Lambda\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "# Load the sorted data directly from the modified CSV\n",
    "df_sorted = pd.read_csv('Sorted_Encoded_Padded_Probabilities.csv')\n",
    "\n",
    "def prepare_data(df_part):\n",
    "    # Extracting features and labels\n",
    "    X = df_part['Padded'].apply(lambda x: [int(xi) for xi in x.strip('[]').split()]).to_list()\n",
    "    y = df_part[['Prob1', 'Prob2']].values\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "\n",
    "    return X, y\n",
    "  \n",
    "# def map_integers_to_gates(int_sequences, gate_matrices):\n",
    "#     \"\"\"\n",
    "#     Map each integer in the sequences to its corresponding quantum gate matrix.\n",
    "    \n",
    "#     Args:\n",
    "#     - int_sequences (np.array): An array of shape (batch_size, seq_length) containing integer sequences.\n",
    "#     - gate_matrices (list): A list of 4x4 matrices representing quantum gates.\n",
    "    \n",
    "#     Returns:\n",
    "#     - np.array: An array of shape (batch_size, seq_length, 4, 4) containing the mapped sequences.\n",
    "#     \"\"\"\n",
    "#     # Initialize an empty array to store the mapped sequences\n",
    "#     mapped_sequences = np.zeros((int_sequences.shape[0], int_sequences.shape[1], 4, 4))\n",
    "    \n",
    "#     # Loop through each sequence\n",
    "#     for i in range(int_sequences.shape[0]):\n",
    "#         # Find the length of the meaningful part of the sequence (non-zero part)\n",
    "#         meaningful_length = np.max(np.nonzero(int_sequences[i])) + 1 if np.any(int_sequences[i]) else 0\n",
    "        \n",
    "#         # Map the integers to their corresponding quantum gate matrices\n",
    "#         for j in range(meaningful_length):\n",
    "#             mapped_sequences[i, j] = gate_matrices[int_sequences[i, j]]\n",
    "            \n",
    "#     return mapped_sequences\n",
    "\n",
    "# # Example usage:\n",
    "# example_sequences = np.array([[1, 2, 2, 3, 0], [3, 1, 0, 0, 0]])\n",
    "# I_gate = np.array([[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]])\n",
    "# X_pi_4_gate = np.array([[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, np.cos(np.pi/4), -np.sin(np.pi/4)], [0, 0, np.sin(np.pi/4), np.cos(np.pi/4)]])\n",
    "# Y_pi_2_gate = np.array([[1, 0, 0, 0], [0, np.cos(np.pi/2), 0, np.sin(np.pi/2)], [0, 0, 1, 0], [0, -np.sin(np.pi/2), 0, np.cos(np.pi/2)]])\n",
    "# gate_matrices = [None, X_pi_4_gate, Y_pi_2_gate, I_gate]  # The \"None\" entry will be skipped "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_integers_to_gates(int_sequences, gate_matrices):\n",
    "    \"\"\"\n",
    "    Map each integer in the sequences to its corresponding quantum gate matrix.\n",
    "    This version also maps zeros to a 4x4 zero matrix.\n",
    "    \n",
    "    Args:\n",
    "    - int_sequences (np.array): An array of shape (batch_size, seq_length) containing integer sequences.\n",
    "    - gate_matrices (list): A list of 4x4 matrices representing quantum gates.\n",
    "    \n",
    "    Returns:\n",
    "    - np.array: An array of shape (batch_size, seq_length, 4, 4) containing the mapped sequences.\n",
    "    \"\"\"\n",
    "    # Initialize an empty array to store the mapped sequences\n",
    "    mapped_sequences = np.zeros((int_sequences.shape[0], int_sequences.shape[1], 4, 4))\n",
    "    \n",
    "    # Loop through each sequence\n",
    "    for i in range(int_sequences.shape[0]):\n",
    "        # Map the integers to their corresponding quantum gate matrices\n",
    "        for j in range(int_sequences.shape[1]):\n",
    "            mapped_sequences[i, j] = gate_matrices[int_sequences[i, j]]\n",
    "            \n",
    "    return mapped_sequences\n",
    "\n",
    "# Example usage:\n",
    "example_sequences = np.array([[1, 2, 2, 3, 0], [3, 1, 0, 0, 0]])\n",
    "zero_matrix = np.zeros((4, 4))\n",
    "I_gate = np.array([[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]])\n",
    "X_pi_2_gate = np.array([[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, np.cos(np.pi/2), -np.sin(np.pi/2)], [0, 0, np.sin(np.pi/2), np.cos(np.pi/2)]])\n",
    "Y_pi_2_gate = np.array([[1, 0, 0, 0], [0, np.cos(np.pi/2), 0, np.sin(np.pi/2)], [0, 0, 1, 0], [0, -np.sin(np.pi/2), 0, np.cos(np.pi/2)]])\n",
    "gate_matrices = [zero_matrix, X_pi_2_gate, Y_pi_2_gate, I_gate]\n",
    "\n",
    "# # Run the function\n",
    "# result = map_integers_to_gates(example_sequences, gate_matrices)\n",
    "# print(\"Result shape:\", result.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X type:  <class 'numpy.ndarray'>\n",
      "y type:  <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "X, y = prepare_data(df_sorted)\n",
    "\n",
    "print('X type: ', type(X))\n",
    "print('y type: ', type(y))\n",
    "\n",
    "# Create new input data\n",
    "X_new = [X, y]\n",
    "\n",
    "# # Split the original input (X[0]) and target labels (y) into training and test sets\n",
    "# X_train_0, X_test_0, y_train, y_test = train_test_split(X_new[0], y, test_size=0.2, random_state=42)\n",
    "\n",
    "# print('X_train_0 type: ', type(X_train_0))\n",
    "# print('y_train type: ', type(y_train))\n",
    "\n",
    "X_train_0 = X\n",
    "y_train = y \n",
    "\n",
    "# Run the mapping function to convert integer sequences to gate matrices\n",
    "X_train_mapped = map_integers_to_gates(X_train_0, gate_matrices)\n",
    "# X_test_mapped = map_integers_to_gates(X_test_0, gate_matrices)\n",
    "\n",
    "# New input data after mapping\n",
    "X_train = [X_train_mapped, y_train]\n",
    "# X_test = [X_test_mapped, y_test]\n",
    "\n",
    "# # Manually combine the split y labels into the X data\n",
    "# X_train = [np.array(X_train_0), np.array(y_train)]\n",
    "# X_test = [np.array(X_test_0), np.array(y_test)]\n",
    "\n",
    "# Convert y data to numpy arrays\n",
    "# y_train = np.array(y_train)\n",
    "# y_test = np.array(y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # @tf.keras.utils.register_keras_serializable()\n",
    "# class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "#     def __init__(self, d_model, warmup_steps=4000):\n",
    "#         super(CustomSchedule, self).__init__()\n",
    "\n",
    "#         self.d_model = tf.cast(d_model, tf.float32)\n",
    "#         self.warmup_steps = warmup_steps\n",
    "\n",
    "#     def __call__(self, step):\n",
    "#         step = tf.cast(step, dtype=tf.float32)\n",
    "#         arg1 = tf.math.rsqrt(step)\n",
    "#         arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "#         return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "#     def get_config(self):\n",
    "#         return {\n",
    "#             \"d_model\": float(self.d_model), # Convert to native Python float\n",
    "#             \"warmup_steps\": self.warmup_steps\n",
    "#         }\n",
    "\n",
    "\n",
    "\n",
    "# @tf.keras.utils.register_keras_serializable()\n",
    "# class TriangularLearningRateScheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "#     def __init__(self, min_lr, max_lr, step_size, mode='triangular', gamma=0.99):\n",
    "#         super(TriangularLearningRateScheduler, self).__init__()\n",
    "        \n",
    "#         self.min_lr = min_lr\n",
    "#         self.max_lr = max_lr\n",
    "#         self.step_size = step_size\n",
    "#         self.mode = mode\n",
    "#         self.gamma = gamma\n",
    "\n",
    "#     def __call__(self, step):\n",
    "#         step = tf.cast(step, dtype=tf.float32)\n",
    "#         cycle = tf.math.floor(1 + step / (2 * self.step_size))\n",
    "#         x = tf.math.abs(step / self.step_size - 2 * cycle + 1)\n",
    "#         lr = self.min_lr + (self.max_lr - self.min_lr) * tf.math.maximum(0.0, (1 - x))\n",
    "\n",
    "#         if self.mode == 'triangular2':\n",
    "#             lr = lr * (1 / (2 ** (cycle - 1)))\n",
    "#         elif self.mode == 'exp_range':\n",
    "#             lr = lr * (self.gamma ** step)\n",
    "\n",
    "#         return lr\n",
    "\n",
    "#     def get_config(self):\n",
    "#         return {\n",
    "#             \"min_lr\": self.min_lr,\n",
    "#             \"max_lr\": self.max_lr,\n",
    "#             \"step_size\": self.step_size,\n",
    "#             \"mode\": self.mode,\n",
    "#             \"gamma\": self.gamma,\n",
    "#         }\n",
    "      \n",
    "\n",
    "\n",
    "\n",
    "# # # Function to generate positional encoding + input\n",
    "# # @tf.keras.utils.register_keras_serializable()\n",
    "# # class PositionalEncodingLayer(tf.keras.layers.Layer):\n",
    "# #     def __init__(self, seq_len, d_model, **kwargs):\n",
    "# #         super(PositionalEncodingLayer, self).__init__(**kwargs)\n",
    "# #         self.seq_len = seq_len\n",
    "# #         self.d_model = d_model\n",
    "# #         self.pos_encoding = self.get_positional_encoding(seq_len, d_model)\n",
    "        \n",
    "# #     def get_positional_encoding(self, seq_len, d_model):\n",
    "# #         angles = np.arange(seq_len)[:, np.newaxis] / np.power(10000, 2 * (np.arange(d_model)[np.newaxis, :] // 2) / d_model)\n",
    "# #         positional_encoding = np.zeros((seq_len, d_model))\n",
    "# #         positional_encoding[:, 0::2] = np.sin(angles[:, 0::2])\n",
    "# #         positional_encoding[:, 1::2] = np.cos(angles[:, 1::2])\n",
    "# #         return tf.constant(positional_encoding, dtype=tf.float32)\n",
    "        \n",
    "# #     def call(self, inputs):\n",
    "# #         pos_encoding = tf.expand_dims(self.pos_encoding, axis=0)\n",
    "# #         return inputs + pos_encoding\n",
    "\n",
    "# #     def get_config(self):\n",
    "# #         config = super(PositionalEncodingLayer, self).get_config()\n",
    "# #         config.update({\n",
    "# #             'seq_len': self.seq_len,\n",
    "# #             'd_model': self.d_model\n",
    "# #         })\n",
    "# #         return config\n",
    "\n",
    "# @tf.keras.utils.register_keras_serializable()\n",
    "# class GroupWiseTimeDistributed(tf.keras.layers.Layer):\n",
    "#     def __init__(self, layer_to_apply, **kwargs):\n",
    "#         super(GroupWiseTimeDistributed, self).__init__(**kwargs)\n",
    "#         self.layer_to_apply = layer_to_apply\n",
    "\n",
    "#     def build(self, input_shape):\n",
    "#         super(GroupWiseTimeDistributed, self).build(input_shape)\n",
    "\n",
    "#     def call(self, x):\n",
    "#         # print(\"Initial input shape:\", x.shape)  # Debugging line\n",
    "#         xs = tf.split(x, x.shape[1], axis=1)\n",
    "#         processed_xs = []\n",
    "#         for group in xs:\n",
    "#             # print(\"Shape after splitting (one group):\", group.shape)  # Debugging line\n",
    "#             group_squeezed = tf.squeeze(group, axis=1)\n",
    "#             # print(\"Shape before applying layer:\", group_squeezed.shape)  # Debugging line\n",
    "#             processed_group = self.layer_to_apply(group_squeezed)\n",
    "#             # print(\"Shape after applying layer:\", processed_group.shape)  # Debugging line\n",
    "#             processed_xs.append(processed_group)\n",
    "#         output = tf.stack(processed_xs, axis=1)\n",
    "#         # print(\"Final output shape:\", output.shape)  # Debugging line\n",
    "#         return output\n",
    "\n",
    "#     def get_config(self):\n",
    "#         config = super(GroupWiseTimeDistributed, self).get_config()\n",
    "#         config.update({\n",
    "#             'layer_to_apply_config': tf.keras.layers.serialize(self.layer_to_apply)\n",
    "#         })\n",
    "#         return config\n",
    "\n",
    "#     @classmethod\n",
    "#     def from_config(cls, config):\n",
    "#         layer_to_apply_config = config.pop('layer_to_apply_config')\n",
    "#         layer_to_apply = tf.keras.layers.deserialize(layer_to_apply_config)\n",
    "#         return cls(layer_to_apply=layer_to_apply, **config)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Function to generate positional encoding + input\n",
    "# @tf.keras.utils.register_keras_serializable()\n",
    "# class PositionalEncodingLayer(tf.keras.layers.Layer):\n",
    "#     def __init__(self, **kwargs):\n",
    "#         super(PositionalEncodingLayer, self).__init__(**kwargs)\n",
    "\n",
    "#     def build(self, input_shape):\n",
    "#         self.seq_len, self.d_model = input_shape[-2], input_shape[-1]\n",
    "#         self.pos_encoding = self.get_positional_encoding(self.seq_len, self.d_model)\n",
    "\n",
    "#     def get_positional_encoding(self, seq_len, d_model):\n",
    "#         angles = np.arange(seq_len)[:, np.newaxis] / np.power(10000, 2 * (np.arange(d_model)[np.newaxis, :] // 2) / d_model)\n",
    "#         positional_encoding = np.zeros((seq_len, d_model))\n",
    "#         positional_encoding[:, 0::2] = np.sin(angles[:, 0::2])\n",
    "#         positional_encoding[:, 1::2] = np.cos(angles[:, 1::2])\n",
    "#         return tf.constant(positional_encoding, dtype=tf.float32)\n",
    "\n",
    "#     def call(self, inputs):\n",
    "#         pos_encoding = tf.expand_dims(self.pos_encoding, 0)  # Modified line\n",
    "#         return inputs + pos_encoding\n",
    "    \n",
    "#     def get_config(self):\n",
    "#         config = super(PositionalEncodingLayer, self).get_config()\n",
    "#         return config\n",
    "\n",
    "#     @classmethod\n",
    "#     def from_config(cls, config):\n",
    "#         return cls(**config)\n",
    "\n",
    "      \n",
    "\n",
    "        \n",
    "# @tf.keras.utils.register_keras_serializable()\n",
    "# class CustomAttention(layers.Layer):\n",
    "#     def __init__(self, **kwargs):\n",
    "#         super(CustomAttention, self).__init__(**kwargs)\n",
    "\n",
    "#     def build(self, input_shape):\n",
    "#         self.W = self.add_weight(shape=(input_shape[-1], 1),\n",
    "#                                  initializer='random_normal',\n",
    "#                                  trainable=True)\n",
    "        \n",
    "#     def call(self, inputs):\n",
    "#         logits = tf.matmul(inputs, self.W)\n",
    "#         attention_weights = tf.nn.softmax(logits, axis=1)\n",
    "#         weighted_sum = tf.reduce_sum(inputs * attention_weights, axis=1)\n",
    "#         return weighted_sum\n",
    "\n",
    "#     def get_config(self):\n",
    "#         config = super().get_config()\n",
    "#         return config\n",
    "\n",
    "#     @classmethod\n",
    "#     def from_config(cls, config):\n",
    "#         return cls(**config)\n",
    "      \n",
    "    \n",
    "      \n",
    "\n",
    "# # Parameters\n",
    "# group_size = 30\n",
    "# vocab_size = 4 # deprecated, as flattened matrices are used for direct input, skipping embedding layer\n",
    "# # embedding_dim = 16\n",
    "# input_length = len(X_train[0][0])\n",
    "# embedding_dim = 16*input_length\n",
    "# prob_emebedding_dim = 16\n",
    "# num_heads = 2\n",
    "# ffn_dim = 512*group_size\n",
    "# # ffn_dim = 16\n",
    "# n_layers = 3\n",
    "\n",
    "\n",
    "# # Inputs\n",
    "# input_y_transformer = layers.Input(shape=(group_size, 2), name='input_y_transformer')\n",
    "# input_gate_transformer = layers.Input(shape=(group_size, input_length, 4, 4), name='input_gate_transformer')\n",
    "\n",
    "# # print(\"Shape of input_y_transformer:\", input_y_transformer.shape)  # Debugging line\n",
    "# # print(\"Shape of input_gate_transformer:\", input_gate_transformer.shape)  # Debugging line\n",
    "\n",
    "# # Create an instance of Flatten layer to apply to each group\n",
    "# flatten_layer = layers.Flatten()\n",
    "\n",
    "# # Create an instance of GroupWiseTimeDistributed with the Flatten layer\n",
    "# groupwise_time_distributed = GroupWiseTimeDistributed(flatten_layer)\n",
    "\n",
    "# # Process the 4x4 matrices using GroupWiseTimeDistributed\n",
    "# processed_gate = groupwise_time_distributed(input_gate_transformer)\n",
    "# # print(\"After GroupWiseTimeDistributed Flatten:\", processed_gate.shape)  # Debugging line\n",
    "\n",
    "\n",
    "# # Add positional encoding directly to processed_gate\n",
    "# positional_encoding = PositionalEncodingLayer()\n",
    "# processed_gate_with_position = positional_encoding(processed_gate)\n",
    "# # print(\"After Positional Encoding:\", processed_gate_with_position.shape)  # Debugging line\n",
    "\n",
    "# # Transformer layers\n",
    "# x = processed_gate_with_position\n",
    "# for _ in range(n_layers):\n",
    "#     attention = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embedding_dim)(x, x)\n",
    "#     # print(\"After MultiHeadAttention:\", attention.shape)  # Debugging line\n",
    "#     x = layers.LayerNormalization()(attention + x)\n",
    "#     # print(\"After LayerNormalization:\", x.shape)  # Debugging line\n",
    "#     ffn_output = layers.Dense(ffn_dim, activation='leaky_relu')(x)\n",
    "#     # print(\"After First Dense:\", ffn_output.shape)  # Debugging line\n",
    "#     ffn_output = layers.Dense(embedding_dim, activation='leaky_relu')(ffn_output)\n",
    "#     # print(\"After Second Dense:\", ffn_output.shape)  # Debugging line\n",
    "#     x = layers.LayerNormalization()(ffn_output + x)\n",
    "\n",
    "\n",
    "# # Custom Attention to convert sequence to single vector\n",
    "# final_output = CustomAttention()(x)\n",
    "# # print(\"After CustomAttention:\", final_output.shape)\n",
    "\n",
    "# # Other layers\n",
    "# prob_dist = layers.Dense(prob_emebedding_dim, activation='leaky_relu')(input_y_transformer)\n",
    "# # print(\"After First prob_dist Dense:\", prob_dist.shape)\n",
    "# prob_dist = layers.Dense(prob_emebedding_dim, activation='leaky_relu')(prob_dist)\n",
    "# # print(\"After Second prob_dist Dense:\", prob_dist.shape)\n",
    "# prob_dist = layers.Flatten()(prob_dist)\n",
    "# # print(\"After prob_dist Flatten:\", prob_dist.shape)\n",
    "# concat_transformer = layers.Concatenate()([final_output, prob_dist])\n",
    "# # print(\"After Concatenation :\", concat_transformer.shape)\n",
    "# dense_transformer = layers.Dense(group_size*128, activation='leaky_relu')(concat_transformer)\n",
    "# dense_transformer = layers.Dense(group_size*128, activation='leaky_relu')(dense_transformer)\n",
    "# output_transformer = layers.Dense(4, activation='linear')(dense_transformer)\n",
    "\n",
    "\n",
    "# # Model\n",
    "# model = Model(inputs=[input_gate_transformer, input_y_transformer], outputs=output_transformer)\n",
    "\n",
    "# # lr_schedule = TriangularLearningRateScheduler(min_lr=1e-6, max_lr=1e-5, step_size=2000)\n",
    "# lr_schedule = TriangularLearningRateScheduler(min_lr=4e-7, max_lr=4e-7, step_size=1000)\n",
    "# optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "# model.compile(optimizer=optimizer, loss='mse')\n",
    "# model.summary()\n",
    "\n",
    "\n",
    "# # lr_schedule = TriangularLearningRateScheduler(min_lr=1e-6, max_lr=1e-5, step_size=2000)\n",
    "# # optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "# # optimizer = tf.keras.optimizers.Adam(lr_schedule, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "rSc7GUbxdydS",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # @tf.keras.utils.register_keras_serializable()\n",
    "# # class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "# #     def __init__(self, d_model, warmup_steps=4000):\n",
    "# #         super(CustomSchedule, self).__init__()\n",
    "\n",
    "# #         self.d_model = tf.cast(d_model, tf.float32)\n",
    "# #         self.warmup_steps = warmup_steps\n",
    "\n",
    "# #     def __call__(self, step):\n",
    "# #         step = tf.cast(step, dtype=tf.float32)\n",
    "# #         arg1 = tf.math.rsqrt(step)\n",
    "# #         arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "# #         return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "# #     def get_config(self):\n",
    "# #         return {\n",
    "# #             \"d_model\": float(self.d_model), # Convert to native Python float\n",
    "# #             \"warmup_steps\": self.warmup_steps\n",
    "# #         }\n",
    "\n",
    "\n",
    "# input_length = len(X_train[0][0])\n",
    "\n",
    "# @tf.keras.utils.register_keras_serializable()\n",
    "# class TriangularLearningRateScheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "#     def __init__(self, min_lr, max_lr, step_size, mode='triangular', gamma=0.99):\n",
    "#         super(TriangularLearningRateScheduler, self).__init__()\n",
    "        \n",
    "#         self.min_lr = min_lr\n",
    "#         self.max_lr = max_lr\n",
    "#         self.step_size = step_size\n",
    "#         self.mode = mode\n",
    "#         self.gamma = gamma\n",
    "\n",
    "#     def __call__(self, step):\n",
    "#         step = tf.cast(step, dtype=tf.float32)\n",
    "#         cycle = tf.math.floor(1 + step / (2 * self.step_size))\n",
    "#         x = tf.math.abs(step / self.step_size - 2 * cycle + 1)\n",
    "#         lr = self.min_lr + (self.max_lr - self.min_lr) * tf.math.maximum(0.0, (1 - x))\n",
    "\n",
    "#         if self.mode == 'triangular2':\n",
    "#             lr = lr * (1 / (2 ** (cycle - 1)))\n",
    "#         elif self.mode == 'exp_range':\n",
    "#             lr = lr * (self.gamma ** step)\n",
    "\n",
    "#         return lr\n",
    "\n",
    "#     def get_config(self):\n",
    "#         return {\n",
    "#             \"min_lr\": self.min_lr,\n",
    "#             \"max_lr\": self.max_lr,\n",
    "#             \"step_size\": self.step_size,\n",
    "#             \"mode\": self.mode,\n",
    "#             \"gamma\": self.gamma,\n",
    "#         }\n",
    "      \n",
    "\n",
    "\n",
    "\n",
    "# # # Function to generate positional encoding + input\n",
    "# # @tf.keras.utils.register_keras_serializable()\n",
    "# # class PositionalEncodingLayer(tf.keras.layers.Layer):\n",
    "# #     def __init__(self, seq_len, d_model, **kwargs):\n",
    "# #         super(PositionalEncodingLayer, self).__init__(**kwargs)\n",
    "# #         self.seq_len = seq_len\n",
    "# #         self.d_model = d_model\n",
    "# #         self.pos_encoding = self.get_positional_encoding(seq_len, d_model)\n",
    "        \n",
    "# #     def get_positional_encoding(self, seq_len, d_model):\n",
    "# #         angles = np.arange(seq_len)[:, np.newaxis] / np.power(10000, 2 * (np.arange(d_model)[np.newaxis, :] // 2) / d_model)\n",
    "# #         positional_encoding = np.zeros((seq_len, d_model))\n",
    "# #         positional_encoding[:, 0::2] = np.sin(angles[:, 0::2])\n",
    "# #         positional_encoding[:, 1::2] = np.cos(angles[:, 1::2])\n",
    "# #         return tf.constant(positional_encoding, dtype=tf.float32)\n",
    "        \n",
    "# #     def call(self, inputs):\n",
    "# #         pos_encoding = tf.expand_dims(self.pos_encoding, axis=0)\n",
    "# #         return inputs + pos_encoding\n",
    "\n",
    "# #     def get_config(self):\n",
    "# #         config = super(PositionalEncodingLayer, self).get_config()\n",
    "# #         config.update({\n",
    "# #             'seq_len': self.seq_len,\n",
    "# #             'd_model': self.d_model\n",
    "# #         })\n",
    "# #         return config\n",
    "\n",
    "# @tf.keras.utils.register_keras_serializable()\n",
    "# class GroupWiseTimeDistributed(tf.keras.layers.Layer):\n",
    "#     def __init__(self, layer_to_apply, **kwargs):\n",
    "#         super(GroupWiseTimeDistributed, self).__init__(**kwargs)\n",
    "#         self.layer_to_apply = layer_to_apply\n",
    "\n",
    "#     def build(self, input_shape):\n",
    "#         super(GroupWiseTimeDistributed, self).build(input_shape)\n",
    "\n",
    "#     def call(self, x):\n",
    "#         # print(\"Initial input shape:\", x.shape)  # Debugging line\n",
    "#         xs = tf.split(x, x.shape[1], axis=1)\n",
    "#         processed_xs = []\n",
    "#         for group in xs:\n",
    "#             # print(\"Shape after splitting (one group):\", group.shape)  # Debugging line\n",
    "#             group_squeezed = tf.squeeze(group, axis=1)\n",
    "#             # print(\"Shape before applying layer:\", group_squeezed.shape)  # Debugging line\n",
    "#             processed_group = self.layer_to_apply(group_squeezed)\n",
    "#             # print(\"Shape after applying layer:\", processed_group.shape)  # Debugging line\n",
    "#             processed_xs.append(processed_group)\n",
    "#         output = tf.stack(processed_xs, axis=1)\n",
    "#         # print(\"Final output shape:\", output.shape)  # Debugging line\n",
    "#         return output\n",
    "\n",
    "#     def get_config(self):\n",
    "#         config = super(GroupWiseTimeDistributed, self).get_config()\n",
    "#         config.update({\n",
    "#             'layer_to_apply_config': tf.keras.layers.serialize(self.layer_to_apply)\n",
    "#         })\n",
    "#         return config\n",
    "\n",
    "#     @classmethod\n",
    "#     def from_config(cls, config):\n",
    "#         layer_to_apply_config = config.pop('layer_to_apply_config')\n",
    "#         layer_to_apply = tf.keras.layers.deserialize(layer_to_apply_config)\n",
    "#         return cls(layer_to_apply=layer_to_apply, **config)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Function to generate positional encoding + input\n",
    "# @tf.keras.utils.register_keras_serializable()\n",
    "# class PositionalEncodingLayer(tf.keras.layers.Layer):\n",
    "#     def __init__(self, **kwargs):\n",
    "#         super(PositionalEncodingLayer, self).__init__(**kwargs)\n",
    "\n",
    "#     def build(self, input_shape):\n",
    "#         self.seq_len, self.d_model = input_shape[-2], input_shape[-1]\n",
    "#         self.pos_encoding = self.get_positional_encoding(self.seq_len, self.d_model)\n",
    "\n",
    "#     def get_positional_encoding(self, seq_len, d_model):\n",
    "#         angles = np.arange(seq_len)[:, np.newaxis] / np.power(10000, 2 * (np.arange(d_model)[np.newaxis, :] // 2) / d_model)\n",
    "#         positional_encoding = np.zeros((seq_len, d_model))\n",
    "#         positional_encoding[:, 0::2] = np.sin(angles[:, 0::2])\n",
    "#         positional_encoding[:, 1::2] = np.cos(angles[:, 1::2])\n",
    "#         return tf.constant(positional_encoding, dtype=tf.float32)\n",
    "\n",
    "#     def call(self, inputs):\n",
    "#         pos_encoding = tf.expand_dims(self.pos_encoding, 0)  # Modified line\n",
    "#         return inputs + pos_encoding\n",
    "    \n",
    "#     def get_config(self):\n",
    "#         config = super(PositionalEncodingLayer, self).get_config()\n",
    "#         return config\n",
    "\n",
    "#     @classmethod\n",
    "#     def from_config(cls, config):\n",
    "#         return cls(**config)\n",
    "\n",
    "      \n",
    "\n",
    "        \n",
    "# @tf.keras.utils.register_keras_serializable()\n",
    "# class CustomAttention(layers.Layer):\n",
    "#     def __init__(self, **kwargs):\n",
    "#         super(CustomAttention, self).__init__(**kwargs)\n",
    "\n",
    "#     def build(self, input_shape):\n",
    "#         self.W = self.add_weight(shape=(input_shape[-1], 1),\n",
    "#                                  initializer='random_normal',\n",
    "#                                  trainable=True)\n",
    "        \n",
    "#     def call(self, inputs):\n",
    "#         logits = tf.matmul(inputs, self.W)\n",
    "#         attention_weights = tf.nn.softmax(logits, axis=1)\n",
    "#         weighted_sum = tf.reduce_sum(inputs * attention_weights, axis=1)\n",
    "#         return weighted_sum\n",
    "\n",
    "#     def get_config(self):\n",
    "#         config = super().get_config()\n",
    "#         return config\n",
    "\n",
    "#     @classmethod\n",
    "#     def from_config(cls, config):\n",
    "#         return cls(**config)\n",
    "    \n",
    "\n",
    "# # Define Advanced Custom Attention layer\n",
    "# @tf.keras.utils.register_keras_serializable()\n",
    "# class AdvancedCustomAttention(layers.Layer):\n",
    "#     def __init__(self, n_heads=4, d_model=None, **kwargs):\n",
    "#         super(AdvancedCustomAttention, self).__init__(**kwargs)\n",
    "#         self.n_heads = n_heads\n",
    "#         self.d_model = d_model\n",
    "\n",
    "#     def build(self, input_shape):\n",
    "#         if self.d_model is None:\n",
    "#             self.d_model = input_shape[-1]\n",
    "#         self.WQ = self.add_weight(shape=(self.d_model, self.d_model),\n",
    "#                                  initializer='random_normal',\n",
    "#                                  trainable=True)\n",
    "#         self.WK = self.add_weight(shape=(self.d_model, self.d_model),\n",
    "#                                  initializer='random_normal',\n",
    "#                                  trainable=True)\n",
    "#         self.WV = self.add_weight(shape=(self.d_model, self.d_model),\n",
    "#                                  initializer='random_normal',\n",
    "#                                  trainable=True)\n",
    "#         self.WO = self.add_weight(shape=(self.d_model, self.d_model),\n",
    "#                                  initializer='random_normal',\n",
    "#                                  trainable=True)\n",
    "\n",
    "#     def call(self, inputs):\n",
    "#         q = tf.matmul(inputs, self.WQ)\n",
    "#         k = tf.matmul(inputs, self.WK)\n",
    "#         v = tf.matmul(inputs, self.WV)\n",
    "        \n",
    "#         # Split heads\n",
    "#         q = self.split_heads(q, self.n_heads)\n",
    "#         k = self.split_heads(k, self.n_heads)\n",
    "#         v = self.split_heads(v, self.n_heads)\n",
    "\n",
    "#         # Scaled dot-product attention\n",
    "#         matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
    "#         d_k = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "#         scaled_attention_logits = matmul_qk / tf.math.sqrt(d_k)\n",
    "        \n",
    "#         attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
    "#         output = tf.matmul(attention_weights, v)\n",
    "        \n",
    "#         # Concatenate heads and pass through final linear layer\n",
    "#         output = self.concat_heads(output, self.n_heads)\n",
    "#         output = tf.matmul(output, self.WO)\n",
    "        \n",
    "#         # Average over the sequence dimension to collapse it\n",
    "#         output = tf.reduce_mean(output, axis=1)\n",
    "        \n",
    "#         return output\n",
    "\n",
    "#     def split_heads(self, x, n_heads):\n",
    "#         batch_size = tf.shape(x)[0]\n",
    "#         depth = self.d_model // n_heads\n",
    "#         reshaped_x = tf.reshape(x, (batch_size, -1, n_heads, depth))\n",
    "#         return tf.transpose(reshaped_x, perm=[0, 2, 1, 3])\n",
    "    \n",
    "#     def concat_heads(self, x, n_heads):\n",
    "#         batch_size = tf.shape(x)[0]\n",
    "#         depth = self.d_model // n_heads\n",
    "#         x = tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "#         return tf.reshape(x, (batch_size, -1, self.d_model))\n",
    "\n",
    "#     def get_config(self):\n",
    "#         config = super(AdvancedCustomAttention, self).get_config()\n",
    "#         config.update({\n",
    "#             'n_heads': self.n_heads,\n",
    "#             'd_model': self.d_model\n",
    "#         })\n",
    "#         return config\n",
    "\n",
    "#     @classmethod\n",
    "#     def from_config(cls, config):\n",
    "#         return cls(**config)\n",
    "    \n",
    "    \n",
    "# # Attention mechanism for prob_dist\n",
    "# @tf.keras.utils.register_keras_serializable()\n",
    "# class ProbDistAttention(layers.Layer):\n",
    "#     def __init__(self, output_dim=input_length*16, **kwargs):\n",
    "#         super(ProbDistAttention, self).__init__(**kwargs)\n",
    "#         self.output_dim = output_dim\n",
    "\n",
    "#     def build(self, input_shape):\n",
    "#         self.W = self.add_weight(shape=(input_shape[-1], self.output_dim),  # use it here\n",
    "#                                  initializer='random_normal',\n",
    "#                                  trainable=True)\n",
    "        \n",
    "#     def call(self, inputs):\n",
    "#         logits = tf.matmul(inputs, self.W)\n",
    "#         attention_weights = tf.nn.softmax(logits, axis=1)\n",
    "#         weighted_sum = tf.reduce_sum(inputs * attention_weights, axis=1)\n",
    "#         return weighted_sum\n",
    "\n",
    "#     def get_config(self):\n",
    "#         config = super().get_config()\n",
    "#         return config\n",
    "\n",
    "#     @classmethod\n",
    "#     def from_config(cls, config):\n",
    "#         return cls(**config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Parameters\n",
    "# group_size = 30\n",
    "# embedding_dim = 16 * input_length  # Modified based on input_length\n",
    "# prob_embedding_dim = 16  # Probability embedding dimension\n",
    "# num_heads = 4  # Increased from 2 to 4\n",
    "# ffn_dim = 512  \n",
    "# n_layers = 4  # Increased from 3 to 4\n",
    "# dropout_rate = 0.2  # Dropout rate for regularization\n",
    "\n",
    "\n",
    "# # Inputs\n",
    "# input_y_transformer = layers.Input(shape=(group_size, 2), name='input_y_transformer')\n",
    "# input_gate_transformer = layers.Input(shape=(group_size, input_length, 4, 4), name='input_gate_transformer')\n",
    "\n",
    "# # Flatten layer instance to apply to each group\n",
    "# flatten_layer = layers.Flatten()\n",
    "\n",
    "# # GroupWiseTimeDistributed with the Flatten layer (from your original code)\n",
    "# groupwise_time_distributed = GroupWiseTimeDistributed(flatten_layer)\n",
    "# processed_gate = groupwise_time_distributed(input_gate_transformer)\n",
    "\n",
    "# # Positional encoding (from your original code)\n",
    "# positional_encoding = PositionalEncodingLayer()\n",
    "# processed_gate_with_position = positional_encoding(processed_gate)\n",
    "\n",
    "# # Initialize the regularization (Dropout) layer\n",
    "# dropout_layer = layers.Dropout(dropout_rate)\n",
    "\n",
    "# # Transformer layers\n",
    "# x = processed_gate_with_position\n",
    "# for _ in range(n_layers):\n",
    "#     # Multi-Head Attention\n",
    "#     attention = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embedding_dim, dropout=dropout_rate)(x, x)\n",
    "    \n",
    "#     # Add & Norm (with Dropout for regularization)\n",
    "#     x = layers.LayerNormalization()(attention + dropout_layer(x))\n",
    "    \n",
    "#     # Feed-forward Network\n",
    "#     ffn_output = layers.Dense(ffn_dim, activation='gelu')(x)  # Changed activation to GELU\n",
    "#     ffn_output = layers.Dense(embedding_dim)(ffn_output)  # Removed activation to make it linear\n",
    "    \n",
    "#     # Add & Norm (with Dropout for regularization)\n",
    "#     x = layers.LayerNormalization()(ffn_output + dropout_layer(x))\n",
    "\n",
    "# # Custom Attention to convert sequence to single vector (from your original code)\n",
    "# # Modified to be more expressive (additional Dense layer for weighting)\n",
    "# attention_weights = layers.Dense(embedding_dim, activation='tanh')(x)\n",
    "# advanced_attention = AdvancedCustomAttention()(attention_weights)\n",
    "\n",
    "# # Existing code for Custom Attention and processing of prob_dist remains the same\n",
    "# prob_dist = layers.Dense(prob_embedding_dim, activation='leaky_relu')(input_y_transformer)\n",
    "# prob_dist = layers.Dense(input_length*16, activation='leaky_relu')(prob_dist)\n",
    "# # print(f\"Shape of prob_dist: {prob_dist.shape}\")\n",
    "# prob_dist_attention = ProbDistAttention()(prob_dist)\n",
    "# # print(f\"Shape of prob_dist_attention: {prob_dist_attention.shape}\")\n",
    "# # prob_dist_attention = layers.Flatten()(prob_dist_attention)\n",
    "\n",
    "# print(f\"Shape of advanced_attention: {advanced_attention.shape}\")\n",
    "# print(f\"Shape of prob_dist_attention: {prob_dist_attention.shape}\")\n",
    "\n",
    "\n",
    "# # Feature-wise multiplication of the final_output and prob_dist\n",
    "# feature_wise_multiplication = layers.Multiply()([advanced_attention , prob_dist_attention])\n",
    "# print(f\"Shape of feature_wise_multiplication: {feature_wise_multiplication.shape}\")\n",
    "\n",
    "# # Learned Combination of final_output and prob_dist\n",
    "# combination_weights = layers.Dense(input_length*16, activation='sigmoid')(feature_wise_multiplication)\n",
    "# learned_combination = layers.Multiply()([feature_wise_multiplication, combination_weights])\n",
    "\n",
    "# # Continue with the Dense layers as before\n",
    "# dense_transformer = layers.Dense(256, activation='leaky_relu')(learned_combination)\n",
    "# dense_transformer = layers.Dropout(dropout_rate)(dense_transformer)\n",
    "# dense_transformer = layers.Dense(256, activation='leaky_relu')(dense_transformer)\n",
    "# dense_transformer = layers.Dropout(dropout_rate)(dense_transformer)\n",
    "\n",
    "# depolar_error_pred = layers.Dense(256, activation='leaky_relu')(dense_transformer)\n",
    "# depolar_error_pred = layers.Dropout(dropout_rate)(depolar_error_pred)\n",
    "# over_rotation_pred = layers.Dense(256, activation='leaky_relu')(dense_transformer)\n",
    "# over_rotation_pred = layers.Dropout(dropout_rate)(over_rotation_pred)\n",
    "\n",
    "# depolar_error_pred = layers.Dense(1, activation='relu')(depolar_error_pred)\n",
    "# over_rotation_pred = layers.Dense(2, activation='tanh')(over_rotation_pred)\n",
    "\n",
    "# output_transformer = layers.Concatenate()([depolar_error_pred, over_rotation_pred])\n",
    "\n",
    "\n",
    "# # Model\n",
    "# model = Model(inputs=[input_gate_transformer, input_y_transformer], outputs=output_transformer)\n",
    "\n",
    "# # lr_schedule = TriangularLearningRateScheduler(min_lr=1e-6, max_lr=1e-5, step_size=2000)\n",
    "# # lr_schedule = TriangularLearningRateScheduler(min_lr=5e-5, max_lr=5e-5, step_size=1000)\n",
    "\n",
    "# initial_learning_rate = 1e-6\n",
    "# decay_steps = 100\n",
    "# # decay_rate = 0.95\n",
    "# decay_rate = 0.95\n",
    "\n",
    "# lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "#     initial_learning_rate, decay_steps, decay_rate, staircase=True\n",
    "# )\n",
    "\n",
    "\n",
    "# optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "# model.compile(optimizer=optimizer, loss='mse')\n",
    "# model.summary()\n",
    "\n",
    "\n",
    "# # lr_schedule = TriangularLearningRateScheduler(min_lr=1e-6, max_lr=1e-5, step_size=2000)\n",
    "# # optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "# # optimizer = tf.keras.optimizers.Adam(lr_schedule, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DebuggingLayer(layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(DebuggingLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # print(\"Shape of inputs:\", tf.shape(inputs))\n",
    "        print(\"Shape of inputs:\", inputs.shape)\n",
    "        return inputs  # Pass-through layer, does not alter the inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ALAN\\anaconda3\\envs\\ML4GST\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_y_transformer (Input  [(None, 30, 2)]              0         []                            \n",
      " Layer)                                                                                           \n",
      "                                                                                                  \n",
      " input_gate_transformer (In  [(None, 30, 19)]             0         []                            \n",
      " putLayer)                                                                                        \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 30, 16)               48        ['input_y_transformer[0][0]'] \n",
      "                                                                                                  \n",
      " embedding (Embedding)       (None, 30, 19, 8)            32        ['input_gate_transformer[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " reshape (Reshape)           (None, 30, 2, 8)             0         ['dense[0][0]']               \n",
      "                                                                                                  \n",
      " positional_encoding_layer   (None, 30, 19, 8)            0         ['embedding[0][0]']           \n",
      " (PositionalEncodingLayer)                                                                        \n",
      "                                                                                                  \n",
      " positional_encoding_layer_  (None, 30, 2, 8)             0         ['reshape[0][0]']             \n",
      " 1 (PositionalEncodingLayer                                                                       \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " multi_head_attention (Mult  (None, 30, 19, 8)            1128      ['positional_encoding_layer[0]\n",
      " iHeadAttention)                                                    [0]',                         \n",
      "                                                                     'positional_encoding_layer_1[\n",
      "                                                                    0][0]']                       \n",
      "                                                                                                  \n",
      " transformer_encoder (Trans  (None, 30, 19, 8)            39488     ['multi_head_attention[0][0]']\n",
      " formerEncoder)                                                                                   \n",
      "                                                                                                  \n",
      " flatten (Flatten)           (None, 4560)                 0         ['transformer_encoder[0][0]'] \n",
      "                                                                                                  \n",
      " dense_9 (Dense)             (None, 2048)                 9340928   ['flatten[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)         (None, 2048)                 0         ['dense_9[0][0]']             \n",
      "                                                                                                  \n",
      " dense_10 (Dense)            (None, 1024)                 2098176   ['dropout_8[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_9 (Dropout)         (None, 1024)                 0         ['dense_10[0][0]']            \n",
      "                                                                                                  \n",
      " dense_11 (Dense)            (None, 256)                  262400    ['dropout_9[0][0]']           \n",
      "                                                                                                  \n",
      " dense_14 (Dense)            (None, 256)                  262400    ['dropout_9[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_10 (Dropout)        (None, 256)                  0         ['dense_11[0][0]']            \n",
      "                                                                                                  \n",
      " dropout_11 (Dropout)        (None, 256)                  0         ['dense_14[0][0]']            \n",
      "                                                                                                  \n",
      " dense_12 (Dense)            (None, 128)                  32896     ['dropout_10[0][0]']          \n",
      "                                                                                                  \n",
      " dense_15 (Dense)            (None, 128)                  32896     ['dropout_11[0][0]']          \n",
      "                                                                                                  \n",
      " dense_13 (Dense)            (None, 2)                    258       ['dense_12[0][0]']            \n",
      "                                                                                                  \n",
      " dense_16 (Dense)            (None, 2)                    258       ['dense_15[0][0]']            \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)   (None, 4)                    0         ['dense_13[0][0]',            \n",
      "                                                                     'dense_16[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 12070908 (46.05 MB)\n",
      "Trainable params: 12070908 (46.05 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Parameters (adjustable)\n",
    "num_layers = 4  # Number of Transformer layers\n",
    "num_heads = 4  # Number of attention heads\n",
    "d_model = 8  # Feature dimension\n",
    "dff = 512  # Dimension of feed-forward network\n",
    "group_size = 30  # Example group size\n",
    "input_length = len(X_train[0][0])  # Example input length\n",
    "prob_embedding_dim = 8  # Probability embedding dimension (Unused)\n",
    "vocab_size = 4\n",
    "\n",
    "# Positional Encoding Layer (Serializable)\n",
    "@tf.keras.utils.register_keras_serializable()\n",
    "class PositionalEncodingLayer(layers.Layer):\n",
    "    def __init__(self, d_model, **kwargs):\n",
    "        super(PositionalEncodingLayer, self).__init__(**kwargs)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Calculate positional encoding for the combined group_size and input_length\n",
    "        seq_len = input_shape[1] * input_shape[2]  # group_size * input_length\n",
    "        # print(f\"input_shape[1]: {input_shape[1]}\")\n",
    "        # print(f\"input_shape[2]: {input_shape[2]}\")\n",
    "        self.pos_encoding = self.get_positional_encoding(seq_len, self.d_model)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Reshape inputs to (batch_size, group_size * input_length, d_model) for adding positional encoding\n",
    "        seq_len = tf.shape(inputs)[1] * tf.shape(inputs)[2]\n",
    "        reshaped_inputs = tf.reshape(inputs, [-1, seq_len, self.d_model])\n",
    "        # print(f\"reshaped_inputs: {reshaped_inputs.shape}\")\n",
    "        output = reshaped_inputs + self.pos_encoding\n",
    "        # print(f\"output: {output.shape}\")\n",
    "        # Reshape back to original input shape\n",
    "        return tf.reshape(output, [-1, tf.shape(inputs)[1], tf.shape(inputs)[2], self.d_model])\n",
    "\n",
    "    def get_positional_encoding(self, seq_len, d_model):\n",
    "        angles = np.arange(seq_len)[:, np.newaxis] / np.power(10000, (2 * (np.arange(d_model)[np.newaxis, :] // 2)) / np.float32(d_model))\n",
    "        sines = np.sin(angles[:, 0::2])\n",
    "        cosines = np.cos(angles[:, 1::2])\n",
    "\n",
    "        pos_encoding = np.concatenate([sines, cosines], axis=-1)\n",
    "        pos_encoding = pos_encoding[np.newaxis, ...]\n",
    "\n",
    "        return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(PositionalEncodingLayer, self).get_config()\n",
    "        config.update({\"d_model\": self.d_model})\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)\n",
    "\n",
    "\n",
    "# Transformer Encoder Layer (Serializable)\n",
    "@tf.keras.utils.register_keras_serializable()\n",
    "class TransformerEncoderLayer(models.Model):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1, **kwargs):\n",
    "        super(TransformerEncoderLayer, self).__init__(**kwargs)\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.dff = dff\n",
    "        self.rate = rate\n",
    "\n",
    "        self.attention = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.norm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.ffn = models.Sequential([\n",
    "            layers.Dense(dff, activation='gelu'),\n",
    "            layers.Dense(d_model)\n",
    "        ])\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "        self.norm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        attn_output = self.attention(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output)\n",
    "        out1 = self.norm1(inputs + attn_output)\n",
    "\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "        return self.norm2(out1 + ffn_output)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(TransformerEncoderLayer, self).get_config()\n",
    "        config.update({\n",
    "            'd_model': self.d_model,\n",
    "            'num_heads': self.num_heads,\n",
    "            'dff': self.dff,\n",
    "            'rate': self.rate\n",
    "        })\n",
    "        return config\n",
    "\n",
    "# Transformer Encoder (Serializable)\n",
    "@tf.keras.utils.register_keras_serializable()\n",
    "class TransformerEncoder(models.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, rate=0.1, **kwargs):\n",
    "        super(TransformerEncoder, self).__init__(**kwargs)\n",
    "        self.num_layers = num_layers\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.dff = dff\n",
    "        self.rate = rate\n",
    "\n",
    "        self.enc_layers = [TransformerEncoderLayer(d_model, num_heads, dff, rate)\n",
    "                           for _ in range(num_layers)]\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = inputs\n",
    "        for layer in self.enc_layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(TransformerEncoder, self).get_config()\n",
    "        config.update({\n",
    "            'num_layers': self.num_layers,\n",
    "            'd_model': self.d_model,\n",
    "            'num_heads': self.num_heads,\n",
    "            'dff': self.dff,\n",
    "            'rate': self.rate\n",
    "        })\n",
    "        return config\n",
    "\n",
    "\n",
    "# Inputs\n",
    "input_y_transformer = layers.Input(shape=(group_size, 2), name='input_y_transformer')\n",
    "input_gate_transformer = layers.Input(shape=(group_size, input_length), name='input_gate_transformer')\n",
    "\n",
    "\n",
    "# Embedding and Positional Encoding for gate sequences\n",
    "gate_embedding_layer = layers.Embedding(output_dim=d_model, input_dim=vocab_size)\n",
    "gate_embeddings = gate_embedding_layer(input_gate_transformer)\n",
    "# print(f'gate_embeddings')\n",
    "# gate_embeddings = DebuggingLayer()(gate_embeddings)\n",
    "gate_pos_encoding_layer = PositionalEncodingLayer(d_model=d_model)\n",
    "gate_pos_encoding = gate_pos_encoding_layer(gate_embeddings)\n",
    "# print(f'gate_pos_encoding')\n",
    "# gate_pos_encoding = DebuggingLayer()(gate_pos_encoding)\n",
    "\n",
    "# Embedding-like transformation for probabilities\n",
    "prob_transform_layer = layers.Dense(2 * d_model, activation='gelu')\n",
    "prob_transformed = prob_transform_layer(input_y_transformer)\n",
    "# print(f'prob_transformed')\n",
    "# prob_transformed = DebuggingLayer()(prob_transformed)\n",
    "\n",
    "# Reshape to match the shape of gate sequence embeddings\n",
    "prob_reshaped = layers.Reshape((group_size, 2, d_model))(prob_transformed)\n",
    "# print(f'prob_reshaped')\n",
    "# prob_reshaped = DebuggingLayer()(prob_reshaped)\n",
    "# print(f'before prob_pos_encoding')\n",
    "prob_pos_encoding_layer = PositionalEncodingLayer(d_model=d_model)\n",
    "prob_pos_encoding = prob_pos_encoding_layer(prob_reshaped)\n",
    "\n",
    "# Cross-Attention between Dataset 1 and Dataset 2\n",
    "cross_attention = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n",
    "cross_attention_output = cross_attention(gate_pos_encoding, prob_pos_encoding)\n",
    "\n",
    "# Applying Transformer Encoder to the output of cross-attention\n",
    "transformer_block = TransformerEncoder(num_layers, d_model, num_heads, dff)\n",
    "encoded_output = transformer_block(cross_attention_output)\n",
    "\n",
    "# Regression Prediction\n",
    "flattened = layers.Flatten()(encoded_output)\n",
    "# flattened = layers.Dropout(0.2)(flattened)\n",
    "dense_layer = layers.Dense(2048, activation='gelu')(flattened)\n",
    "dense_layer = layers.Dropout(0.2)(dense_layer)\n",
    "dense_layer = layers.Dense(1024, activation='gelu')(dense_layer)\n",
    "dense_layer = layers.Dropout(0.2)(dense_layer)\n",
    "# dense_layer = layers.Dense(256, activation='gelu')(dense_layer)\n",
    "# dense_layer = layers.Dropout(0.2)(dense_layer)\n",
    "# output = layers.Dense(3, activation='tanh')(dense_layer)\n",
    "\n",
    "depolar_error_pred = layers.Dense(256, activation='gelu')(dense_layer)\n",
    "depolar_error_pred = layers.Dropout(0.2)(depolar_error_pred)\n",
    "depolar_error_pred = layers.Dense(128, activation='gelu')(depolar_error_pred)\n",
    "depolar_error_pred = layers.Dense(2, activation='tanh')(depolar_error_pred)\n",
    "# depolar_error_pred = layers.Dense(1, activation='gelu')(depolar_error_pred)\n",
    "\n",
    "over_rotation_pred = layers.Dense(256, activation='gelu')(dense_layer)\n",
    "over_rotation_pred = layers.Dropout(0.2)(over_rotation_pred)\n",
    "over_rotation_pred = layers.Dense(128, activation='gelu')(over_rotation_pred)\n",
    "over_rotation_pred = layers.Dense(2, activation='tanh')(over_rotation_pred)\n",
    "\n",
    "output = layers.Concatenate()([depolar_error_pred, over_rotation_pred])\n",
    "\n",
    "# Full Model\n",
    "model = models.Model(inputs=[input_gate_transformer, input_y_transformer], outputs=output)\n",
    "\n",
    "# lr_schedule = TriangularLearningRateScheduler(min_lr=1e-6, max_lr=1e-5, step_size=2000)\n",
    "# lr_schedule = TriangularLearningRateScheduler(min_lr=5e-5, max_lr=5e-5, step_size=1000)\n",
    "\n",
    "initial_learning_rate = 1e-6\n",
    "decay_steps = 200\n",
    "# decay_rate = 0.95\n",
    "decay_rate = 0.95\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate, decay_steps, decay_rate, staircase=True\n",
    ")\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "# Compile the Model\n",
    "model.compile(optimizer=optimizer, loss='mse')\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def easy_PTM_depol_channel(depol_mat):\n",
    "#   PTM_depol = (1-depol_mat)*np.eye(4)\n",
    "#   PTM_depol[0,0] = 1\n",
    "#   return PTM_depol\n",
    "\n",
    "# def pauli_matrices():\n",
    "#     \"\"\"Return the Pauli matrices including identity.\"\"\"\n",
    "#     I = np.eye(2, dtype=complex)\n",
    "#     X = np.array([[0, 1], [1, 0]], dtype=complex)\n",
    "#     Y = np.array([[0, -1j], [1j, 0]], dtype=complex)\n",
    "#     Z = np.array([[1, 0], [0, -1]], dtype=complex)\n",
    "#     return [I, X, Y, Z]\n",
    "\n",
    "# def compute_ideal_ptm(unitary):\n",
    "#     \"\"\"Compute the ideal PTM from a given unitary.\"\"\"\n",
    "#     paulis = pauli_matrices()\n",
    "#     ptm_ideal = np.zeros((4, 4), dtype=complex)\n",
    "\n",
    "#     for i in range(4):\n",
    "#         for j in range(4):\n",
    "#             ptm_ideal[i, j] = 0.5 * np.trace(np.dot(paulis[i], np.dot(unitary, np.dot(paulis[j], np.conjugate(unitary.T)))))\n",
    "#     return ptm_ideal\n",
    "\n",
    "# def general_custom_gate(theta, delta, depol_amt, gate):\n",
    "#   # Parameters\n",
    "#   # theta = np.pi / 2  # Example theta (45 degrees)\n",
    "#   # delta = 0.1  # Over-rotational error in radians\n",
    "#   # depolarizing_error = 0.01  # Depolarizing error rate\n",
    "\n",
    "#   # Calculate PTM for ideal Rx(theta + delta) rotation including the over-rotational error\n",
    "#   unitary_rx_adjusted = np.cos((theta + delta) / 2) * np.eye(2) - 1j * np.sin((theta + delta) / 2) * pauli_matrices()[gate]\n",
    "#   ptm_adjusted_rx = compute_ideal_ptm(unitary_rx_adjusted)\n",
    "\n",
    "#   # Calculate combined PTM with depolarizing error\n",
    "#   ptm = np.dot(easy_PTM_depol_channel(depol_amt), ptm_adjusted_rx)\n",
    "\n",
    "#   return ptm.real\n",
    "\n",
    "# # print('ptm_adjusted_rx: \\n', np.round(ptm_adjusted_rx,5))\n",
    "# # print('final ptm: \\n', np.round(ptm,5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorflow does not support numpy array operations for backprop\n",
    "# def easy_PTM_depol_channel(depol_mat):\n",
    "#     PTM_depol = (1 - np.abs(depol_mat)) * np.eye(4)\n",
    "#     # Create a new tensor with the modified value at index [0, 0]\n",
    "#     PTM_depol = np.array(PTM_depol)\n",
    "#     PTM_depol[0, 0] = 1\n",
    "#     PTM_depol = tf.convert_to_tensor(PTM_depol, dtype=tf.complex64)\n",
    "#     return PTM_depol\n",
    "\n",
    "def easy_PTM_depol_channel(depol_mat):\n",
    "    identity = tf.eye(4, dtype=tf.float32)\n",
    "    PTM_depol = (1 - tf.math.abs(depol_mat)) * identity\n",
    "    \n",
    "    # Update the value at index [0,0] to 1\n",
    "    # We use Tensorflow operations to ensure gradient computation\n",
    "    PTM_depol = tf.tensor_scatter_nd_update(PTM_depol, [[0,0]], [1])\n",
    "    PTM_depol = tf.cast(PTM_depol, dtype=tf.complex64)\n",
    "    return PTM_depol\n",
    "\n",
    "\n",
    "\n",
    "def pauli_matrices():\n",
    "    \"\"\"Return the Pauli matrices including identity.\"\"\"\n",
    "    I = tf.eye(2, dtype=tf.complex64)\n",
    "    X = tf.constant([[0, 1], [1, 0]], dtype=tf.complex64)\n",
    "    Y_imag = tf.constant([[0, -1], [1, 0]], dtype=tf.float32)\n",
    "    Y_real = tf.constant([[0, 0], [0, 0]], dtype=tf.float32)\n",
    "    Y = tf.complex(Y_real, Y_imag)\n",
    "    # Y = tf.constant([[0, -1j], [1j, 0]], dtype=tf.complex64)\n",
    "    Z = tf.constant([[1, 0], [0, -1]], dtype=tf.complex64)\n",
    "    \n",
    "    return [I, X, Y, Z]\n",
    "\n",
    "def compute_ideal_ptm(unitary):\n",
    "    \"\"\"Compute the ideal PTM from a given unitary.\"\"\"\n",
    "    paulis = pauli_matrices()\n",
    "    ptm_ideal = tf.zeros((4, 4), dtype=tf.complex64)\n",
    "\n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            term = tf.matmul(unitary, tf.matmul(paulis[j], tf.linalg.adjoint(unitary)))\n",
    "            trace_value = 0.5 * tf.linalg.trace(tf.matmul(paulis[i], term))\n",
    "            \n",
    "            # Update ptm_ideal at position [i, j] with the calculated trace_value\n",
    "            indices = tf.constant([[i, j]])\n",
    "            ptm_ideal = tf.tensor_scatter_nd_add(ptm_ideal, indices, [trace_value])\n",
    "            \n",
    "    return ptm_ideal\n",
    "\n",
    "\n",
    "def general_custom_gate(theta, delta, depol_amt, gate):\n",
    "    # Compute real and imaginary parts as real numbers initially\n",
    "    real_part = tf.cos((theta + delta) / 2)\n",
    "    imag_part = tf.sin((theta + delta) / 2)\n",
    "    \n",
    "    # Cast them to complex numbers only when necessary\n",
    "    unitary_rx_adjusted = tf.cast(real_part, dtype=tf.complex64) * tf.eye(2, dtype=tf.complex64) - 1j * tf.cast(imag_part, dtype=tf.complex64) * pauli_matrices()[gate]\n",
    "    \n",
    "    ptm_adjusted_rx = compute_ideal_ptm(unitary_rx_adjusted)\n",
    "    ptm = tf.matmul(easy_PTM_depol_channel(depol_amt), ptm_adjusted_rx)\n",
    "    \n",
    "    return tf.math.real(ptm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 4), dtype=float32, numpy=\n",
       "array([[ 1.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        , -0.09883507,  0.        ,  0.9850541 ],\n",
       "       [ 0.        ,  0.        ,  0.99      ,  0.        ],\n",
       "       [ 0.        , -0.9850541 ,  0.        , -0.09883507]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "general_custom_gate(theta=math.pi/2, delta=0.1, depol_amt=0.01, gate=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Custom X gate\n",
    "# def custom_X(depol_amt, over_rotation):\n",
    "\n",
    "#   # print(f'depol_amt: {self.depol_amt}, over_rotation: {self.over_rotation}')\n",
    "\n",
    "#   theta = (math.pi/4 + over_rotation)/2\n",
    "#   a = 1.0-depol_amt\n",
    "#   b = a*2*tf.math.cos(theta)*tf.math.sin(theta)\n",
    "#   c = a*(tf.math.sin(theta)**2 - tf.math.cos(theta)**2)\n",
    "\n",
    "#   # print(f'a: {a}, b: {b}, c: {c}')\n",
    "\n",
    "#   # ._ptr is a member of DenseOperator and is a numpy array that is\n",
    "#   # the dense Pauli transfer matrix of this operator\n",
    "#   # Technical note: use [:,:] instead of direct assignment so id of self._ptr doesn't change\n",
    "#   custom_X_arr = tf.convert_to_tensor([[1,   0,   0,   0],\n",
    "#                             [0,   a,   0,   0],\n",
    "#                             [0,   0,   c,  -b],\n",
    "#                             [0,   0,   b,   c]], dtype=tf.float32)\n",
    "\n",
    "#   return custom_X_arr\n",
    "\n",
    "def custom_X(depol_amt, over_rotation):\n",
    "  # custom_X_arr = tf.convert_to_tensor(general_custom_gate, dtype=tf.float32)\n",
    "  return general_custom_gate(theta=math.pi/2, delta=over_rotation, depol_amt=depol_amt, gate=1)\n",
    "\n",
    "def custom_Y(depol_amt, over_rotation):\n",
    "  # custom_Y_arr = tf.convert_to_tensor(general_custom_gate, dtype=tf.float32)\n",
    "  return general_custom_gate(theta=math.pi/2, delta=over_rotation, depol_amt=depol_amt, gate=2)\n",
    "\n",
    "\n",
    "def custom_gate_set(depol_amt_X, over_rotation_X, depol_amt_Y, over_rotation_Y):\n",
    "  # Define gates in PTM form\n",
    "  \n",
    "  I = tf.constant([[1, 0, 0, 0],\n",
    "                  [0, 1, 0, 0],\n",
    "                  [0, 0, 1, 0],\n",
    "                  [0, 0, 0, 1]], dtype=tf.float32)  #Gi\n",
    "\n",
    "  # Normalized State corresponding to |0⟩ in Pauli basis\n",
    "  # state_np = np.array([1/np.sqrt(2), 0, 0, 1/np.sqrt(2)], dtype=np.float32)\n",
    "\n",
    "  X_theta = custom_X(depol_amt_X, over_rotation_X)\n",
    "  Y_theta = custom_Y(depol_amt_Y, over_rotation_Y)\n",
    "  \n",
    "  \n",
    "\n",
    "  return [X_theta, Y_theta, I]\n",
    "  \n",
    "\n",
    "  \n",
    "# Define gate application function\n",
    "def apply_gate(state, gate_set, label):\n",
    "    \n",
    "    # print(f\"label: {label}\")\n",
    "  \n",
    "    X_theta = gate_set[0]\n",
    "    Y_theta = gate_set[1]\n",
    "    I = gate_set[2]\n",
    "    \n",
    "    # print(f\"X_theta: {X_theta}\")\n",
    "    # print(f\"Y_theta: {Y_theta}\")\n",
    "    # print(f\"I: {I}\")\n",
    "    \n",
    "    if label == 1:\n",
    "        return tf.linalg.matmul(X_theta, tf.reshape(state, [-1, 1]))\n",
    "    elif label == 2:\n",
    "        return tf.linalg.matmul(Y_theta, tf.reshape(state, [-1, 1]))\n",
    "    elif label == 3:\n",
    "        return tf.linalg.matmul(I, tf.reshape(state, [-1, 1]))\n",
    "    else:\n",
    "        return state  # If label is 0, don't apply any gate  \n",
    "\n",
    "  \n",
    "# # Define gate application function\n",
    "# def apply_gate(state, depol_amt_X, over_rotation_X, depol_amt_Y, over_rotation_Y, label):\n",
    "#     # Construct arrays using NumPy\n",
    "#     # Define gates in PTM form\n",
    "#     I = tf.constant([[1, 0, 0, 0],\n",
    "#                     [0, 1, 0, 0],\n",
    "#                     [0, 0, 1, 0],\n",
    "#                     [0, 0, 0, 1]], dtype=tf.float32)  #Gi\n",
    "    \n",
    "#     # Normalized State corresponding to |0⟩ in Pauli basis\n",
    "#     # state_np = np.array([1/np.sqrt(2), 0, 0, 1/np.sqrt(2)], dtype=np.float32)\n",
    "\n",
    "#     X_theta = custom_X(depol_amt_X, over_rotation_X)\n",
    "#     Y_theta = custom_Y(depol_amt_Y, over_rotation_Y)\n",
    "    \n",
    "\n",
    "#     # X_theta = tf.convert_to_tensor([[1, 0, 0, 0], [0, tf.math.cos(theta_value), 0, tf.math.sin(theta_value)],\n",
    "#     #                        [0, 0, 1, 0], [0, -tf.math.sin(theta_value), 0, tf.math.cos(theta_value)]], dtype=tf.float32)  # Gx\n",
    "\n",
    "\n",
    "#     # print('current label: ', label)\n",
    "\n",
    "#     if label == 1:\n",
    "#         return tf.linalg.matmul(X_theta, tf.reshape(state, [-1, 1]))\n",
    "#     elif label == 2:\n",
    "#         return tf.linalg.matmul(Y_theta, tf.reshape(state, [-1, 1]))\n",
    "#     elif label == 3:\n",
    "#         return tf.linalg.matmul(I, tf.reshape(state, [-1, 1]))\n",
    "#     else:\n",
    "#         return state  # If label is 0, don't apply any gate\n",
    "\n",
    "\n",
    "# def apply_gate_sequence(single_gate_sequence):\n",
    "#     # Initialize state in Pauli basis\n",
    "#     state = tf.convert_to_tensor([1/np.sqrt(2), 0, 0, 1/np.sqrt(2)], dtype=tf.float32)\n",
    "\n",
    "#   # Apply each gate in the sequence\n",
    "#     # print('model(single_gate_sequence[tf.newaxis, :]) ->', model(single_gate_sequence[tf.newaxis, :]))\n",
    "#     depol_amt, over_rotation = tf.squeeze(model(single_gate_sequence[tf.newaxis, :])) # Predict depolar_error, over_rotation for the current gate sequence\n",
    "#     # print('theta_value: ', theta_value)\n",
    "#     # depol_amt = tf.clip_by_value(tf.squeeze(depol_amt), 0, 0.1)\n",
    "#     # over_rotation = tf.clip_by_value(tf.squeeze(over_rotation), 0, 0.1)\n",
    "#     # print(f\"depol_amt: {depol_amt}, over_rotation: {over_rotation}\")\n",
    "#     # print('squeezed theta_value: ', theta_value)\n",
    "#     for i in range(tf.shape(single_gate_sequence)[0]):\n",
    "#       if single_gate_sequence[i] == 0:\n",
    "#         break\n",
    "#       # print('tf.shape(single_gate_sequence): ', tf.shape(single_gate_sequence))\n",
    "#       # print('tf.shape(single_gate_sequence[0]): ', tf.shape(single_gate_sequence)[0])\n",
    "#       # print('single_gate_sequence[i]: ', single_gate_sequence[i])\n",
    "#       state = apply_gate(state, depol_amt, over_rotation, single_gate_sequence[i])\n",
    "#       # print('current state: ', state)\n",
    "\n",
    "#     return state\n",
    "\n",
    "# def apply_gate_sequence(single_gate_sequence, single_y_label, single_gate_matrix_sequence):\n",
    "#     # Initialize state in Pauli basis\n",
    "#     state = tf.constant([1/math.sqrt(2), 0, 0, 1/math.sqrt(2)], dtype=tf.float32)\n",
    "\n",
    "#     # print(\"Shape of single_gate_sequence:\", tf.shape(single_gate_sequence))\n",
    "#     # print(\"Shape of single_y_label:\", tf.shape(single_y_label))\n",
    "#     # print(\"Shape of model input single_gate_sequence:\", tf.shape(single_gate_sequence[tf.newaxis, :]))\n",
    "#     # print(\"Shape of model input single_gate_sequence:\", tf.shape(single_y_label[tf.newaxis, :]))\n",
    "\n",
    "\n",
    "#     # Apply each gate in the sequence\n",
    "#     depol_amt_X, over_rotation_X, depol_amt_Y, over_rotation_Y  = tf.squeeze(model([single_gate_matrix_sequence[tf.newaxis, :], single_y_label[tf.newaxis, :]])) # Predict depolar_error, over_rotation for the current gate sequence\n",
    "#     reconstructed_gate_set = custom_gate_set(depol_amt_X, over_rotation_X, depol_amt_Y, over_rotation_Y)\n",
    "#     for i in range(tf.shape(single_gate_sequence)[0]):\n",
    "#         # print(f'single_gate_sequence{i}: {single_gate_sequence[i]}')\n",
    "#         if single_gate_sequence[i] == 0:\n",
    "#             break\n",
    "#         state = apply_gate(state, reconstructed_gate_set, single_gate_sequence[i])\n",
    "\n",
    "#     return state\n",
    "\n",
    "def apply_gate_sequence(grouped_gate_sequence, grouped_y_label, grouped_gate_matrix_sequence, grouped_model_output):\n",
    "    # Initialize a list to collect final states for each data point in the group\n",
    "    grouped_final_states = []\n",
    "    \n",
    "#     print(\"Debug in apply_gate_sequence:\")\n",
    "#     print(\"Shape of grouped_gate_sequence:\", tf.shape(grouped_gate_sequence))\n",
    "    \n",
    "#     print(\"Shape of grouped_gate_sequence:\", tf.shape(grouped_gate_sequence))\n",
    "    \n",
    "    # Iterate through each data point within the group\n",
    "    for i in range(tf.shape(grouped_gate_sequence)[0]):\n",
    "        # print(f\"current gate sequence number: {i}\")\n",
    "        single_gate_sequence = tf.gather(grouped_gate_sequence, i, axis=0)\n",
    "        \n",
    "        # Debug prints\n",
    "        # print(\"Before tf.gather:\")\n",
    "        # print(\"Shape of grouped_gate_sequence:\", tf.shape(grouped_gate_sequence))\n",
    "        # print(\"After tf.gather:\")\n",
    "        # print(\"Shape of single_gate_sequence:\", tf.shape(single_gate_sequence))\n",
    "        \n",
    "        single_y_label = tf.gather(grouped_y_label, i, axis=0)\n",
    "        single_gate_matrix_sequence = tf.gather(grouped_gate_matrix_sequence, i, axis=0)\n",
    "        \n",
    "        # Initialize state in Pauli basis\n",
    "        state = tf.constant([1/math.sqrt(2), 0, 0, 1/math.sqrt(2)], dtype=tf.float32)\n",
    "        \n",
    "        # Extract the model output corresponding to this data point\n",
    "        depol_amt_X, depol_amt_Y, over_rotation_X, over_rotation_Y = grouped_model_output\n",
    "        \n",
    "        # Single depolarizaing error prediction\n",
    "#         depol_amt_XY, over_rotation_X, over_rotation_Y = grouped_model_output\n",
    "  \n",
    "#         depol_amt_X = depol_amt_XY\n",
    "#         depol_amt_Y = depol_amt_XY\n",
    "        \n",
    "        # Debugging purpose, depolarizing noise are fixed to grounth truth values to test the predictability of ONLY over_rotations\n",
    "        # depol_amt_X = 0.01\n",
    "        # depol_amt_Y = 0.01\n",
    "        \n",
    "        # Debugging purpose, over_rotations are fixed to grounth truth values to test the predictability of ONLY depolarizing noise\n",
    "        # over_rotation_X = 0.1\n",
    "        # over_rotation_Y = 0.15\n",
    "        \n",
    "        \n",
    "        # print(f'depol_amt_X: {depol_amt_X}, over_rotation_X: {over_rotation_X}, depol_amt_Y: {depol_amt_Y}, over_rotation_Y: {over_rotation_Y}')\n",
    "\n",
    "        \n",
    "        # Reconstruct the gate set using the predicted depolarization and over-rotation\n",
    "        reconstructed_gate_set = custom_gate_set(depol_amt_X, over_rotation_X, depol_amt_Y, over_rotation_Y)\n",
    "        \n",
    "        \n",
    "\n",
    "        # Apply each gate in the sequence\n",
    "        debug_label_arr = []\n",
    "        for j in range(tf.shape(single_gate_sequence)[0]):\n",
    "            debug_label_arr.append(single_gate_sequence[j])\n",
    "            if single_gate_sequence[j] == 0:\n",
    "                # print(f\"current gate sequence: {debug_label_arr}\")\n",
    "                # print(f\"length of current gate sequence: {len(debug_label_arr)}\")\n",
    "                break\n",
    "            state = apply_gate(state, reconstructed_gate_set, single_gate_sequence[j])\n",
    "\n",
    "        # Append the final state for this data point to the list of final states\n",
    "        grouped_final_states.append(state)\n",
    "        # print(f\"Shape of current final state: {tf.shape(state)}\")\n",
    "\n",
    "    # Convert the list of final states into a tensor\n",
    "    grouped_final_states = tf.stack(grouped_final_states)\n",
    "    # print(f\"Shape of current grouped final state: {tf.shape(grouped_final_states)}\")\n",
    "    \n",
    "    return grouped_final_states\n",
    "\n",
    "\n",
    "\n",
    "# def compute_probabilities(ptm_vector):\n",
    "#     # PTM representations for |0> and |1> states\n",
    "#     ptm_0 = tf.constant([1, 0, 0, 1], dtype=tf.float32)\n",
    "#     ptm_1 = tf.constant([1, 0, 0, -1], dtype=tf.float32)\n",
    "#     # ptm_0 = tf.convert_to_tensor([1, 0, 0, 1], dtype=tf.float32)\n",
    "#     # ptm_1 = tf.convert_to_tensor([1, 0, 0, -1], dtype=tf.float32)\n",
    "\n",
    "#     # Normalize the vectors\n",
    "#     ptm_vector = tf.squeeze(tf.linalg.l2_normalize(ptm_vector))\n",
    "#     ptm_0 = tf.linalg.l2_normalize(ptm_0)\n",
    "#     ptm_1 = tf.linalg.l2_normalize(ptm_1)\n",
    "\n",
    "#     # Compute dot products\n",
    "#     prob_0 = tf.tensordot(ptm_vector, ptm_0, axes=1)\n",
    "#     prob_1 = tf.tensordot(ptm_vector, ptm_1, axes=1)\n",
    "\n",
    "#     return tf.stack([prob_0, prob_1])\n",
    "\n",
    "def compute_probabilities(grouped_ptm_vector):\n",
    "    # print(\"Debug in compute_probabilities:\")\n",
    "    \n",
    "    # Remove singleton dimensions if any (like the last '1' in [10, 4, 1])\n",
    "    grouped_ptm_vector = tf.squeeze(grouped_ptm_vector, axis=-1)\n",
    "    \n",
    "    # print(f\"Shape of grouped_ptm_vector: {tf.shape(grouped_ptm_vector)}\")\n",
    "    \n",
    "    # PTM representations for |0> and |1> states\n",
    "    # ptm_0 = tf.constant([[1, 0, 0, 1]], dtype=tf.float32)\n",
    "    # ptm_1 = tf.constant([[1, 0, 0, -1]], dtype=tf.float32)\n",
    "    \n",
    "    ptm_0 = tf.constant([1/math.sqrt(2), 0, 0, 1/math.sqrt(2)], dtype=tf.float32)\n",
    "    ptm_1 = tf.constant([1/math.sqrt(2), 0, 0, -1/math.sqrt(2)], dtype=tf.float32)\n",
    "    \n",
    "    \n",
    "    # print(f\"Shape of ptm_0: {tf.shape(ptm_0)}\")\n",
    "    # print(f\"Shape of ptm_1: {tf.shape(ptm_1)}\")\n",
    "\n",
    "    # Normalize the vectors\n",
    "    # grouped_ptm_vector = tf.linalg.l2_normalize(grouped_ptm_vector, axis=-1)\n",
    "    # ptm_0 = tf.linalg.l2_normalize(ptm_0)\n",
    "    # ptm_1 = tf.linalg.l2_normalize(ptm_1)\n",
    "    \n",
    "    \n",
    "    # print(f\"Shape of normalized grouped_ptm_vector: {tf.shape(grouped_ptm_vector)}\")\n",
    "    # print(f\"Shape of normalized ptm_0: {tf.shape(ptm_0)}\")\n",
    "    # print(f\"Shape of normalized ptm_1: {tf.shape(ptm_1)}\")\n",
    "\n",
    "    # Compute dot products in a batched manner using broadcasting\n",
    "    prob_0 = tf.reduce_sum(grouped_ptm_vector * ptm_0, axis=-1)\n",
    "    prob_1 = tf.reduce_sum(grouped_ptm_vector * ptm_1, axis=-1)\n",
    "\n",
    "    # # Perform batch dot product\n",
    "    # prob_0 = tf.tensordot(grouped_ptm_vector, ptm_0, axes=[[-1], [-1]])\n",
    "    # prob_1 = tf.tensordot(grouped_ptm_vector, ptm_1, axes=[[-1], [-1]])\n",
    "\n",
    "    \n",
    "    # print(f\"Shape of prob_0: {tf.shape(prob_0)}\")\n",
    "    # print(f\"Shape of prob_1: {tf.shape(prob_1)}\")\n",
    "    \n",
    "    final_probabilities = tf.stack([prob_0, prob_1], axis=-1)\n",
    "    # print(f\"Shape of final_probabilities: {tf.shape(final_probabilities)}\")\n",
    "    \n",
    "    return final_probabilities\n",
    "\n",
    "\n",
    "\n",
    "# Define loss function\n",
    "loss_fn = MeanSquaredError()\n",
    "  \n",
    "\n",
    "# def train_step(X, y, X_ms):\n",
    "#     with tf.GradientTape() as tape:\n",
    "#         # Predict depolarization and over-rotation for the current gate sequence\n",
    "#         grouped_model_output = model([X_ms[0], X[1]])\n",
    "\n",
    "#         # Process the entire group at once\n",
    "#         print(\"The following are the prints from train_step\")\n",
    "#         print(\"Shape of grouped_gate_sequence: \", tf.shape(X[0]))\n",
    "#         print(\"Shape of grouped_y_label: \", tf.shape(X[1]))\n",
    "#         print(\"Shape of grouped_gate_matrix_sequence: \", tf.shape(X_ms[0]))\n",
    "#         print(\"Shape of grouped_model_output: \", tf.shape(grouped_model_output))     \n",
    "#         batched_final_states = apply_gate_sequence(X[0], X[1], X_ms[0], grouped_model_output)\n",
    "#         batched_probabilities = compute_probabilities(batched_final_states)\n",
    "        \n",
    "#         loss = loss_fn(y, batched_probabilities)\n",
    "\n",
    "#     grads = tape.gradient(loss, model.trainable_weights)\n",
    "#     optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "#     return loss\n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "# def validate_step(X, y, X_ms, print_results = False):\n",
    "#     batched_final_states = []\n",
    "#     batched_probabilities = []\n",
    "#     batched_values = []\n",
    "#     batched_sum_depol_amt_X = 0\n",
    "#     batched_sum_over_rotation_X = 0\n",
    "#     batched_sum_depol_amt_Y = 0\n",
    "#     batched_sum_over_rotation_Y = 0\n",
    "\n",
    "#     for i in range(tf.shape(X[0])[0]):\n",
    "#         single_sequence = tf.gather(X[0], i, axis=0)\n",
    "#         single_mat_sequence = tf.gather(X_ms[0], i, axis=0)\n",
    "#         single_y_label = tf.gather(X[1], i, axis=0)\n",
    "#         final_state = apply_gate_sequence(single_sequence, single_y_label, single_mat_sequence)\n",
    "#         # print(f'final_state_validating: {final_state}')\n",
    "#         probabilities = compute_probabilities(final_state)\n",
    "#         batched_final_states.append(final_state)\n",
    "#         batched_probabilities.append(probabilities)\n",
    "#         if print_results == True:\n",
    "#             depol_amt_X, over_rotation_X, depol_amt_Y, over_rotation_Y = tf.squeeze(model([single_mat_sequence[tf.newaxis, :], single_y_label[tf.newaxis, :]])) # Predict depolar_error, over_rotation for the current gate sequence\n",
    "#             batched_values.append(tf.stack([depol_amt_X, over_rotation_X, depol_amt_Y, over_rotation_Y]))\n",
    "#             batched_sum_depol_amt_X += depol_amt_X\n",
    "#             batched_sum_over_rotation_X += over_rotation_X\n",
    "#             batched_sum_depol_amt_Y += depol_amt_Y\n",
    "#             batched_sum_over_rotation_Y += over_rotation_Y\n",
    "\n",
    "#     batched_final_states = tf.stack(batched_final_states)\n",
    "#     batched_probabilities = tf.stack(batched_probabilities)\n",
    "#     loss = loss_fn(y, batched_probabilities)\n",
    "#     if print_results == True:\n",
    "#         print('\\nbatched_values: ', batched_values)\n",
    "#         print('\\nlen of batched_values: ', len(batched_values))\n",
    "#         print('\\nbatched_mean_depol_amt_X: ', batched_sum_depol_amt_X/len(batched_values))\n",
    "#         print('\\nbatched_over_rotation_X: ', batched_sum_over_rotation_X/len(batched_values))\n",
    "#         print('\\nbatched_mean_depol_amt_Y: ', batched_sum_depol_amt_Y/len(batched_values))\n",
    "#         print('\\nbatched_over_rotation_Y: ', batched_sum_over_rotation_Y/len(batched_values))\n",
    "#     return loss\n",
    "\n",
    "# def validate_step(X, y, X_ms, print_results=False):\n",
    "#     # Predict depolarization and over-rotation for the current gate sequence\n",
    "#     grouped_model_output = model([X_ms[0], X[1]])\n",
    "\n",
    "#     # Process the entire group at once\n",
    "#     batched_final_states = apply_gate_sequence(X[0], X[1], X_ms[0], grouped_model_output)\n",
    "#     batched_probabilities = compute_probabilities(batched_final_states)\n",
    "    \n",
    "#     loss = loss_fn(y, batched_probabilities)\n",
    "\n",
    "#     if print_results:\n",
    "#         num_values = tf.shape(grouped_model_output)[0]\n",
    "#         depol_amt_X, over_rotation_X, depol_amt_Y, over_rotation_Y = tf.split(grouped_model_output, 4, axis=1)\n",
    "#         print('\\nBatched Mean Depolarization Amount X:', tf.reduce_mean(depol_amt_X))\n",
    "#         print('\\nBatched Over Rotation X:', tf.reduce_mean(over_rotation_X))\n",
    "#         print('\\nBatched Mean Depolarization Amount Y:', tf.reduce_mean(depol_amt_Y))\n",
    "#         print('\\nBatched Over Rotation Y:', tf.reduce_mean(over_rotation_Y))\n",
    "\n",
    "#     return loss\n",
    "\n",
    "\n",
    "def train_step(X, y, X_ms):\n",
    "    train_losses = []  # Initialize a list to collect the losses for each group in the mini-batch\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        # Compute model output for the entire mini-batch here\n",
    "        full_batch_model_output = model([X_ms[0], X[1]])\n",
    "        # print(\"Shape of full_batch_model_output: \", tf.shape(full_batch_model_output))  # Debugging line\n",
    "        \n",
    "        # Loop over each group in the mini-batch\n",
    "        for i in range(tf.shape(X[0])[0]):\n",
    "            single_group_gate_sequence = tf.gather(X[0], i, axis=0)\n",
    "            single_group_y_label = tf.gather(X[1], i, axis=0)\n",
    "            single_group_gate_matrix_sequence = tf.gather(X_ms[0], i, axis=0)\n",
    "\n",
    "            # Gather the model's output for the i-th group from the full batch output\n",
    "            single_group_model_output = tf.gather(full_batch_model_output, i, axis=0)\n",
    "            # print(\"Shape of single_group_model_output:\", tf.shape(single_group_model_output))  # Debugging line\n",
    "\n",
    "            # Process a single group\n",
    "            final_states = apply_gate_sequence(single_group_gate_sequence, single_group_y_label, single_group_gate_matrix_sequence, single_group_model_output)\n",
    "            # print(\"Shape of final_states:\", tf.shape(final_states))  # Debugging line\n",
    "            probabilities = compute_probabilities(final_states)\n",
    "            # if i == 0:\n",
    "            #     print(f\"debugging computed probabilities: \\n {probabilities}\")\n",
    "            \n",
    "            # print(\"Shape of probabilities:\", tf.shape(probabilities))  # Debugging line\n",
    "            \n",
    "            # Calculate loss for this group\n",
    "            # print(\"tf.gather(y, i, axis=0):\", tf.gather(y, i, axis=0))  # Debugging line\n",
    "            loss = loss_fn(tf.gather(y, i, axis=0), probabilities)\n",
    "            train_losses.append(loss)\n",
    "            \n",
    "        # Average the losses across the mini-batch\n",
    "        average_loss = tf.reduce_mean(train_losses)\n",
    "        # print(\"average_loss: \", average_loss)\n",
    "\n",
    "    # Compute the gradients for the entire mini-batch and update model parameters\n",
    "    grads = tape.gradient(average_loss, model.trainable_weights)\n",
    "    \n",
    "    # Debugging lines to print out gradients\n",
    "    # for i, (grad, var) in enumerate(zip(grads, model.trainable_weights)):\n",
    "    #     if grad is not None:\n",
    "    #         print(f\"Gradient for {var.name}, Max: {tf.math.reduce_max(grad)}, Min: {tf.math.reduce_min(grad)}\")\n",
    "    #     else:\n",
    "    #         print(f\"Gradient for {var.name} is None\")\n",
    "    \n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "    \n",
    "    return average_loss, tf.squeeze(full_batch_model_output)\n",
    "\n",
    "\n",
    "def validate_step(X, y, X_ms, print_results=False):\n",
    "    val_losses = []  # Initialize a list to collect the losses for each group in the mini-batch\n",
    "    \n",
    "    # Initialize a list for printing and evaluating results\n",
    "    batched_values = []\n",
    "    batched_sum_depol_amt_X = 0\n",
    "    batched_sum_over_rotation_X = 0\n",
    "    batched_sum_depol_amt_Y = 0\n",
    "    batched_sum_over_rotation_Y = 0\n",
    "\n",
    "    # Compute model output for the entire mini-batch here\n",
    "    full_batch_model_output = model([X_ms[0], X[1]])\n",
    "\n",
    "    # Loop over each group in the mini-batch\n",
    "    for i in range(tf.shape(X[0])[0]):\n",
    "        single_group_gate_sequence = tf.gather(X[0], i, axis=0)\n",
    "        single_group_y_label = tf.gather(X[1], i, axis=0)\n",
    "        single_group_gate_matrix_sequence = tf.gather(X_ms[0], i, axis=0)\n",
    "\n",
    "        # Gather the model's output for the i-th group from the full batch output\n",
    "        single_group_model_output = tf.gather(full_batch_model_output, i, axis=0)\n",
    "\n",
    "        # Process a single group\n",
    "        final_states = apply_gate_sequence(single_group_gate_sequence, single_group_y_label, single_group_gate_matrix_sequence, single_group_model_output)\n",
    "        probabilities = compute_probabilities(final_states)\n",
    "        \n",
    "        if print_results == True:\n",
    "            # depol_amt_X, over_rotation_X, depol_amt_Y, over_rotation_Y = single_group_model_output\n",
    "            depol_amt_X, depol_amt_Y, over_rotation_X, over_rotation_Y = single_group_model_output\n",
    "            batched_values.append(tf.stack([depol_amt_X, over_rotation_X, depol_amt_Y, over_rotation_Y]))\n",
    "            batched_sum_depol_amt_X += depol_amt_X\n",
    "            batched_sum_over_rotation_X += over_rotation_X\n",
    "            batched_sum_depol_amt_Y += depol_amt_Y\n",
    "            batched_sum_over_rotation_Y += over_rotation_Y\n",
    "        \n",
    "\n",
    "        # Calculate loss for this group\n",
    "        loss = loss_fn(tf.gather(y, i, axis=0), probabilities)\n",
    "        val_losses.append(loss)\n",
    "\n",
    "    # Average the losses across the mini-batch\n",
    "    average_loss = tf.reduce_mean(val_losses)\n",
    "    \n",
    "    if print_results == True:\n",
    "        print('\\nbatched_values: ', batched_values)\n",
    "        print('\\nlen of batched_values: ', len(batched_values))\n",
    "        print('\\nbatched_mean_depol_amt_X: ', batched_sum_depol_amt_X/len(batched_values))\n",
    "        print('\\nbatched_over_rotation_X: ', batched_sum_over_rotation_X/len(batched_values))\n",
    "        print('\\nbatched_mean_depol_amt_Y: ', batched_sum_depol_amt_Y/len(batched_values))\n",
    "        print('\\nbatched_over_rotation_Y: ', batched_sum_over_rotation_Y/len(batched_values))\n",
    "    \n",
    "    \n",
    "    return average_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "mUZNtlhbiL7n",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Directory to save the model and weights\n",
    "model_save_dir = \"saved_models\"\n",
    "if not os.path.exists(model_save_dir):\n",
    "    os.makedirs(model_save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for i in range (0,2):\n",
    "#   print(tf.shape(tf.gather(X_train[0][0], i, axis=0)))\n",
    "#   print(tf.gather(X_train[0][0], i, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "382"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "382"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "debug_X, debug_y = prepare_data(df_sorted.iloc[:339]) # DO NOT USE panda iloc method!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "339"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(debug_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_debug_X = X_train_0\n",
    "new_debug_y = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "339"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_debug_X[:339])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# np.linalg.norm(new_debug_X-new_debug_X[:339])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Reshape function\n",
    "# def reshape_for_grouping(data, group_size):\n",
    "#     n_samples = data.shape[0]\n",
    "#     n_batches = n_samples // group_size\n",
    "#     remainder = n_samples % group_size\n",
    "#     if remainder:\n",
    "#         padding_shape = [group_size - remainder] + list(data.shape[1:])\n",
    "#         padding = np.zeros(padding_shape)\n",
    "#         data = np.concatenate([data, padding], axis=0)\n",
    "#     return data.reshape(-1, group_size, *data.shape[1:])\n",
    "\n",
    "\n",
    "def reshape_for_grouping(data, group_size):\n",
    "    \"\"\"\n",
    "    Reshape the data for grouping with repeating the last indices in case of remainder.\n",
    "    \n",
    "    Parameters:\n",
    "        data (numpy.ndarray): The data to be reshaped.\n",
    "        group_size (int): The desired group size.\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: The reshaped data.\n",
    "    \"\"\"\n",
    "    n_samples = data.shape[0]\n",
    "    n_batches = n_samples // group_size\n",
    "    remainder = n_samples % group_size\n",
    "    \n",
    "    # If the remainder exists, pad the data by repeating the last indices\n",
    "    if remainder:\n",
    "        padding_count = group_size - remainder\n",
    "        padding_indices = [-i % n_samples - 1 for i in range(padding_count)]\n",
    "        padding = data[padding_indices]\n",
    "        data = np.concatenate([data, padding], axis=0)\n",
    "        \n",
    "    return data.reshape(-1, group_size, *data.shape[1:])\n",
    "\n",
    "# group_size = 30  # Set this to your desired group size\n",
    "\n",
    "X_train_0_grouped = reshape_for_grouping(X_train_0, group_size)\n",
    "y_train_grouped = reshape_for_grouping(y_train, group_size)\n",
    "X_train_grouped = [reshape_for_grouping(X, group_size) for X in X_train]\n",
    "\n",
    "# X_test_0_grouped = reshape_for_grouping(X_test_0, group_size)\n",
    "# y_test_grouped = reshape_for_grouping(y_test, group_size)\n",
    "# X_test_grouped = [reshape_for_grouping(X, group_size) for X in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 1,  2],\n",
       "        [ 3,  4],\n",
       "        [ 5,  6],\n",
       "        [ 7,  8],\n",
       "        [ 9, 10],\n",
       "        [10, 11],\n",
       "        [10, 11],\n",
       "        [ 9, 10],\n",
       "        [ 7,  8],\n",
       "        [ 5,  6],\n",
       "        [ 3,  4],\n",
       "        [ 1,  2],\n",
       "        [10, 11],\n",
       "        [ 9, 10],\n",
       "        [ 7,  8],\n",
       "        [ 5,  6],\n",
       "        [ 3,  4],\n",
       "        [ 1,  2],\n",
       "        [10, 11],\n",
       "        [ 9, 10],\n",
       "        [ 7,  8],\n",
       "        [ 5,  6],\n",
       "        [ 3,  4],\n",
       "        [ 1,  2],\n",
       "        [10, 11],\n",
       "        [ 9, 10],\n",
       "        [ 7,  8],\n",
       "        [ 5,  6],\n",
       "        [ 3,  4],\n",
       "        [ 1,  2]]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the function again with test data of size 3\n",
    "test_data_small = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10], [10, 11]])\n",
    "reshaped_test_data_small = reshape_for_grouping(test_data_small, group_size)\n",
    "reshaped_test_data_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of X_train_0_grouped: 13\n",
      "part size: 3\n",
      "start_idx: 0\n",
      "end_idx: 3\n",
      "Shape of X_subset: (3, 30, 19)\n",
      "Shape of y_subset: (3, 30, 2)\n",
      "Part: 1/4, Epoch: 1/100, Total Epochs: 1, Train Loss: 0.06658151745796204, Learning Rate: 9.999999974752427e-07\n",
      "Losses and learning rates saved at epoch 1\n",
      "Part: 1/4, Epoch: 2/100, Total Epochs: 2, Train Loss: 0.06374183297157288, Learning Rate: 9.999999974752427e-07\n",
      "Part: 1/4, Epoch: 3/100, Total Epochs: 3, Train Loss: 0.05866602063179016, Learning Rate: 9.999999974752427e-07\n",
      "Part: 1/4, Epoch: 4/100, Total Epochs: 4, Train Loss: 0.049609001725912094, Learning Rate: 9.999999974752427e-07\n",
      "Part: 1/4, Epoch: 5/100, Total Epochs: 5, Train Loss: 0.03514695167541504, Learning Rate: 9.999999974752427e-07\n",
      "Part: 1/4, Epoch: 6/100, Total Epochs: 6, Train Loss: 0.018477842211723328, Learning Rate: 9.999999974752427e-07\n",
      "Part: 1/4, Epoch: 7/100, Total Epochs: 7, Train Loss: 0.014940631575882435, Learning Rate: 9.999999974752427e-07\n",
      "Part: 1/4, Epoch: 8/100, Total Epochs: 8, Train Loss: 0.009252935647964478, Learning Rate: 9.999999974752427e-07\n",
      "Part: 1/4, Epoch: 9/100, Total Epochs: 9, Train Loss: 0.0029481046367436647, Learning Rate: 9.999999974752427e-07\n",
      "Part: 1/4, Epoch: 10/100, Total Epochs: 10, Train Loss: 0.0015570191899314523, Learning Rate: 9.999999974752427e-07\n",
      "Losses and learning rates saved at epoch 10\n",
      "Part: 1/4, Epoch: 11/100, Total Epochs: 11, Train Loss: 0.0038428495172411203, Learning Rate: 9.999999974752427e-07\n",
      "Part: 1/4, Epoch: 12/100, Total Epochs: 12, Train Loss: 0.0041609699837863445, Learning Rate: 9.999999974752427e-07\n",
      "Part: 1/4, Epoch: 13/100, Total Epochs: 13, Train Loss: 0.0017033321782946587, Learning Rate: 9.999999974752427e-07\n",
      "Part: 1/4, Epoch: 14/100, Total Epochs: 14, Train Loss: 0.0005379935610108078, Learning Rate: 9.999999974752427e-07\n",
      "Part: 1/4, Epoch: 15/100, Total Epochs: 15, Train Loss: 0.0005491849151439965, Learning Rate: 9.999999974752427e-07\n",
      "Part: 1/4, Epoch: 16/100, Total Epochs: 16, Train Loss: 0.0007436203886754811, Learning Rate: 9.999999974752427e-07\n",
      "Part: 1/4, Epoch: 17/100, Total Epochs: 17, Train Loss: 0.0004964557010680437, Learning Rate: 9.999999974752427e-07\n",
      "Part: 1/4, Epoch: 18/100, Total Epochs: 18, Train Loss: 0.0003679301589727402, Learning Rate: 9.999999974752427e-07\n",
      "Part: 1/4, Epoch: 19/100, Total Epochs: 19, Train Loss: 0.0003260399680584669, Learning Rate: 9.999999974752427e-07\n",
      "Part: 1/4, Epoch: 20/100, Total Epochs: 20, Train Loss: 0.00020348669204395264, Learning Rate: 9.999999974752427e-07\n",
      "Losses and learning rates saved at epoch 20\n",
      "Part: 1/4, Epoch: 21/100, Total Epochs: 21, Train Loss: 0.00013622532424051315, Learning Rate: 9.999999974752427e-07\n",
      "Part: 1/4, Epoch: 22/100, Total Epochs: 22, Train Loss: 0.000126213111798279, Learning Rate: 9.999999974752427e-07\n",
      "Part: 1/4, Epoch: 23/100, Total Epochs: 23, Train Loss: 0.00010629266762407497, Learning Rate: 9.999999974752427e-07\n",
      "Part: 1/4, Epoch: 24/100, Total Epochs: 24, Train Loss: 7.429850666085258e-05, Learning Rate: 9.999999974752427e-07\n",
      "Part: 1/4, Epoch: 25/100, Total Epochs: 25, Train Loss: 8.083450666163117e-05, Learning Rate: 9.999999974752427e-07\n",
      "Part: 1/4, Epoch: 26/100, Total Epochs: 26, Train Loss: 9.177422180073336e-05, Learning Rate: 9.999999974752427e-07\n",
      "Part: 1/4, Epoch: 27/100, Total Epochs: 27, Train Loss: 7.68265817896463e-05, Learning Rate: 9.999999974752427e-07\n",
      "Part: 1/4, Epoch: 28/100, Total Epochs: 28, Train Loss: 5.7997738622361794e-05, Learning Rate: 9.999999974752427e-07\n",
      "Part: 1/4, Epoch: 29/100, Total Epochs: 29, Train Loss: 5.207667709328234e-05, Learning Rate: 9.999999974752427e-07\n",
      "Part: 1/4, Epoch: 30/100, Total Epochs: 30, Train Loss: 5.841390884597786e-05, Learning Rate: 9.999999974752427e-07\n",
      "Losses and learning rates saved at epoch 30\n",
      "Part: 1/4, Epoch: 31/100, Total Epochs: 31, Train Loss: 5.939077527727932e-05, Learning Rate: 9.999999974752427e-07\n",
      "Part: 1/4, Epoch: 32/100, Total Epochs: 32, Train Loss: 5.2510309615172446e-05, Learning Rate: 9.999999974752427e-07\n",
      "Part: 1/4, Epoch: 33/100, Total Epochs: 33, Train Loss: 4.9377853429177776e-05, Learning Rate: 9.999999974752427e-07\n",
      "Part: 1/4, Epoch: 34/100, Total Epochs: 34, Train Loss: 5.041560507379472e-05, Learning Rate: 9.999999974752427e-07\n",
      "Part: 1/4, Epoch: 35/100, Total Epochs: 35, Train Loss: 5.042421616963111e-05, Learning Rate: 9.999999974752427e-07\n",
      "Part: 1/4, Epoch: 36/100, Total Epochs: 36, Train Loss: 4.870495104114525e-05, Learning Rate: 9.999999974752427e-07\n",
      "Part: 1/4, Epoch: 37/100, Total Epochs: 37, Train Loss: 4.6683690015925094e-05, Learning Rate: 9.999999974752427e-07\n",
      "Part: 1/4, Epoch: 38/100, Total Epochs: 38, Train Loss: 4.5621713070431724e-05, Learning Rate: 9.999999974752427e-07\n",
      "Part: 1/4, Epoch: 39/100, Total Epochs: 39, Train Loss: 4.499382703215815e-05, Learning Rate: 9.999999974752427e-07\n",
      "Part: 1/4, Epoch: 40/100, Total Epochs: 40, Train Loss: 4.449362677405588e-05, Learning Rate: 9.999999974752427e-07\n",
      "Losses and learning rates saved at epoch 40\n",
      "Part: 1/4, Epoch: 41/100, Total Epochs: 41, Train Loss: 4.4403986976249143e-05, Learning Rate: 9.999999974752427e-07\n",
      "Part: 1/4, Epoch: 42/100, Total Epochs: 42, Train Loss: 4.419506876729429e-05, Learning Rate: 9.999999974752427e-07\n",
      "Part: 1/4, Epoch: 43/100, Total Epochs: 43, Train Loss: 4.369120506453328e-05, Learning Rate: 9.999999974752427e-07\n",
      "Part: 1/4, Epoch: 44/100, Total Epochs: 44, Train Loss: 4.308994903112762e-05, Learning Rate: 9.999999974752427e-07\n",
      "Part: 1/4, Epoch: 45/100, Total Epochs: 45, Train Loss: 4.2387120629427955e-05, Learning Rate: 9.999999974752427e-07\n",
      "Part: 1/4, Epoch: 46/100, Total Epochs: 46, Train Loss: 4.157325383857824e-05, Learning Rate: 9.999999974752427e-07\n",
      "Part: 1/4, Epoch: 47/100, Total Epochs: 47, Train Loss: 4.0838844142854214e-05, Learning Rate: 9.999999974752427e-07\n",
      "Part: 1/4, Epoch: 48/100, Total Epochs: 48, Train Loss: 4.039783379994333e-05, Learning Rate: 9.999999974752427e-07\n",
      "Part: 1/4, Epoch: 49/100, Total Epochs: 49, Train Loss: 4.0132406866177917e-05, Learning Rate: 9.999999974752427e-07\n",
      "Part: 1/4, Epoch: 50/100, Total Epochs: 50, Train Loss: 3.980712790507823e-05, Learning Rate: 9.999999974752427e-07\n",
      "Losses and learning rates saved at epoch 50\n",
      "Part: 1/4, Epoch: 51/100, Total Epochs: 51, Train Loss: 3.935125278076157e-05, Learning Rate: 9.999999974752427e-07\n",
      "Part: 1/4, Epoch: 52/100, Total Epochs: 52, Train Loss: 3.8810132537037134e-05, Learning Rate: 9.999999974752427e-07\n",
      "Part: 1/4, Epoch: 53/100, Total Epochs: 53, Train Loss: 3.825526073342189e-05, Learning Rate: 9.999999974752427e-07\n",
      "Part: 1/4, Epoch: 54/100, Total Epochs: 54, Train Loss: 3.7727488233940676e-05, Learning Rate: 9.999999974752427e-07\n",
      "Part: 1/4, Epoch: 55/100, Total Epochs: 55, Train Loss: 3.723575355252251e-05, Learning Rate: 9.999999974752427e-07\n",
      "Part: 1/4, Epoch: 56/100, Total Epochs: 56, Train Loss: 3.6781744711333886e-05, Learning Rate: 9.999999974752427e-07\n",
      "Part: 1/4, Epoch: 57/100, Total Epochs: 57, Train Loss: 3.635736720752902e-05, Learning Rate: 9.999999974752427e-07\n",
      "Part: 1/4, Epoch: 58/100, Total Epochs: 58, Train Loss: 3.5942164686275646e-05, Learning Rate: 9.999999974752427e-07\n",
      "Part: 1/4, Epoch: 59/100, Total Epochs: 59, Train Loss: 3.550783367245458e-05, Learning Rate: 9.999999974752427e-07\n",
      "Part: 1/4, Epoch: 60/100, Total Epochs: 60, Train Loss: 3.5051572922384366e-05, Learning Rate: 9.999999974752427e-07\n",
      "Losses and learning rates saved at epoch 60\n",
      "Part: 1/4, Epoch: 61/100, Total Epochs: 61, Train Loss: 3.459184154053219e-05, Learning Rate: 9.999999974752427e-07\n",
      "Part: 1/4, Epoch: 62/100, Total Epochs: 62, Train Loss: 3.415262108319439e-05, Learning Rate: 9.999999974752427e-07\n",
      "Part: 1/4, Epoch: 63/100, Total Epochs: 63, Train Loss: 3.372989522176795e-05, Learning Rate: 9.999999974752427e-07\n",
      "Part: 1/4, Epoch: 64/100, Total Epochs: 64, Train Loss: 3.331007974338718e-05, Learning Rate: 9.999999974752427e-07\n",
      "Part: 1/4, Epoch: 65/100, Total Epochs: 65, Train Loss: 3.289104643044993e-05, Learning Rate: 9.999999974752427e-07\n",
      "Part: 1/4, Epoch: 66/100, Total Epochs: 66, Train Loss: 3.247765198466368e-05, Learning Rate: 9.999999974752427e-07\n",
      "Part: 1/4, Epoch: 67/100, Total Epochs: 67, Train Loss: 3.207506233593449e-05, Learning Rate: 9.499999578110874e-07\n",
      "Part: 1/4, Epoch: 68/100, Total Epochs: 68, Train Loss: 3.1598025088896975e-05, Learning Rate: 9.499999578110874e-07\n",
      "Part: 1/4, Epoch: 69/100, Total Epochs: 69, Train Loss: 3.1223080441122875e-05, Learning Rate: 9.499999578110874e-07\n",
      "Part: 1/4, Epoch: 70/100, Total Epochs: 70, Train Loss: 3.086465221713297e-05, Learning Rate: 9.499999578110874e-07\n",
      "Losses and learning rates saved at epoch 70\n",
      "Part: 1/4, Epoch: 71/100, Total Epochs: 71, Train Loss: 3.050725172215607e-05, Learning Rate: 9.499999578110874e-07\n",
      "Part: 1/4, Epoch: 72/100, Total Epochs: 72, Train Loss: 3.014059620909393e-05, Learning Rate: 9.499999578110874e-07\n",
      "Part: 1/4, Epoch: 73/100, Total Epochs: 73, Train Loss: 2.9779757824144326e-05, Learning Rate: 9.499999578110874e-07\n",
      "Part: 1/4, Epoch: 74/100, Total Epochs: 74, Train Loss: 2.9430091672111303e-05, Learning Rate: 9.499999578110874e-07\n",
      "Part: 1/4, Epoch: 75/100, Total Epochs: 75, Train Loss: 2.909289651142899e-05, Learning Rate: 9.499999578110874e-07\n",
      "Part: 1/4, Epoch: 76/100, Total Epochs: 76, Train Loss: 2.87556613329798e-05, Learning Rate: 9.499999578110874e-07\n",
      "Part: 1/4, Epoch: 77/100, Total Epochs: 77, Train Loss: 2.841952482413035e-05, Learning Rate: 9.499999578110874e-07\n",
      "Part: 1/4, Epoch: 78/100, Total Epochs: 78, Train Loss: 2.8087129976483993e-05, Learning Rate: 9.499999578110874e-07\n",
      "Part: 1/4, Epoch: 79/100, Total Epochs: 79, Train Loss: 2.7760295779444277e-05, Learning Rate: 9.499999578110874e-07\n",
      "Part: 1/4, Epoch: 80/100, Total Epochs: 80, Train Loss: 2.744122321018949e-05, Learning Rate: 9.499999578110874e-07\n",
      "Losses and learning rates saved at epoch 80\n",
      "Part: 1/4, Epoch: 81/100, Total Epochs: 81, Train Loss: 2.712727291509509e-05, Learning Rate: 9.499999578110874e-07\n",
      "Part: 1/4, Epoch: 82/100, Total Epochs: 82, Train Loss: 2.6816103854798712e-05, Learning Rate: 9.499999578110874e-07\n",
      "Part: 1/4, Epoch: 83/100, Total Epochs: 83, Train Loss: 2.6511859687161632e-05, Learning Rate: 9.499999578110874e-07\n",
      "Part: 1/4, Epoch: 84/100, Total Epochs: 84, Train Loss: 2.620996929181274e-05, Learning Rate: 9.499999578110874e-07\n",
      "Part: 1/4, Epoch: 85/100, Total Epochs: 85, Train Loss: 2.5916981030604802e-05, Learning Rate: 9.499999578110874e-07\n",
      "Part: 1/4, Epoch: 86/100, Total Epochs: 86, Train Loss: 2.5623912733863108e-05, Learning Rate: 9.499999578110874e-07\n",
      "Part: 1/4, Epoch: 87/100, Total Epochs: 87, Train Loss: 2.533679253247101e-05, Learning Rate: 9.499999578110874e-07\n",
      "Part: 1/4, Epoch: 88/100, Total Epochs: 88, Train Loss: 2.5055791411432438e-05, Learning Rate: 9.499999578110874e-07\n",
      "Part: 1/4, Epoch: 89/100, Total Epochs: 89, Train Loss: 2.4779643354122527e-05, Learning Rate: 9.499999578110874e-07\n",
      "mean_train_loss: 2.4779643354122527e-05 <= 2.5e-5, skip to next stage.\n",
      "start_idx: 3\n",
      "end_idx: 6\n",
      "Shape of X_subset: (3, 30, 19)\n",
      "Shape of y_subset: (3, 30, 2)\n",
      "Part: 2/4, Epoch: 1/100, Total Epochs: 90, Train Loss: 0.0002619059232529253, Learning Rate: 9.499999578110874e-07\n",
      "Losses and learning rates saved at epoch 90\n",
      "Part: 2/4, Epoch: 2/100, Total Epochs: 91, Train Loss: 0.00020477257203310728, Learning Rate: 9.499999578110874e-07\n",
      "Part: 2/4, Epoch: 3/100, Total Epochs: 92, Train Loss: 0.00018682825611904263, Learning Rate: 9.499999578110874e-07\n",
      "Part: 2/4, Epoch: 4/100, Total Epochs: 93, Train Loss: 0.00017431488959118724, Learning Rate: 9.499999578110874e-07\n",
      "Part: 2/4, Epoch: 5/100, Total Epochs: 94, Train Loss: 0.0001524262479506433, Learning Rate: 9.499999578110874e-07\n",
      "Part: 2/4, Epoch: 6/100, Total Epochs: 95, Train Loss: 0.00012080935994163156, Learning Rate: 9.499999578110874e-07\n",
      "Part: 2/4, Epoch: 7/100, Total Epochs: 96, Train Loss: 0.00012078898726031184, Learning Rate: 9.499999578110874e-07\n",
      "Part: 2/4, Epoch: 8/100, Total Epochs: 97, Train Loss: 0.00010016828309744596, Learning Rate: 9.499999578110874e-07\n",
      "Part: 2/4, Epoch: 9/100, Total Epochs: 98, Train Loss: 8.698136662133038e-05, Learning Rate: 9.499999578110874e-07\n",
      "Part: 2/4, Epoch: 10/100, Total Epochs: 99, Train Loss: 7.862388883950189e-05, Learning Rate: 9.499999578110874e-07\n",
      "Part: 2/4, Epoch: 11/100, Total Epochs: 100, Train Loss: 7.212111813714728e-05, Learning Rate: 9.499999578110874e-07\n",
      "Model and weights saved at epoch 100\n",
      "Losses and learning rates saved at epoch 100\n",
      "Part: 2/4, Epoch: 12/100, Total Epochs: 101, Train Loss: 6.166799721540883e-05, Learning Rate: 9.499999578110874e-07\n",
      "Part: 2/4, Epoch: 13/100, Total Epochs: 102, Train Loss: 5.6692897487664595e-05, Learning Rate: 9.499999578110874e-07\n",
      "Part: 2/4, Epoch: 14/100, Total Epochs: 103, Train Loss: 5.314387090038508e-05, Learning Rate: 9.499999578110874e-07\n",
      "Part: 2/4, Epoch: 15/100, Total Epochs: 104, Train Loss: 4.826771328225732e-05, Learning Rate: 9.499999578110874e-07\n",
      "Part: 2/4, Epoch: 16/100, Total Epochs: 105, Train Loss: 4.442878707777709e-05, Learning Rate: 9.499999578110874e-07\n",
      "Part: 2/4, Epoch: 17/100, Total Epochs: 106, Train Loss: 4.241630085743964e-05, Learning Rate: 9.499999578110874e-07\n",
      "Part: 2/4, Epoch: 18/100, Total Epochs: 107, Train Loss: 4.019340849481523e-05, Learning Rate: 9.499999578110874e-07\n",
      "Part: 2/4, Epoch: 19/100, Total Epochs: 108, Train Loss: 3.7698780943173915e-05, Learning Rate: 9.499999578110874e-07\n",
      "Part: 2/4, Epoch: 20/100, Total Epochs: 109, Train Loss: 3.662594826892018e-05, Learning Rate: 9.499999578110874e-07\n",
      "Part: 2/4, Epoch: 21/100, Total Epochs: 110, Train Loss: 3.508199370116927e-05, Learning Rate: 9.499999578110874e-07\n",
      "Losses and learning rates saved at epoch 110\n",
      "Part: 2/4, Epoch: 22/100, Total Epochs: 111, Train Loss: 3.404595554457046e-05, Learning Rate: 9.499999578110874e-07\n",
      "Part: 2/4, Epoch: 23/100, Total Epochs: 112, Train Loss: 3.295146962045692e-05, Learning Rate: 9.499999578110874e-07\n",
      "Part: 2/4, Epoch: 24/100, Total Epochs: 113, Train Loss: 3.237597047700547e-05, Learning Rate: 9.499999578110874e-07\n",
      "Part: 2/4, Epoch: 25/100, Total Epochs: 114, Train Loss: 3.1322928407462314e-05, Learning Rate: 9.499999578110874e-07\n",
      "Part: 2/4, Epoch: 26/100, Total Epochs: 115, Train Loss: 3.0995390261523426e-05, Learning Rate: 9.499999578110874e-07\n",
      "Part: 2/4, Epoch: 27/100, Total Epochs: 116, Train Loss: 3.0191853511496447e-05, Learning Rate: 9.499999578110874e-07\n",
      "Part: 2/4, Epoch: 28/100, Total Epochs: 117, Train Loss: 2.997247975145001e-05, Learning Rate: 9.499999578110874e-07\n",
      "Part: 2/4, Epoch: 29/100, Total Epochs: 118, Train Loss: 2.92137629003264e-05, Learning Rate: 9.499999578110874e-07\n",
      "Part: 2/4, Epoch: 30/100, Total Epochs: 119, Train Loss: 2.917452366091311e-05, Learning Rate: 9.499999578110874e-07\n",
      "Part: 2/4, Epoch: 31/100, Total Epochs: 120, Train Loss: 2.8418269721441902e-05, Learning Rate: 9.499999578110874e-07\n",
      "Losses and learning rates saved at epoch 120\n",
      "Part: 2/4, Epoch: 32/100, Total Epochs: 121, Train Loss: 2.858798143279273e-05, Learning Rate: 9.499999578110874e-07\n",
      "Part: 2/4, Epoch: 33/100, Total Epochs: 122, Train Loss: 2.7753128961194307e-05, Learning Rate: 9.499999578110874e-07\n",
      "Part: 2/4, Epoch: 34/100, Total Epochs: 123, Train Loss: 2.816605228872504e-05, Learning Rate: 9.499999578110874e-07\n",
      "Part: 2/4, Epoch: 35/100, Total Epochs: 124, Train Loss: 2.723456418607384e-05, Learning Rate: 9.499999578110874e-07\n",
      "Part: 2/4, Epoch: 36/100, Total Epochs: 125, Train Loss: 2.802223571052309e-05, Learning Rate: 9.499999578110874e-07\n",
      "Part: 2/4, Epoch: 37/100, Total Epochs: 126, Train Loss: 2.6847994377021678e-05, Learning Rate: 9.499999578110874e-07\n",
      "Part: 2/4, Epoch: 38/100, Total Epochs: 127, Train Loss: 2.8338710762909614e-05, Learning Rate: 9.499999578110874e-07\n",
      "Part: 2/4, Epoch: 39/100, Total Epochs: 128, Train Loss: 2.6908193831332028e-05, Learning Rate: 9.499999578110874e-07\n",
      "Part: 2/4, Epoch: 40/100, Total Epochs: 129, Train Loss: 2.968485205201432e-05, Learning Rate: 9.499999578110874e-07\n",
      "Part: 2/4, Epoch: 41/100, Total Epochs: 130, Train Loss: 2.8145776013843715e-05, Learning Rate: 9.499999578110874e-07\n",
      "Losses and learning rates saved at epoch 130\n",
      "Part: 2/4, Epoch: 42/100, Total Epochs: 131, Train Loss: 3.37267410941422e-05, Learning Rate: 9.499999578110874e-07\n",
      "Part: 2/4, Epoch: 43/100, Total Epochs: 132, Train Loss: 3.3051845093723387e-05, Learning Rate: 9.499999578110874e-07\n",
      "Part: 2/4, Epoch: 44/100, Total Epochs: 133, Train Loss: 4.5434800995280966e-05, Learning Rate: 9.499999578110874e-07\n",
      "Part: 2/4, Epoch: 45/100, Total Epochs: 134, Train Loss: 4.963777246302925e-05, Learning Rate: 9.024999485518492e-07\n",
      "Part: 2/4, Epoch: 46/100, Total Epochs: 135, Train Loss: 7.621355325682089e-05, Learning Rate: 9.024999485518492e-07\n",
      "Part: 2/4, Epoch: 47/100, Total Epochs: 136, Train Loss: 8.832040475681424e-05, Learning Rate: 9.024999485518492e-07\n",
      "Part: 2/4, Epoch: 48/100, Total Epochs: 137, Train Loss: 0.00013741022848989815, Learning Rate: 9.024999485518492e-07\n",
      "Part: 2/4, Epoch: 49/100, Total Epochs: 138, Train Loss: 0.0001675524254096672, Learning Rate: 9.024999485518492e-07\n",
      "Part: 2/4, Epoch: 50/100, Total Epochs: 139, Train Loss: 0.0001460661442251876, Learning Rate: 9.024999485518492e-07\n",
      "Part: 2/4, Epoch: 51/100, Total Epochs: 140, Train Loss: 0.00046783083234913647, Learning Rate: 9.024999485518492e-07\n",
      "Losses and learning rates saved at epoch 140\n",
      "Part: 2/4, Epoch: 52/100, Total Epochs: 141, Train Loss: 0.00018635934975463897, Learning Rate: 9.024999485518492e-07\n",
      "Part: 2/4, Epoch: 53/100, Total Epochs: 142, Train Loss: 0.0005233900737948716, Learning Rate: 9.024999485518492e-07\n",
      "Part: 2/4, Epoch: 54/100, Total Epochs: 143, Train Loss: 0.0001207974783028476, Learning Rate: 9.024999485518492e-07\n",
      "Part: 2/4, Epoch: 55/100, Total Epochs: 144, Train Loss: 0.0005544550367631018, Learning Rate: 9.024999485518492e-07\n",
      "Part: 2/4, Epoch: 56/100, Total Epochs: 145, Train Loss: 8.788172999629751e-05, Learning Rate: 9.024999485518492e-07\n",
      "Part: 2/4, Epoch: 57/100, Total Epochs: 146, Train Loss: 0.00024160544853657484, Learning Rate: 9.024999485518492e-07\n",
      "Part: 2/4, Epoch: 58/100, Total Epochs: 147, Train Loss: 8.311006968142465e-05, Learning Rate: 9.024999485518492e-07\n",
      "Part: 2/4, Epoch: 59/100, Total Epochs: 148, Train Loss: 0.0004187414888292551, Learning Rate: 9.024999485518492e-07\n",
      "Part: 2/4, Epoch: 60/100, Total Epochs: 149, Train Loss: 0.00019964137754868716, Learning Rate: 9.024999485518492e-07\n",
      "Part: 2/4, Epoch: 61/100, Total Epochs: 150, Train Loss: 0.0007994221523404121, Learning Rate: 9.024999485518492e-07\n",
      "Losses and learning rates saved at epoch 150\n",
      "Part: 2/4, Epoch: 62/100, Total Epochs: 151, Train Loss: 0.00012142924970248714, Learning Rate: 9.024999485518492e-07\n",
      "Part: 2/4, Epoch: 63/100, Total Epochs: 152, Train Loss: 0.0008111507049761713, Learning Rate: 9.024999485518492e-07\n",
      "Part: 2/4, Epoch: 64/100, Total Epochs: 153, Train Loss: 0.00012151031842222437, Learning Rate: 9.024999485518492e-07\n",
      "Part: 2/4, Epoch: 65/100, Total Epochs: 154, Train Loss: 0.00021107018983457237, Learning Rate: 9.024999485518492e-07\n",
      "Part: 2/4, Epoch: 66/100, Total Epochs: 155, Train Loss: 8.62226661411114e-05, Learning Rate: 9.024999485518492e-07\n",
      "Part: 2/4, Epoch: 67/100, Total Epochs: 156, Train Loss: 0.00045537788537330925, Learning Rate: 9.024999485518492e-07\n",
      "Part: 2/4, Epoch: 68/100, Total Epochs: 157, Train Loss: 0.0001601882977411151, Learning Rate: 9.024999485518492e-07\n",
      "Part: 2/4, Epoch: 69/100, Total Epochs: 158, Train Loss: 0.0006067946669645607, Learning Rate: 9.024999485518492e-07\n",
      "Part: 2/4, Epoch: 70/100, Total Epochs: 159, Train Loss: 0.00016577885253354907, Learning Rate: 9.024999485518492e-07\n",
      "Part: 2/4, Epoch: 71/100, Total Epochs: 160, Train Loss: 0.001030813786201179, Learning Rate: 9.024999485518492e-07\n",
      "Losses and learning rates saved at epoch 160\n",
      "Part: 2/4, Epoch: 72/100, Total Epochs: 161, Train Loss: 0.0001117754727602005, Learning Rate: 9.024999485518492e-07\n",
      "Part: 2/4, Epoch: 73/100, Total Epochs: 162, Train Loss: 0.0004488083941396326, Learning Rate: 9.024999485518492e-07\n",
      "Part: 2/4, Epoch: 74/100, Total Epochs: 163, Train Loss: 7.314954564208165e-05, Learning Rate: 9.024999485518492e-07\n",
      "Part: 2/4, Epoch: 75/100, Total Epochs: 164, Train Loss: 0.0005013541667722166, Learning Rate: 9.024999485518492e-07\n",
      "Part: 2/4, Epoch: 76/100, Total Epochs: 165, Train Loss: 0.00010886320524150506, Learning Rate: 9.024999485518492e-07\n",
      "Part: 2/4, Epoch: 77/100, Total Epochs: 166, Train Loss: 0.000549818912986666, Learning Rate: 9.024999485518492e-07\n",
      "Part: 2/4, Epoch: 78/100, Total Epochs: 167, Train Loss: 0.00014870520681142807, Learning Rate: 9.024999485518492e-07\n",
      "Part: 2/4, Epoch: 79/100, Total Epochs: 168, Train Loss: 0.000864248548168689, Learning Rate: 9.024999485518492e-07\n",
      "Part: 2/4, Epoch: 80/100, Total Epochs: 169, Train Loss: 0.00011179073044331744, Learning Rate: 9.024999485518492e-07\n",
      "Part: 2/4, Epoch: 81/100, Total Epochs: 170, Train Loss: 0.0006774134817533195, Learning Rate: 9.024999485518492e-07\n",
      "Losses and learning rates saved at epoch 170\n",
      "Part: 2/4, Epoch: 82/100, Total Epochs: 171, Train Loss: 0.0001777405122993514, Learning Rate: 9.024999485518492e-07\n",
      "Part: 2/4, Epoch: 83/100, Total Epochs: 172, Train Loss: 0.0010475270682945848, Learning Rate: 9.024999485518492e-07\n",
      "Part: 2/4, Epoch: 84/100, Total Epochs: 173, Train Loss: 0.0002035048819379881, Learning Rate: 9.024999485518492e-07\n",
      "Part: 2/4, Epoch: 85/100, Total Epochs: 174, Train Loss: 0.0008562162984162569, Learning Rate: 9.024999485518492e-07\n",
      "Part: 2/4, Epoch: 86/100, Total Epochs: 175, Train Loss: 0.00045930492342449725, Learning Rate: 9.024999485518492e-07\n",
      "Part: 2/4, Epoch: 87/100, Total Epochs: 176, Train Loss: 0.0002993246598634869, Learning Rate: 9.024999485518492e-07\n",
      "Part: 2/4, Epoch: 88/100, Total Epochs: 177, Train Loss: 0.0002378888166276738, Learning Rate: 9.024999485518492e-07\n",
      "Part: 2/4, Epoch: 89/100, Total Epochs: 178, Train Loss: 0.00011112642823718488, Learning Rate: 9.024999485518492e-07\n",
      "Part: 2/4, Epoch: 90/100, Total Epochs: 179, Train Loss: 0.00020104557916056365, Learning Rate: 9.024999485518492e-07\n",
      "Part: 2/4, Epoch: 91/100, Total Epochs: 180, Train Loss: 0.00020142718858551234, Learning Rate: 9.024999485518492e-07\n",
      "Losses and learning rates saved at epoch 180\n",
      "Part: 2/4, Epoch: 92/100, Total Epochs: 181, Train Loss: 6.97875875630416e-05, Learning Rate: 9.024999485518492e-07\n",
      "Part: 2/4, Epoch: 93/100, Total Epochs: 182, Train Loss: 8.860625530360267e-05, Learning Rate: 9.024999485518492e-07\n",
      "Part: 2/4, Epoch: 94/100, Total Epochs: 183, Train Loss: 8.174801041604951e-05, Learning Rate: 9.024999485518492e-07\n",
      "Part: 2/4, Epoch: 95/100, Total Epochs: 184, Train Loss: 7.11863103788346e-05, Learning Rate: 9.024999485518492e-07\n",
      "Part: 2/4, Epoch: 96/100, Total Epochs: 185, Train Loss: 5.133983722771518e-05, Learning Rate: 9.024999485518492e-07\n",
      "Part: 2/4, Epoch: 97/100, Total Epochs: 186, Train Loss: 4.8797188355820253e-05, Learning Rate: 9.024999485518492e-07\n",
      "Part: 2/4, Epoch: 98/100, Total Epochs: 187, Train Loss: 4.359733429737389e-05, Learning Rate: 9.024999485518492e-07\n",
      "Part: 2/4, Epoch: 99/100, Total Epochs: 188, Train Loss: 4.271850411896594e-05, Learning Rate: 9.024999485518492e-07\n",
      "Part: 2/4, Epoch: 100/100, Total Epochs: 189, Train Loss: 3.514426862238906e-05, Learning Rate: 9.024999485518492e-07\n",
      "Model and weights saved at epoch 189\n",
      "Losses and learning rates saved at epoch 189\n",
      "start_idx: 6\n",
      "end_idx: 9\n",
      "Shape of X_subset: (3, 30, 19)\n",
      "Shape of y_subset: (3, 30, 2)\n",
      "Part: 3/4, Epoch: 1/100, Total Epochs: 190, Train Loss: 0.00011533479118952528, Learning Rate: 9.024999485518492e-07\n",
      "Losses and learning rates saved at epoch 190\n",
      "Part: 3/4, Epoch: 2/100, Total Epochs: 191, Train Loss: 0.00012706301640719175, Learning Rate: 9.024999485518492e-07\n",
      "Part: 3/4, Epoch: 3/100, Total Epochs: 192, Train Loss: 0.0002722546341829002, Learning Rate: 9.024999485518492e-07\n",
      "Part: 3/4, Epoch: 4/100, Total Epochs: 193, Train Loss: 0.0008924811263568699, Learning Rate: 9.024999485518492e-07\n",
      "Part: 3/4, Epoch: 5/100, Total Epochs: 194, Train Loss: 0.0014501940459012985, Learning Rate: 9.024999485518492e-07\n",
      "Part: 3/4, Epoch: 6/100, Total Epochs: 195, Train Loss: 0.0027472663205116987, Learning Rate: 9.024999485518492e-07\n",
      "Part: 3/4, Epoch: 7/100, Total Epochs: 196, Train Loss: 0.000808903481811285, Learning Rate: 9.024999485518492e-07\n",
      "Part: 3/4, Epoch: 8/100, Total Epochs: 197, Train Loss: 0.0017732406267896295, Learning Rate: 9.024999485518492e-07\n",
      "Part: 3/4, Epoch: 9/100, Total Epochs: 198, Train Loss: 0.0020459715742617846, Learning Rate: 9.024999485518492e-07\n",
      "Part: 3/4, Epoch: 10/100, Total Epochs: 199, Train Loss: 0.0014948205789551139, Learning Rate: 9.024999485518492e-07\n",
      "Part: 3/4, Epoch: 11/100, Total Epochs: 200, Train Loss: 0.0003983984643127769, Learning Rate: 8.573749710194534e-07\n",
      "Model and weights saved at epoch 200\n",
      "Losses and learning rates saved at epoch 200\n",
      "Part: 3/4, Epoch: 12/100, Total Epochs: 201, Train Loss: 0.0006639463827013969, Learning Rate: 8.573749710194534e-07\n",
      "Part: 3/4, Epoch: 13/100, Total Epochs: 202, Train Loss: 0.0005686786025762558, Learning Rate: 8.573749710194534e-07\n",
      "Part: 3/4, Epoch: 14/100, Total Epochs: 203, Train Loss: 0.00020607402257155627, Learning Rate: 8.573749710194534e-07\n",
      "Part: 3/4, Epoch: 15/100, Total Epochs: 204, Train Loss: 0.00033874696237035096, Learning Rate: 8.573749710194534e-07\n",
      "Part: 3/4, Epoch: 16/100, Total Epochs: 205, Train Loss: 0.0003159849438816309, Learning Rate: 8.573749710194534e-07\n",
      "Part: 3/4, Epoch: 17/100, Total Epochs: 206, Train Loss: 9.702550596557558e-05, Learning Rate: 8.573749710194534e-07\n",
      "Part: 3/4, Epoch: 18/100, Total Epochs: 207, Train Loss: 0.00012591005361173302, Learning Rate: 8.573749710194534e-07\n",
      "Part: 3/4, Epoch: 19/100, Total Epochs: 208, Train Loss: 0.00012992027041036636, Learning Rate: 8.573749710194534e-07\n",
      "Part: 3/4, Epoch: 20/100, Total Epochs: 209, Train Loss: 9.038745338330045e-05, Learning Rate: 8.573749710194534e-07\n",
      "Part: 3/4, Epoch: 21/100, Total Epochs: 210, Train Loss: 9.23427360248752e-05, Learning Rate: 8.573749710194534e-07\n",
      "Losses and learning rates saved at epoch 210\n",
      "Part: 3/4, Epoch: 22/100, Total Epochs: 211, Train Loss: 8.580942085245624e-05, Learning Rate: 8.573749710194534e-07\n",
      "Part: 3/4, Epoch: 23/100, Total Epochs: 212, Train Loss: 4.510220969677903e-05, Learning Rate: 8.573749710194534e-07\n",
      "Part: 3/4, Epoch: 24/100, Total Epochs: 213, Train Loss: 4.440163684193976e-05, Learning Rate: 8.573749710194534e-07\n",
      "Part: 3/4, Epoch: 25/100, Total Epochs: 214, Train Loss: 5.028071973356418e-05, Learning Rate: 8.573749710194534e-07\n",
      "Part: 3/4, Epoch: 26/100, Total Epochs: 215, Train Loss: 4.8391782911494374e-05, Learning Rate: 8.573749710194534e-07\n",
      "Part: 3/4, Epoch: 27/100, Total Epochs: 216, Train Loss: 4.450085907592438e-05, Learning Rate: 8.573749710194534e-07\n",
      "Part: 3/4, Epoch: 28/100, Total Epochs: 217, Train Loss: 4.2246243538102135e-05, Learning Rate: 8.573749710194534e-07\n",
      "Part: 3/4, Epoch: 29/100, Total Epochs: 218, Train Loss: 3.7583235098281875e-05, Learning Rate: 8.573749710194534e-07\n",
      "Part: 3/4, Epoch: 30/100, Total Epochs: 219, Train Loss: 3.222325540264137e-05, Learning Rate: 8.573749710194534e-07\n",
      "Part: 3/4, Epoch: 31/100, Total Epochs: 220, Train Loss: 3.5632754588732496e-05, Learning Rate: 8.573749710194534e-07\n",
      "Losses and learning rates saved at epoch 220\n",
      "Part: 3/4, Epoch: 32/100, Total Epochs: 221, Train Loss: 3.6995101254433393e-05, Learning Rate: 8.573749710194534e-07\n",
      "Part: 3/4, Epoch: 33/100, Total Epochs: 222, Train Loss: 3.8844180380692706e-05, Learning Rate: 8.573749710194534e-07\n",
      "Part: 3/4, Epoch: 34/100, Total Epochs: 223, Train Loss: 3.88003682019189e-05, Learning Rate: 8.573749710194534e-07\n",
      "Part: 3/4, Epoch: 35/100, Total Epochs: 224, Train Loss: 4.0534985600970685e-05, Learning Rate: 8.573749710194534e-07\n",
      "Part: 3/4, Epoch: 36/100, Total Epochs: 225, Train Loss: 3.929167360183783e-05, Learning Rate: 8.573749710194534e-07\n",
      "Part: 3/4, Epoch: 37/100, Total Epochs: 226, Train Loss: 4.1877177864080295e-05, Learning Rate: 8.573749710194534e-07\n",
      "Part: 3/4, Epoch: 38/100, Total Epochs: 227, Train Loss: 4.4159907702123746e-05, Learning Rate: 8.573749710194534e-07\n",
      "Part: 3/4, Epoch: 39/100, Total Epochs: 228, Train Loss: 4.852167330682278e-05, Learning Rate: 8.573749710194534e-07\n",
      "Part: 3/4, Epoch: 40/100, Total Epochs: 229, Train Loss: 5.248190791462548e-05, Learning Rate: 8.573749710194534e-07\n",
      "Part: 3/4, Epoch: 41/100, Total Epochs: 230, Train Loss: 5.627535210805945e-05, Learning Rate: 8.573749710194534e-07\n",
      "Losses and learning rates saved at epoch 230\n",
      "Part: 3/4, Epoch: 42/100, Total Epochs: 231, Train Loss: 6.093381671234965e-05, Learning Rate: 8.573749710194534e-07\n",
      "Part: 3/4, Epoch: 43/100, Total Epochs: 232, Train Loss: 6.528939411509782e-05, Learning Rate: 8.573749710194534e-07\n",
      "Part: 3/4, Epoch: 44/100, Total Epochs: 233, Train Loss: 7.307640771614388e-05, Learning Rate: 8.573749710194534e-07\n",
      "Part: 3/4, Epoch: 45/100, Total Epochs: 234, Train Loss: 7.858573371777311e-05, Learning Rate: 8.573749710194534e-07\n",
      "Part: 3/4, Epoch: 46/100, Total Epochs: 235, Train Loss: 9.09720329218544e-05, Learning Rate: 8.573749710194534e-07\n",
      "Part: 3/4, Epoch: 47/100, Total Epochs: 236, Train Loss: 9.678781498223543e-05, Learning Rate: 8.573749710194534e-07\n",
      "Part: 3/4, Epoch: 48/100, Total Epochs: 237, Train Loss: 0.00011391788575565442, Learning Rate: 8.573749710194534e-07\n",
      "Part: 3/4, Epoch: 49/100, Total Epochs: 238, Train Loss: 0.00011818771599791944, Learning Rate: 8.573749710194534e-07\n",
      "Part: 3/4, Epoch: 50/100, Total Epochs: 239, Train Loss: 0.00014094497601035982, Learning Rate: 8.573749710194534e-07\n",
      "Part: 3/4, Epoch: 51/100, Total Epochs: 240, Train Loss: 0.0001433165743947029, Learning Rate: 8.573749710194534e-07\n",
      "Losses and learning rates saved at epoch 240\n",
      "Part: 3/4, Epoch: 52/100, Total Epochs: 241, Train Loss: 0.0001731839292915538, Learning Rate: 8.573749710194534e-07\n",
      "Part: 3/4, Epoch: 53/100, Total Epochs: 242, Train Loss: 0.0001706651964923367, Learning Rate: 8.573749710194534e-07\n",
      "Part: 3/4, Epoch: 54/100, Total Epochs: 243, Train Loss: 0.0002055172953987494, Learning Rate: 8.573749710194534e-07\n",
      "Part: 3/4, Epoch: 55/100, Total Epochs: 244, Train Loss: 0.00019455312576610595, Learning Rate: 8.573749710194534e-07\n",
      "Part: 3/4, Epoch: 56/100, Total Epochs: 245, Train Loss: 0.00022974146122578532, Learning Rate: 8.573749710194534e-07\n",
      "Part: 3/4, Epoch: 57/100, Total Epochs: 246, Train Loss: 0.0002077947137877345, Learning Rate: 8.573749710194534e-07\n",
      "Part: 3/4, Epoch: 58/100, Total Epochs: 247, Train Loss: 0.00023647384659852833, Learning Rate: 8.573749710194534e-07\n",
      "Part: 3/4, Epoch: 59/100, Total Epochs: 248, Train Loss: 0.0002041192929027602, Learning Rate: 8.573749710194534e-07\n",
      "Part: 3/4, Epoch: 60/100, Total Epochs: 249, Train Loss: 0.00021993368864059448, Learning Rate: 8.573749710194534e-07\n",
      "Part: 3/4, Epoch: 61/100, Total Epochs: 250, Train Loss: 0.0001807219086913392, Learning Rate: 8.573749710194534e-07\n",
      "Losses and learning rates saved at epoch 250\n",
      "Part: 3/4, Epoch: 62/100, Total Epochs: 251, Train Loss: 0.00018185430963058025, Learning Rate: 8.573749710194534e-07\n",
      "Part: 3/4, Epoch: 63/100, Total Epochs: 252, Train Loss: 0.00014291559637058526, Learning Rate: 8.573749710194534e-07\n",
      "Part: 3/4, Epoch: 64/100, Total Epochs: 253, Train Loss: 0.00013453738938551396, Learning Rate: 8.573749710194534e-07\n",
      "Part: 3/4, Epoch: 65/100, Total Epochs: 254, Train Loss: 0.00010209617903456092, Learning Rate: 8.573749710194534e-07\n",
      "Part: 3/4, Epoch: 66/100, Total Epochs: 255, Train Loss: 9.139275789493695e-05, Learning Rate: 8.573749710194534e-07\n",
      "Part: 3/4, Epoch: 67/100, Total Epochs: 256, Train Loss: 6.821053102612495e-05, Learning Rate: 8.573749710194534e-07\n",
      "Part: 3/4, Epoch: 68/100, Total Epochs: 257, Train Loss: 5.957752000540495e-05, Learning Rate: 8.573749710194534e-07\n",
      "Part: 3/4, Epoch: 69/100, Total Epochs: 258, Train Loss: 4.505785182118416e-05, Learning Rate: 8.573749710194534e-07\n",
      "Part: 3/4, Epoch: 70/100, Total Epochs: 259, Train Loss: 3.948492303607054e-05, Learning Rate: 8.573749710194534e-07\n",
      "Part: 3/4, Epoch: 71/100, Total Epochs: 260, Train Loss: 3.145190930808894e-05, Learning Rate: 8.573749710194534e-07\n",
      "Losses and learning rates saved at epoch 260\n",
      "Part: 3/4, Epoch: 72/100, Total Epochs: 261, Train Loss: 2.8190004741190933e-05, Learning Rate: 8.573749710194534e-07\n",
      "Part: 3/4, Epoch: 73/100, Total Epochs: 262, Train Loss: 2.424483136564959e-05, Learning Rate: 8.573749710194534e-07\n",
      "mean_train_loss: 2.424483136564959e-05 <= 2.5e-5, skip to next stage.\n",
      "start_idx: 9\n",
      "end_idx: 12\n",
      "Shape of X_subset: (3, 30, 19)\n",
      "Shape of y_subset: (3, 30, 2)\n",
      "Part: 4/4, Epoch: 1/100, Total Epochs: 263, Train Loss: 8.362656808458269e-05, Learning Rate: 8.573749710194534e-07\n",
      "Part: 4/4, Epoch: 2/100, Total Epochs: 264, Train Loss: 0.00021781621035188437, Learning Rate: 8.573749710194534e-07\n",
      "Part: 4/4, Epoch: 3/100, Total Epochs: 265, Train Loss: 0.00012972667173016816, Learning Rate: 8.573749710194534e-07\n",
      "Part: 4/4, Epoch: 4/100, Total Epochs: 266, Train Loss: 0.00024817243684083223, Learning Rate: 8.573749710194534e-07\n",
      "Part: 4/4, Epoch: 5/100, Total Epochs: 267, Train Loss: 0.0005391730810515583, Learning Rate: 8.145062224684807e-07\n",
      "Part: 4/4, Epoch: 6/100, Total Epochs: 268, Train Loss: 0.0005684833158738911, Learning Rate: 8.145062224684807e-07\n",
      "Part: 4/4, Epoch: 7/100, Total Epochs: 269, Train Loss: 0.0022129497956484556, Learning Rate: 8.145062224684807e-07\n",
      "Part: 4/4, Epoch: 8/100, Total Epochs: 270, Train Loss: 0.00047577745863236487, Learning Rate: 8.145062224684807e-07\n",
      "Losses and learning rates saved at epoch 270\n",
      "Part: 4/4, Epoch: 9/100, Total Epochs: 271, Train Loss: 0.0021652942523360252, Learning Rate: 8.145062224684807e-07\n",
      "Part: 4/4, Epoch: 10/100, Total Epochs: 272, Train Loss: 0.0007693844963796437, Learning Rate: 8.145062224684807e-07\n",
      "Part: 4/4, Epoch: 11/100, Total Epochs: 273, Train Loss: 0.0012120892060920596, Learning Rate: 8.145062224684807e-07\n",
      "Part: 4/4, Epoch: 12/100, Total Epochs: 274, Train Loss: 0.0009547777590341866, Learning Rate: 8.145062224684807e-07\n",
      "Part: 4/4, Epoch: 13/100, Total Epochs: 275, Train Loss: 0.0013556038029491901, Learning Rate: 8.145062224684807e-07\n",
      "Part: 4/4, Epoch: 14/100, Total Epochs: 276, Train Loss: 0.0009407817269675434, Learning Rate: 8.145062224684807e-07\n",
      "Part: 4/4, Epoch: 15/100, Total Epochs: 277, Train Loss: 0.0033913764636963606, Learning Rate: 8.145062224684807e-07\n",
      "Part: 4/4, Epoch: 16/100, Total Epochs: 278, Train Loss: 0.002469185274094343, Learning Rate: 8.145062224684807e-07\n",
      "Part: 4/4, Epoch: 17/100, Total Epochs: 279, Train Loss: 0.002797873690724373, Learning Rate: 8.145062224684807e-07\n",
      "Part: 4/4, Epoch: 18/100, Total Epochs: 280, Train Loss: 0.002312362426891923, Learning Rate: 8.145062224684807e-07\n",
      "Losses and learning rates saved at epoch 280\n",
      "Part: 4/4, Epoch: 19/100, Total Epochs: 281, Train Loss: 0.008350285701453686, Learning Rate: 8.145062224684807e-07\n",
      "Part: 4/4, Epoch: 20/100, Total Epochs: 282, Train Loss: 0.00834645051509142, Learning Rate: 8.145062224684807e-07\n",
      "Part: 4/4, Epoch: 21/100, Total Epochs: 283, Train Loss: 0.0016939634224399924, Learning Rate: 8.145062224684807e-07\n",
      "Part: 4/4, Epoch: 22/100, Total Epochs: 284, Train Loss: 0.004177117254585028, Learning Rate: 8.145062224684807e-07\n",
      "Part: 4/4, Epoch: 23/100, Total Epochs: 285, Train Loss: 0.002880408661440015, Learning Rate: 8.145062224684807e-07\n",
      "Part: 4/4, Epoch: 24/100, Total Epochs: 286, Train Loss: 0.0004412872076500207, Learning Rate: 8.145062224684807e-07\n",
      "Part: 4/4, Epoch: 25/100, Total Epochs: 287, Train Loss: 0.0012029424542561173, Learning Rate: 8.145062224684807e-07\n",
      "Part: 4/4, Epoch: 26/100, Total Epochs: 288, Train Loss: 0.0004281875735614449, Learning Rate: 8.145062224684807e-07\n",
      "Part: 4/4, Epoch: 27/100, Total Epochs: 289, Train Loss: 0.0006177734467200935, Learning Rate: 8.145062224684807e-07\n",
      "Part: 4/4, Epoch: 28/100, Total Epochs: 290, Train Loss: 0.0002554177481215447, Learning Rate: 8.145062224684807e-07\n",
      "Losses and learning rates saved at epoch 290\n",
      "Part: 4/4, Epoch: 29/100, Total Epochs: 291, Train Loss: 0.000239576751482673, Learning Rate: 8.145062224684807e-07\n",
      "Part: 4/4, Epoch: 30/100, Total Epochs: 292, Train Loss: 0.0001764571206877008, Learning Rate: 8.145062224684807e-07\n",
      "Part: 4/4, Epoch: 31/100, Total Epochs: 293, Train Loss: 0.0002701497287489474, Learning Rate: 8.145062224684807e-07\n",
      "Part: 4/4, Epoch: 32/100, Total Epochs: 294, Train Loss: 0.0002128041087416932, Learning Rate: 8.145062224684807e-07\n",
      "Part: 4/4, Epoch: 33/100, Total Epochs: 295, Train Loss: 0.00010498472693143412, Learning Rate: 8.145062224684807e-07\n",
      "Part: 4/4, Epoch: 34/100, Total Epochs: 296, Train Loss: 9.482751920586452e-05, Learning Rate: 8.145062224684807e-07\n",
      "Part: 4/4, Epoch: 35/100, Total Epochs: 297, Train Loss: 0.0001264230813831091, Learning Rate: 8.145062224684807e-07\n",
      "Part: 4/4, Epoch: 36/100, Total Epochs: 298, Train Loss: 0.0001436579186702147, Learning Rate: 8.145062224684807e-07\n",
      "Part: 4/4, Epoch: 37/100, Total Epochs: 299, Train Loss: 5.863695696461946e-05, Learning Rate: 8.145062224684807e-07\n",
      "Part: 4/4, Epoch: 38/100, Total Epochs: 300, Train Loss: 4.851594349020161e-05, Learning Rate: 8.145062224684807e-07\n",
      "Model and weights saved at epoch 300\n",
      "Losses and learning rates saved at epoch 300\n",
      "Part: 4/4, Epoch: 39/100, Total Epochs: 301, Train Loss: 5.4766449466114864e-05, Learning Rate: 8.145062224684807e-07\n",
      "Part: 4/4, Epoch: 40/100, Total Epochs: 302, Train Loss: 7.411287515424192e-05, Learning Rate: 8.145062224684807e-07\n",
      "Part: 4/4, Epoch: 41/100, Total Epochs: 303, Train Loss: 3.858535274048336e-05, Learning Rate: 8.145062224684807e-07\n",
      "Part: 4/4, Epoch: 42/100, Total Epochs: 304, Train Loss: 3.343231219332665e-05, Learning Rate: 8.145062224684807e-07\n",
      "Part: 4/4, Epoch: 43/100, Total Epochs: 305, Train Loss: 3.536535587045364e-05, Learning Rate: 8.145062224684807e-07\n",
      "Part: 4/4, Epoch: 44/100, Total Epochs: 306, Train Loss: 4.0228715079138055e-05, Learning Rate: 8.145062224684807e-07\n",
      "Part: 4/4, Epoch: 45/100, Total Epochs: 307, Train Loss: 3.257444768678397e-05, Learning Rate: 8.145062224684807e-07\n",
      "Part: 4/4, Epoch: 46/100, Total Epochs: 308, Train Loss: 3.022079727088567e-05, Learning Rate: 8.145062224684807e-07\n",
      "Part: 4/4, Epoch: 47/100, Total Epochs: 309, Train Loss: 3.412294972804375e-05, Learning Rate: 8.145062224684807e-07\n",
      "Part: 4/4, Epoch: 48/100, Total Epochs: 310, Train Loss: 3.1278421374736354e-05, Learning Rate: 8.145062224684807e-07\n",
      "Losses and learning rates saved at epoch 310\n",
      "Part: 4/4, Epoch: 49/100, Total Epochs: 311, Train Loss: 3.1220501114148647e-05, Learning Rate: 8.145062224684807e-07\n",
      "Part: 4/4, Epoch: 50/100, Total Epochs: 312, Train Loss: 2.979675446113106e-05, Learning Rate: 8.145062224684807e-07\n",
      "Part: 4/4, Epoch: 51/100, Total Epochs: 313, Train Loss: 3.2770494726719335e-05, Learning Rate: 8.145062224684807e-07\n",
      "Part: 4/4, Epoch: 52/100, Total Epochs: 314, Train Loss: 2.9632232326548547e-05, Learning Rate: 8.145062224684807e-07\n",
      "Part: 4/4, Epoch: 53/100, Total Epochs: 315, Train Loss: 3.0082781449891627e-05, Learning Rate: 8.145062224684807e-07\n",
      "Part: 4/4, Epoch: 54/100, Total Epochs: 316, Train Loss: 2.975175993924495e-05, Learning Rate: 8.145062224684807e-07\n",
      "Part: 4/4, Epoch: 55/100, Total Epochs: 317, Train Loss: 3.0271739888121374e-05, Learning Rate: 8.145062224684807e-07\n",
      "Part: 4/4, Epoch: 56/100, Total Epochs: 318, Train Loss: 2.939417572633829e-05, Learning Rate: 8.145062224684807e-07\n",
      "Part: 4/4, Epoch: 57/100, Total Epochs: 319, Train Loss: 2.9361761335167103e-05, Learning Rate: 8.145062224684807e-07\n",
      "Part: 4/4, Epoch: 58/100, Total Epochs: 320, Train Loss: 2.9652301236637868e-05, Learning Rate: 8.145062224684807e-07\n",
      "Losses and learning rates saved at epoch 320\n",
      "Part: 4/4, Epoch: 59/100, Total Epochs: 321, Train Loss: 2.9034921681159176e-05, Learning Rate: 8.145062224684807e-07\n",
      "Part: 4/4, Epoch: 60/100, Total Epochs: 322, Train Loss: 2.9195769457146525e-05, Learning Rate: 8.145062224684807e-07\n",
      "Part: 4/4, Epoch: 61/100, Total Epochs: 323, Train Loss: 2.895513898693025e-05, Learning Rate: 8.145062224684807e-07\n",
      "Part: 4/4, Epoch: 62/100, Total Epochs: 324, Train Loss: 2.908813621615991e-05, Learning Rate: 8.145062224684807e-07\n",
      "Part: 4/4, Epoch: 63/100, Total Epochs: 325, Train Loss: 2.8659802410402335e-05, Learning Rate: 8.145062224684807e-07\n",
      "Part: 4/4, Epoch: 64/100, Total Epochs: 326, Train Loss: 2.8745604140567593e-05, Learning Rate: 8.145062224684807e-07\n",
      "Part: 4/4, Epoch: 65/100, Total Epochs: 327, Train Loss: 2.8690745239146054e-05, Learning Rate: 8.145062224684807e-07\n",
      "Part: 4/4, Epoch: 66/100, Total Epochs: 328, Train Loss: 2.8483867936301976e-05, Learning Rate: 8.145062224684807e-07\n",
      "Part: 4/4, Epoch: 67/100, Total Epochs: 329, Train Loss: 2.842284447979182e-05, Learning Rate: 8.145062224684807e-07\n",
      "Part: 4/4, Epoch: 68/100, Total Epochs: 330, Train Loss: 2.8359214411466382e-05, Learning Rate: 8.145062224684807e-07\n",
      "Losses and learning rates saved at epoch 330\n",
      "Part: 4/4, Epoch: 69/100, Total Epochs: 331, Train Loss: 2.8347945772111416e-05, Learning Rate: 8.145062224684807e-07\n",
      "Part: 4/4, Epoch: 70/100, Total Epochs: 332, Train Loss: 2.8121619834564626e-05, Learning Rate: 8.145062224684807e-07\n",
      "Part: 4/4, Epoch: 71/100, Total Epochs: 333, Train Loss: 2.8130591090302914e-05, Learning Rate: 8.145062224684807e-07\n",
      "Part: 4/4, Epoch: 72/100, Total Epochs: 334, Train Loss: 2.799362118821591e-05, Learning Rate: 7.737808687124925e-07\n",
      "Part: 4/4, Epoch: 73/100, Total Epochs: 335, Train Loss: 2.768471858871635e-05, Learning Rate: 7.737808687124925e-07\n",
      "Part: 4/4, Epoch: 74/100, Total Epochs: 336, Train Loss: 2.768843296507839e-05, Learning Rate: 7.737808687124925e-07\n",
      "Part: 4/4, Epoch: 75/100, Total Epochs: 337, Train Loss: 2.7516020054463297e-05, Learning Rate: 7.737808687124925e-07\n",
      "Part: 4/4, Epoch: 76/100, Total Epochs: 338, Train Loss: 2.7602631234913133e-05, Learning Rate: 7.737808687124925e-07\n",
      "Part: 4/4, Epoch: 77/100, Total Epochs: 339, Train Loss: 2.741283788054716e-05, Learning Rate: 7.737808687124925e-07\n",
      "Part: 4/4, Epoch: 78/100, Total Epochs: 340, Train Loss: 2.7408712412579916e-05, Learning Rate: 7.737808687124925e-07\n",
      "Losses and learning rates saved at epoch 340\n",
      "Part: 4/4, Epoch: 79/100, Total Epochs: 341, Train Loss: 2.7325519113219343e-05, Learning Rate: 7.737808687124925e-07\n",
      "Part: 4/4, Epoch: 80/100, Total Epochs: 342, Train Loss: 2.729306106630247e-05, Learning Rate: 7.737808687124925e-07\n",
      "Part: 4/4, Epoch: 81/100, Total Epochs: 343, Train Loss: 2.7212319764657877e-05, Learning Rate: 7.737808687124925e-07\n",
      "Part: 4/4, Epoch: 82/100, Total Epochs: 344, Train Loss: 2.7150343157700263e-05, Learning Rate: 7.737808687124925e-07\n",
      "Part: 4/4, Epoch: 83/100, Total Epochs: 345, Train Loss: 2.712623063416686e-05, Learning Rate: 7.737808687124925e-07\n",
      "Part: 4/4, Epoch: 84/100, Total Epochs: 346, Train Loss: 2.7042804504162632e-05, Learning Rate: 7.737808687124925e-07\n",
      "Part: 4/4, Epoch: 85/100, Total Epochs: 347, Train Loss: 2.7009897166863084e-05, Learning Rate: 7.737808687124925e-07\n",
      "Part: 4/4, Epoch: 86/100, Total Epochs: 348, Train Loss: 2.6936015274259262e-05, Learning Rate: 7.737808687124925e-07\n",
      "Part: 4/4, Epoch: 87/100, Total Epochs: 349, Train Loss: 2.691497684281785e-05, Learning Rate: 7.737808687124925e-07\n",
      "Part: 4/4, Epoch: 88/100, Total Epochs: 350, Train Loss: 2.685067920538131e-05, Learning Rate: 7.737808687124925e-07\n",
      "Losses and learning rates saved at epoch 350\n",
      "Part: 4/4, Epoch: 89/100, Total Epochs: 351, Train Loss: 2.6796813472174108e-05, Learning Rate: 7.737808687124925e-07\n",
      "Part: 4/4, Epoch: 90/100, Total Epochs: 352, Train Loss: 2.676687108760234e-05, Learning Rate: 7.737808687124925e-07\n",
      "Part: 4/4, Epoch: 91/100, Total Epochs: 353, Train Loss: 2.6711140890256502e-05, Learning Rate: 7.737808687124925e-07\n",
      "Part: 4/4, Epoch: 92/100, Total Epochs: 354, Train Loss: 2.667248190846294e-05, Learning Rate: 7.737808687124925e-07\n",
      "Part: 4/4, Epoch: 93/100, Total Epochs: 355, Train Loss: 2.6616484319674782e-05, Learning Rate: 7.737808687124925e-07\n",
      "Part: 4/4, Epoch: 94/100, Total Epochs: 356, Train Loss: 2.6591404093778692e-05, Learning Rate: 7.737808687124925e-07\n",
      "Part: 4/4, Epoch: 95/100, Total Epochs: 357, Train Loss: 2.6538618840277195e-05, Learning Rate: 7.737808687124925e-07\n",
      "Part: 4/4, Epoch: 96/100, Total Epochs: 358, Train Loss: 2.6498868464841507e-05, Learning Rate: 7.737808687124925e-07\n",
      "Part: 4/4, Epoch: 97/100, Total Epochs: 359, Train Loss: 2.6462608730071224e-05, Learning Rate: 7.737808687124925e-07\n",
      "Part: 4/4, Epoch: 98/100, Total Epochs: 360, Train Loss: 2.6427960619912483e-05, Learning Rate: 7.737808687124925e-07\n",
      "Losses and learning rates saved at epoch 360\n",
      "Part: 4/4, Epoch: 99/100, Total Epochs: 361, Train Loss: 2.6388297555968165e-05, Learning Rate: 7.737808687124925e-07\n",
      "Part: 4/4, Epoch: 100/100, Total Epochs: 362, Train Loss: 2.6350424377596937e-05, Learning Rate: 7.737808687124925e-07\n",
      "Model and weights saved at epoch 362\n",
      "Losses and learning rates saved at epoch 362\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "EPOCHS = 100\n",
    "num_parts = 4\n",
    "part_size = len(X_train_0_grouped) // num_parts\n",
    "print(f\"Length of X_train_0_grouped: {len(X_train_0_grouped)}\")\n",
    "print(f\"part size: {part_size}\")\n",
    "\n",
    "# Lists to store the mean train and validation losses for each epoch\n",
    "all_train_losses = []\n",
    "all_train_predicted_values = []\n",
    "# all_train_unified_depolar = []\n",
    "all_train_X_depolar = []\n",
    "all_train_Y_depolar = []\n",
    "all_train_predicted_X_overrotation = []\n",
    "all_train_predicted_Y_overrotation = []\n",
    "# all_val_losses = []\n",
    "\n",
    "# List to store learning rates\n",
    "learning_rates = []\n",
    "\n",
    "total_epochs_elapsed = 0  # Counter for the total number of epochs elapsed\n",
    "\n",
    "end_idx = 0  # Initialize end_idx to zero\n",
    "\n",
    "\n",
    "for part in range(num_parts):\n",
    "    # Determine the dataset subset for the current part\n",
    "    start_idx = part * part_size\n",
    "    end_idx = (part + 1) * part_size  # Exclusive\n",
    "    \n",
    "    print(f\"start_idx: {start_idx}\")\n",
    "    print(f\"end_idx: {end_idx}\")\n",
    "    \n",
    "    # end_idx += part_size  # Increment the end_idx by part_size\n",
    "\n",
    "    X_subset = X_train_0_grouped[start_idx:end_idx]\n",
    "    print(f\"Shape of X_subset: {X_subset.shape}\")\n",
    "\n",
    "    y_subset = y_train_grouped[start_idx:end_idx]\n",
    "    print(f\"Shape of y_subset: {y_subset.shape}\")\n",
    "\n",
    "    X_subset_ms = [X_train_grouped[0][start_idx:end_idx], X_train_grouped[1][start_idx:end_idx]]\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_losses_per_epoch = []\n",
    "        predicted_values_per_epoch = np.zeros(4) # Initialize zeros array, number of zeros corresponds to model output dimension \n",
    "        predicted_values_counter_per_epoch = 0 \n",
    "        # val_losses_per_epoch = []\n",
    "\n",
    "        for i in range(len(X_subset)):\n",
    "            X_batch = [X_subset[i:i + 1], y_subset[i:i + 1]]\n",
    "            X_batch_ms = [X_subset_ms[0][i:i + 1], X_subset_ms[1][i:i + 1]]\n",
    "            y_batch = y_subset[i:i + 1]\n",
    "\n",
    "            # train_loss, train_predicted_values = train_step(X_batch, y_batch, X_batch_ms)\n",
    "            train_loss, train_predicted_values = train_step(X_batch, y_batch, X_batch)\n",
    "            \n",
    "            \n",
    "            # val_loss = validate_step([X_test_0_grouped, y_test_grouped], y_test_grouped, [X_test_grouped[0], X_test_grouped[1]])\n",
    "            train_losses_per_epoch.append(train_loss)\n",
    "            # print(f\"current_predicted_values: {train_predicted_values}\")\n",
    "            predicted_values_per_epoch = predicted_values_per_epoch + np.array(train_predicted_values)\n",
    "            predicted_values_counter_per_epoch += 1\n",
    "            # print(f\"predicted_values_per_epoch: {predicted_values_per_epoch}\")\n",
    "            \n",
    "                \n",
    "            # val_losses_per_epoch.append(val_loss)\n",
    "\n",
    "        mean_train_loss = np.mean(train_losses_per_epoch)\n",
    "        mean_predicted_values = predicted_values_per_epoch/predicted_values_counter_per_epoch\n",
    "        # print(f\"predicted_values_counter_per_epoch: {predicted_values_counter_per_epoch}\")\n",
    "        # print(f\"mean_predicted_values: {mean_predicted_values}\")\n",
    "        # mean_val_loss = np.mean(val_losses_per_epoch)\n",
    "\n",
    "        # Store the mean losses for this epoch\n",
    "        all_train_losses.append(mean_train_loss)\n",
    "        all_train_predicted_values.append(mean_predicted_values)\n",
    "        \n",
    "        all_train_X_depolar.append(mean_predicted_values[0])\n",
    "        all_train_Y_depolar.append(mean_predicted_values[1])\n",
    "        all_train_predicted_X_overrotation.append(mean_predicted_values[2])\n",
    "        all_train_predicted_Y_overrotation.append(mean_predicted_values[3])\n",
    "            \n",
    "        \n",
    "        # all_val_losses.append(mean_val_loss)\n",
    "\n",
    "        # Record the learning rate at this epoch\n",
    "        current_step = model.optimizer.iterations\n",
    "        current_lr = lr_schedule(current_step).numpy()\n",
    "        learning_rates.append(current_lr)\n",
    "\n",
    "        total_epochs_elapsed += 1\n",
    "        # print(f\"Part: {part+1}/{num_parts}, Epoch: {epoch+1}/{EPOCHS}, Total Epochs: {total_epochs_elapsed}, Train Loss: {mean_train_loss}, Validation Loss: {mean_val_loss}, Learning Rate: {current_lr}\")\n",
    "        print(f\"Part: {part+1}/{num_parts}, Epoch: {epoch+1}/{EPOCHS}, Total Epochs: {total_epochs_elapsed}, Train Loss: {mean_train_loss}, Learning Rate: {current_lr}\")\n",
    "        if (total_epochs_elapsed % 10 == 0) or (total_epochs_elapsed == 1) or (epoch+1 == EPOCHS):\n",
    "            model_path = os.path.join(model_save_dir, f\"model_epoch_{total_epochs_elapsed}\")\n",
    "            weights_path = os.path.join(model_save_dir, f\"weights_epoch_{total_epochs_elapsed}\")\n",
    "            # tf.debugging.disable_traceback_filtering()\n",
    "            # model.save(model_path)\n",
    "            if (total_epochs_elapsed % 100 == 0) or (epoch+1 == EPOCHS):\n",
    "                model.save(f\"{model_path}.keras\", save_format='keras')\n",
    "            # model.save_weights(weights_path)\n",
    "                print(f\"Model and weights saved at epoch {total_epochs_elapsed}\")          \n",
    "\n",
    "            loss_lr_df = pd.DataFrame({\n",
    "                'Epoch': list(range(1, len(all_train_losses) + 1)),\n",
    "                'Training_Loss': all_train_losses,\n",
    "                'Learning_Rate': learning_rates # Include learning rates\n",
    "            })\n",
    "            \n",
    "            predicted_values_df = pd.DataFrame({\n",
    "                'Epoch': list(range(1, len(all_train_predicted_values) + 1)),\n",
    "                'Predicted_Values': all_train_predicted_values,\n",
    "                'X_Depolarizing_Error': all_train_X_depolar,\n",
    "                'Y_Depolarizing_Error': all_train_Y_depolar,\n",
    "                'X_Over_Rotation_Error': all_train_predicted_X_overrotation,\n",
    "                'Y_Over_Rotation_Error':all_train_predicted_Y_overrotation\n",
    "            })\n",
    "            \n",
    "            # Save the DataFrame to a CSV file\n",
    "            loss_lr_df.to_csv(f'losses_and_lr_epoch_{total_epochs_elapsed}.csv', index=False)\n",
    "            predicted_values_df.to_csv(f'predicted_values_epoch_{total_epochs_elapsed}.csv', index=False)\n",
    "\n",
    "            print(f\"Losses and learning rates saved at epoch {total_epochs_elapsed}\")\n",
    "            \n",
    "\n",
    "        if (part+1) != num_parts:\n",
    "            if mean_train_loss <= 2.5e-5:\n",
    "                print(f\"mean_train_loss: {mean_train_loss} <= 2.5e-5, skip to next stage.\")\n",
    "                break\n",
    "                \n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save the entire model as a `.keras` zip archive.\n",
    "model.save('my_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_saved_model = tf.keras.models.load_model('my_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "saved_model = tf.keras.models.load_model(r\"C:\\Users\\ALAN\\Documents\\ML4GST\\saved_models\\model_epoch_600.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "random_data_x = np.random.rand(1,20,4,4)\n",
    "random_data_y = np.random.rand(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model([random_data_x, random_data_y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "saved_model([random_data_x, random_data_y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "saved_model.optimizer.iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a new Adam optimizer with the desired learning rate\n",
    "new_learning_rate = 1e-6\n",
    "new_optimizer = tf.keras.optimizers.Adam(learning_rate=new_learning_rate)\n",
    "\n",
    "# Compile the model with the new optimizer\n",
    "saved_model.compile(optimizer=new_optimizer, loss='mean_squared_error')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "saved_model.optimizer.learning_rate.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 262
    },
    "id": "aL6iMAOhH2nj",
    "outputId": "31d27df0-8b1e-4084-cd8b-c30e48a9a5e6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plotting training and validation losses\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(all_train_losses, label='Training Loss')\n",
    "# plt.plot(all_val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Losses')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(learning_rates)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.title('Custom Learning Rate Schedule')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f0vf6jwkNtYD",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Convert the recorded losses into a DataFrame\n",
    "# loss_df = pd.DataFrame({\n",
    "#     'Epoch': list(range(1, len(all_train_losses) + 1)),\n",
    "#     'Training_Loss': all_train_losses,\n",
    "#     'Validation_Loss': all_val_losses\n",
    "# })\n",
    "\n",
    "# # Save the DataFrame to a CSV file\n",
    "# loss_df.to_csv('losses.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vd-mE6MJH48G",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to sample part of the training data and evaluate model's performance\n",
    "def sample_and_evaluate(num_samples=13):\n",
    "    indices = np.random.choice(len(X_train_grouped[0]), size=num_samples, replace=False)\n",
    "    # indices = [i for i in range(5)]\n",
    "    sampled_X = [tf.cast(X_train_0_grouped[indices], dtype=tf.float32), tf.cast(y_train_grouped[indices], dtype=tf.float32)]\n",
    "    sampled_y = tf.cast(y_train_grouped[indices], dtype=tf.float32)\n",
    "    sampled_X_ms = [tf.cast(X_train_grouped[0][indices], dtype=tf.float32), tf.cast(X_train_grouped[1][indices], dtype=tf.float32)]\n",
    "    loss = validate_step(sampled_X, sampled_y, sampled_X_ms, print_results = True)\n",
    "    print(f\"Loss on sampled data: {loss.numpy()}\")\n",
    "    return loss.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wUOTaXO3Mhvq",
    "outputId": "541c2f4a-8e69-48e8-bfc9-a83471375282",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# X_train[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aurMt4AwNa40",
    "outputId": "550fcb81-85ed-40a6-fd52-de67b6154c83",
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_and_evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to sample part of the training data and evaluate model's performance\n",
    "def sample_and_evaluate_single(num_samples=13):\n",
    "    # indices = np.random.choice(len(X_train_grouped[0]), size=num_samples, replace=False)\n",
    "    indices = [28]\n",
    "    sampled_X = [tf.cast(X_train_0_grouped[indices], dtype=tf.float32), tf.cast(y_train_grouped[indices], dtype=tf.float32)]\n",
    "    sampled_y = tf.cast(y_train_grouped[indices], dtype=tf.float32)\n",
    "    sampled_X_ms = [tf.cast(X_train_grouped[0][indices], dtype=tf.float32), tf.cast(X_train_grouped[1][indices], dtype=tf.float32)]\n",
    "    loss = validate_step(sampled_X, sampled_y, sampled_X_ms, print_results = True)\n",
    "    print(f\"Loss on sampled data: {loss.numpy()}\")\n",
    "    return loss.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_and_evaluate_single()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def apply_gate_sequence_ground_truth(single_gate_sequence, single_y_label, single_gate_matrix_sequence, ground_truth_params):\n",
    "    # Initialize state in Pauli basis\n",
    "    state = tf.constant([1/math.sqrt(2), 0, 0, 1/math.sqrt(2)], dtype=tf.float32)\n",
    "\n",
    "    # print(\"Shape of single_gate_sequence:\", tf.shape(single_gate_sequence))\n",
    "    # print(\"Shape of single_y_label:\", tf.shape(single_y_label))\n",
    "    # print(\"Shape of model input single_gate_sequence:\", tf.shape(single_gate_sequence[tf.newaxis, :]))\n",
    "    # print(\"Shape of model input single_gate_sequence:\", tf.shape(single_y_label[tf.newaxis, :]))\n",
    "\n",
    "\n",
    "    # Apply each gate in the sequence\n",
    "    depol_amt_X, depol_amt_Y, over_rotation_X, over_rotation_Y  = ground_truth_params\n",
    "    reconstructed_gate_set = custom_gate_set(depol_amt_X, over_rotation_X, depol_amt_Y, over_rotation_Y)\n",
    "    for i in range(tf.shape(single_gate_sequence)[0]):\n",
    "        # print(f'single_gate_sequence{i}: {single_gate_sequence[i]}')\n",
    "        if single_gate_sequence[i] == 0:\n",
    "            break\n",
    "        state = apply_gate(state, reconstructed_gate_set, single_gate_sequence[i])\n",
    "\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def validate_step_ground_truth(X, y, X_ms, ground_truth_params, print_results = False):\n",
    "    val_losses = []  # Initialize a list to collect the losses for each group in the mini-batch\n",
    "    \n",
    "    # Initialize a list for printing and evaluating results\n",
    "    batched_values = []\n",
    "    batched_sum_depol_amt_X = 0\n",
    "    batched_sum_over_rotation_X = 0\n",
    "    batched_sum_depol_amt_Y = 0\n",
    "    batched_sum_over_rotation_Y = 0\n",
    "\n",
    "    # # Compute model output for the entire mini-batch here\n",
    "    # full_batch_model_output = model([X_ms[0], X[1]])\n",
    "\n",
    "    # Loop over each group in the mini-batch\n",
    "    for i in range(tf.shape(X[0])[0]):\n",
    "        single_group_gate_sequence = tf.gather(X[0], i, axis=0)\n",
    "        single_group_y_label = tf.gather(X[1], i, axis=0)\n",
    "        single_group_gate_matrix_sequence = tf.gather(X_ms[0], i, axis=0)\n",
    "\n",
    "        # Gather the model's output for the i-th group from the full batch output\n",
    "        # single_group_model_output = tf.gather(full_batch_model_output, i, axis=0)\n",
    "        single_group_model_output = ground_truth_params\n",
    "\n",
    "        # Process a single group\n",
    "        final_states = apply_gate_sequence(single_group_gate_sequence, single_group_y_label, single_group_gate_matrix_sequence, ground_truth_params)\n",
    "        probabilities = compute_probabilities(final_states)\n",
    "        \n",
    "        if print_results == True:\n",
    "            depol_amt_X, depol_amt_Y, over_rotation_X, over_rotation_Y = ground_truth_params\n",
    "            batched_values.append(tf.stack([depol_amt_X, over_rotation_X, depol_amt_Y, over_rotation_Y]))\n",
    "            batched_sum_depol_amt_X += depol_amt_X\n",
    "            batched_sum_over_rotation_X += over_rotation_X\n",
    "            batched_sum_depol_amt_Y += depol_amt_Y\n",
    "            batched_sum_over_rotation_Y += over_rotation_Y\n",
    "        \n",
    "        \n",
    "        print(f\"Ground truth probabilities: {tf.gather(y, i, axis=0)}\")\n",
    "        print(f\"Computed probabilities: {probabilities}\")\n",
    "        # Calculate loss for this group\n",
    "        loss = loss_fn(tf.gather(y, i, axis=0), probabilities)\n",
    "        val_losses.append(loss)\n",
    "\n",
    "    # Average the losses across the mini-batch\n",
    "    average_loss = tf.reduce_mean(val_losses)\n",
    "    \n",
    "    if print_results == True:\n",
    "        print('\\nbatched_values: ', batched_values)\n",
    "        print('\\nlen of batched_values: ', len(batched_values))\n",
    "        print('\\nbatched_mean_depol_amt_X: ', batched_sum_depol_amt_X/len(batched_values))\n",
    "        print('\\nbatched_over_rotation_X: ', batched_sum_over_rotation_X/len(batched_values))\n",
    "        print('\\nbatched_mean_depol_amt_Y: ', batched_sum_depol_amt_Y/len(batched_values))\n",
    "        print('\\nbatched_over_rotation_Y: ', batched_sum_over_rotation_Y/len(batched_values))\n",
    "    \n",
    "    \n",
    "    return average_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to sample part of the training data and evaluate model's performance\n",
    "def sample_and_evaluate_ground_truth(ground_truth_params, num_samples=10):\n",
    "    # indices = np.random.choice(len(X_train_grouped[0]), size=num_samples, replace=False)\n",
    "    indices = [i for i in range(0,17)]\n",
    "    print(f\"indices: {indices}\")\n",
    "    sampled_X = [tf.cast(X_train_0_grouped[indices], dtype=tf.float32), tf.cast(y_train_grouped[indices], dtype=tf.float32)]\n",
    "    sampled_y = tf.cast(y_train_grouped[indices], dtype=tf.float32)\n",
    "    print(f\"sampled_y: {sampled_y}\")\n",
    "    sampled_X_ms = [tf.cast(X_train_grouped[0][indices], dtype=tf.float32), tf.cast(X_train_grouped[1][indices], dtype=tf.float32)]\n",
    "    loss = validate_step_ground_truth(sampled_X, sampled_y, sampled_X_ms, ground_truth_list, print_results = True)\n",
    "    print(f\"Loss on sampled data: {loss.numpy()}\")\n",
    "    return loss.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ground_truth_list = [0.01, 0.01, 0.1, 0.15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_and_evaluate_ground_truth(ground_truth_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "custom_gate_set(0.01, 0.1, 0.01, 0.15)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "custom_gate_set(0.0, 0.0, 0.0, 0.0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "debug_X_pi_gate = tf.linalg.matmul(custom_gate_set(0.0, 0.0, 0.0, 0.0)[0], custom_gate_set(0.0, 0.0, 0.0, 0.0)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "debug_X_pi_gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "debug_X_2pi_gate = tf.linalg.matmul(debug_X_pi_gate, debug_X_pi_gate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "debug_X_2pi_gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.linalg.norm(debug_X_2pi_gate - np.eye(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "debug_init_state_0 = tf.constant([1/math.sqrt(2), 0, 0, 1/math.sqrt(2)], dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "debug_init_state_1 = tf.constant([1/math.sqrt(2), 0, 0, -1/math.sqrt(2)], dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "debug_final_state = tf.linalg.matmul(custom_gate_set(0.01, 0.1, 0.01, 0.15)[0], tf.reshape(debug_init_state_0, [-1,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "debug_final_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "compute_probabilities(debug_final_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "debug_final_state_2 = tf.linalg.matmul(custom_gate_set(0.0, 0.0, 0.0, 0.0)[0], tf.reshape(debug_final_state, [-1,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "compute_probabilities(debug_final_state_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "debug_init_state_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "debug_init_outer_0 = np.outer(debug_init_state_0, debug_init_state_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "debug_init_outer_1 = np.outer(debug_init_state_1, debug_init_state_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "debug_final_outer = tf.linalg.matmul(custom_gate_set(0.01, 0.1, 0.01, 0.15)[0], debug_init_outer_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "debug_final_outer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.trace(tf.linalg.matmul(debug_final_outer, debug_init_outer_0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.dot(np.squeeze(debug_final_state), debug_init_state_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.dot(np.squeeze(debug_final_state), debug_init_state_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tf.reduce_sum(np.squeeze(debug_final_state)*debug_init_state_0, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tf.reduce_sum(np.squeeze(debug_final_state)*debug_init_state_1, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tf.linalg.l2_normalize(tf.constant([1,0,0,1], dtype=tf.float32)) - debug_init_state_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.linalg.norm(debug_final_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_and_evaluate_ground_truth(ground_truth_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_train_grouped[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train_0_grouped[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fCP-Ov4QcTyl",
    "outputId": "82feba0f-61ce-4c1e-a2ab-e6ca7c2d3d37"
   },
   "outputs": [],
   "source": [
    "tf.shape(X_train)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jw0N66NJk3IN"
   },
   "outputs": [],
   "source": [
    "test_vector = np.array(\n",
    "    [[ 0.70710677],\n",
    "     [-0.7064972 ],\n",
    "     [ 0.        ],\n",
    "     [-0.02935636]], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zp_bRrqOlnbm",
    "outputId": "b2cfafa3-bc67-4983-d253-52fa87ddaeb7"
   },
   "outputs": [],
   "source": [
    "test_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LacrNsyblsfq",
    "outputId": "a81f37b0-73b8-4afb-b3d7-e897e6889fe5"
   },
   "outputs": [],
   "source": [
    "tf.squeeze(test_vector).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lmMaS4Crkokb",
    "outputId": "15ccd2cd-df57-40f9-b457-a5828f2f5e9e"
   },
   "outputs": [],
   "source": [
    "compute_probabilities(tf.constant([1, 0, 0, 1], dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dTnDDEUNkbg-",
    "outputId": "a2706368-ea03-42a4-d950-faad34fa70b2"
   },
   "outputs": [],
   "source": [
    "compute_probabilities(tf.constant([1, 0, 0, -1], dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ASCr3ie5SjJN"
   },
   "outputs": [],
   "source": [
    "# for i in range (tf.shape(X_train[:1])[0]):\n",
    "#   single_sequence = tf.gather(X_train, i, axis=0)\n",
    "#   print('single_sequence: ', single_sequence)\n",
    "#   final_state = apply_gate_sequence(single_sequence)\n",
    "#   print('final_state: ', final_state)\n",
    "#   probabilities = compute_probabilities(final_state)\n",
    "#   print('probabilities: ',probabilities )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bF4T9A-dOFuD",
    "outputId": "492d9a19-85de-4169-f308-73d24b015811"
   },
   "outputs": [],
   "source": [
    "len(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7EpcfNO0PMaL"
   },
   "outputs": [],
   "source": [
    "debug = tf.convert_to_tensor([[1, 0, 0, 0], [0, tf.math.cos(0.5), 0, tf.math.sin(0.5)],\n",
    "                           [0, 0, 1, 0], [0, -tf.math.sin(0.5), 0, tf.math.cos(0.5)]], dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DNKTrgdkRHhi",
    "outputId": "7f3f7c76-53de-476b-c10c-9999a25d3189"
   },
   "outputs": [],
   "source": [
    "debug"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
