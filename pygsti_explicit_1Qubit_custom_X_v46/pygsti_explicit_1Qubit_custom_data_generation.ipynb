{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Google Colab specific code for mounting Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# # Define the directory path on your Google Drive\n",
        "# # Replace 'Your_directory' with the actual directory\n",
        "# directory = '/content/drive/My Drive/Colab Notebooks/ML4GST/'\n",
        "\n",
        "# # Now use this directory for reading and writing data\n",
        "# data_template_filename = directory + \"dataset.txt\"\n",
        "# gst_dir = directory + \"test_gst_dir\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zd1H8cDcvaHr",
        "outputId": "a62139f8-d589-4aaa-dbe9-5d17cefcd939"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/Colab Notebooks/ML4GST_v2"
      ],
      "metadata": {
        "id": "0RJyTecyWOVa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90f5180a-4221-44a7-ad3d-080f68c30b4c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/ML4GST_v2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vDzCiN2Ru6ky",
        "outputId": "03fa8366-d1fb-4d3c-a792-35691eb5b490"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pygsti\n",
            "  Downloading pyGSTi-0.9.11.2-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (17.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.2/17.2 MB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from pygsti) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pygsti) (1.11.2)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from pygsti) (5.15.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from pygsti) (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->pygsti) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->pygsti) (2023.3.post1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly->pygsti) (8.2.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from plotly->pygsti) (23.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->pygsti) (1.16.0)\n",
            "Installing collected packages: pygsti\n",
            "Successfully installed pygsti-0.9.11.2\n"
          ]
        }
      ],
      "source": [
        "pip install pygsti"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pygsti\n",
        "import pygsti.algorithms.fiducialselection as fidsel\n",
        "import pygsti.algorithms.germselection as germsel\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "9QX8UwQAH0GZ"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def easy_PTM_depol_channel(depol_mat):\n",
        "  PTM_depol = (1-depol_mat)*np.eye(4)\n",
        "  PTM_depol[0,0] = 1\n",
        "  return PTM_depol"
      ],
      "metadata": {
        "id": "aLnvf1G05SDP"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "easy_PTM_depol_channel(0.1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ut0_mos25xfW",
        "outputId": "d36d8230-765e-4787-e6a9-889b7763c78b"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1. , 0. , 0. , 0. ],\n",
              "       [0. , 0.9, 0. , 0. ],\n",
              "       [0. , 0. , 0.9, 0. ],\n",
              "       [0. , 0. , 0. , 0.9]])"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def pauli_matrices():\n",
        "    \"\"\"Return the Pauli matrices including identity.\"\"\"\n",
        "    I = np.eye(2, dtype=complex)\n",
        "    X = np.array([[0, 1], [1, 0]], dtype=complex)\n",
        "    Y = np.array([[0, -1j], [1j, 0]], dtype=complex)\n",
        "    Z = np.array([[1, 0], [0, -1]], dtype=complex)\n",
        "    return [I, X, Y, Z]\n",
        "\n",
        "def compute_ideal_ptm(unitary):\n",
        "    \"\"\"Compute the ideal PTM from a given unitary.\"\"\"\n",
        "    paulis = pauli_matrices()\n",
        "    ptm_ideal = np.zeros((4, 4), dtype=complex)\n",
        "\n",
        "    for i in range(4):\n",
        "        for j in range(4):\n",
        "            ptm_ideal[i, j] = 0.5 * np.trace(np.dot(paulis[i], np.dot(unitary, np.dot(paulis[j], np.conjugate(unitary.T)))))\n",
        "    return ptm_ideal\n",
        "\n",
        "def general_custom_gate(theta, delta, depol_amt, gate):\n",
        "  # Parameters\n",
        "  # theta = np.pi / 2  # Example theta (45 degrees)\n",
        "  # delta = 0.1  # Over-rotational error in radians\n",
        "  # depolarizing_error = 0.01  # Depolarizing error rate\n",
        "\n",
        "  # Calculate PTM for ideal Rx(theta + delta) rotation including the over-rotational error\n",
        "  unitary_rx_adjusted = np.cos((theta + delta) / 2) * np.eye(2) - 1j * np.sin((theta + delta) / 2) * pauli_matrices()[gate]\n",
        "  ptm_adjusted_rx = compute_ideal_ptm(unitary_rx_adjusted)\n",
        "\n",
        "  # Calculate combined PTM with depolarizing error\n",
        "  ptm = np.dot(easy_PTM_depol_channel(depol_amt), ptm_adjusted_rx)\n",
        "\n",
        "  return ptm.real\n",
        "\n",
        "# print('ptm_adjusted_rx: \\n', np.round(ptm_adjusted_rx,5))\n",
        "# print('final ptm: \\n', np.round(ptm,5))\n"
      ],
      "metadata": {
        "id": "MnwejUDe6Z1H"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.round(general_custom_gate(theta=np.pi/2, delta=0.0, depol_amt=0.0, gate=1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ke6_Kc0J7rSA",
        "outputId": "7eabd6b7-2ac1-4f90-fce1-7219d1f65f1c"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1.,  0.,  0.,  0.],\n",
              "       [ 0.,  1.,  0.,  0.],\n",
              "       [ 0.,  0.,  0., -1.],\n",
              "       [ 0.,  0.,  1.,  0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Pauli Transfer Matrices for the gates\n",
        "# I = np.array([[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]])\n",
        "# X_pi_4 = np.array([[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, np.cos(np.pi/4), -np.sin(np.pi/4)], [0, 0, np.sin(np.pi/4), np.cos(np.pi/4)]])\n",
        "# Y_pi_2 = np.array([[1, 0, 0, 0], [0, np.cos(np.pi/2), 0, np.sin(np.pi/2)], [0, 0, 1, 0], [0, -np.sin(np.pi/2), 0, np.cos(np.pi/2)]])\n",
        "\n",
        "# Create the explicit model\n",
        "ideal_target_model = pygsti.models.create_explicit_model_from_expressions(\n",
        "    [('Q0',)], ['Gi', 'Gx', 'Gy'],\n",
        "    [\"I(Q0)\", \"X(pi/2,Q0)\", \"Y(pi/2,Q0)\"])\n",
        "\n",
        "class MyXPi2Operator(pygsti.modelmembers.operations.DenseOperator):\n",
        "    def __init__(self):\n",
        "        #initialize with no noise\n",
        "        super(MyXPi2Operator,self).__init__(np.identity(4,'d'), 'pp', \"densitymx\") # this is *super*-operator, so \"densitymx\"\n",
        "        self.from_vector([0, 0.1])\n",
        "\n",
        "    @property\n",
        "    def num_params(self):\n",
        "        return 2 # we have two parameters\n",
        "\n",
        "    def to_vector(self):\n",
        "        return np.array([self.depol_amt, self.over_rotation],'d') #our parameter vector\n",
        "\n",
        "    def from_vector(self, v, close=False, dirty_value=True):\n",
        "        #initialize from parameter vector v\n",
        "        self.depol_amt = v[0]\n",
        "        self.over_rotation = v[1]\n",
        "\n",
        "        # print(f'depol_amt: {self.depol_amt}, over_rotation: {self.over_rotation}')\n",
        "\n",
        "        # theta = (np.pi/4 + self.over_rotation)/2\n",
        "        # a = 1.0-self.depol_amt\n",
        "        # b = a*2*np.cos(theta)*np.sin(theta)\n",
        "        # c = a*(np.sin(theta)**2 - np.cos(theta)**2)\n",
        "\n",
        "        # print(f'a: {a}, b: {b}, c: {c}')\n",
        "\n",
        "        # ._ptr is a member of DenseOperator and is a numpy array that is\n",
        "        # the dense Pauli transfer matrix of this operator\n",
        "        # Technical note: use [:,:] instead of direct assignment so id of self._ptr doesn't change\n",
        "        # self._ptr[:,:] = np.array([[1,   0,   0,   0],\n",
        "        #                           [0,   a,   0,   0],\n",
        "        #                           [0,   0,   c,  -b],\n",
        "        #                           [0,   0,   b,   c]],'d')\n",
        "        self._ptr[:,:] = np.array(general_custom_gate(theta=np.pi/2, delta=0.1, depol_amt=0.01, gate=1), 'd')\n",
        "\n",
        "        general_custom_gate\n",
        "        self.dirty = dirty_value  # mark that parameter vector may have changed\n",
        "\n",
        "    def transform(self, S):\n",
        "        # Update self with inverse(S) * self * S (used in gauge optimization)\n",
        "        raise NotImplementedError(\"MyXPi2Operator cannot be transformed!\")\n",
        "\n",
        "\n",
        "\n",
        "class MyYPi2Operator(pygsti.modelmembers.operations.DenseOperator):\n",
        "    def __init__(self):\n",
        "        #initialize with no noise\n",
        "        super(MyYPi2Operator,self).__init__(np.identity(4,'d'), 'pp', \"densitymx\") # this is *super*-operator, so \"densitymx\"\n",
        "        self.from_vector([0, 0.1])\n",
        "\n",
        "    @property\n",
        "    def num_params(self):\n",
        "        return 2 # we have two parameters\n",
        "\n",
        "    def to_vector(self):\n",
        "        return np.array([self.depol_amt, self.over_rotation],'d') #our parameter vector\n",
        "\n",
        "    def from_vector(self, v, close=False, dirty_value=True):\n",
        "        #initialize from parameter vector v\n",
        "        self.depol_amt = v[0]\n",
        "        self.over_rotation = v[1]\n",
        "\n",
        "        # print(f'depol_amt: {self.depol_amt}, over_rotation: {self.over_rotation}')\n",
        "\n",
        "        # theta = (np.pi/4 + self.over_rotation)/2\n",
        "        # a = 1.0-self.depol_amt\n",
        "        # b = a*2*np.cos(theta)*np.sin(theta)\n",
        "        # c = a*(np.sin(theta)**2 - np.cos(theta)**2)\n",
        "\n",
        "        # print(f'a: {a}, b: {b}, c: {c}')\n",
        "\n",
        "        # ._ptr is a member of DenseOperator and is a numpy array that is\n",
        "        # the dense Pauli transfer matrix of this operator\n",
        "        # Technical note: use [:,:] instead of direct assignment so id of self._ptr doesn't change\n",
        "        # self._ptr[:,:] = np.array([[1,   0,   0,   0],\n",
        "        #                           [0,   a,   0,   0],\n",
        "        #                           [0,   0,   c,  -b],\n",
        "        #                           [0,   0,   b,   c]],'d')\n",
        "        self._ptr[:,:] = np.array(general_custom_gate(theta=np.pi/2, delta=0.15, depol_amt=0.01, gate=2), 'd')\n",
        "\n",
        "        general_custom_gate\n",
        "        self.dirty = dirty_value  # mark that parameter vector may have changed\n",
        "\n",
        "    def transform(self, S):\n",
        "        # Update self with inverse(S) * self * S (used in gauge optimization)\n",
        "        raise NotImplementedError(\"MyXPi2Operator cannot be transformed!\")\n",
        "\n",
        "import copy\n",
        "target_model = copy.deepcopy(ideal_target_model)\n",
        "target_model.operations[('Gx')] = MyXPi2Operator()\n",
        "target_model.operations[('Gy')] = MyYPi2Operator()\n",
        "print('target_model: \\n', target_model)\n",
        "print('ideal_target_model: \\n', ideal_target_model)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZLOeLti-u9tg",
        "outputId": "569921a0-a2ef-413d-d52a-a9ef40906574"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "target_model: \n",
            " rho0 = FullState with dimension 4\n",
            " 0.71   0   0 0.71\n",
            "\n",
            "\n",
            "Mdefault = UnconstrainedPOVM with effect vectors:\n",
            "0: FullPOVMEffect with dimension 4\n",
            " 0.71   0   0 0.71\n",
            "\n",
            "1: FullPOVMEffect with dimension 4\n",
            " 0.71   0   0-0.71\n",
            "\n",
            "\n",
            "\n",
            "Gi = \n",
            "FullArbitraryOp with shape (4, 4)\n",
            " 1.00   0   0   0\n",
            "   0 1.00   0   0\n",
            "   0   0 1.00   0\n",
            "   0   0   0 1.00\n",
            "\n",
            "\n",
            "Gx = \n",
            "MyXPi2Operator with shape (4, 4)\n",
            " 1.00   0   0   0\n",
            "   0 0.99   0   0\n",
            "   0   0-0.10-0.99\n",
            "   0   0 0.99-0.10\n",
            "\n",
            "\n",
            "Gy = \n",
            "MyYPi2Operator with shape (4, 4)\n",
            " 1.00   0   0   0\n",
            "   0-0.15   0 0.98\n",
            "   0   0 0.99   0\n",
            "   0-0.98   0-0.15\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "ideal_target_model: \n",
            " rho0 = FullState with dimension 4\n",
            " 0.71   0   0 0.71\n",
            "\n",
            "\n",
            "Mdefault = UnconstrainedPOVM with effect vectors:\n",
            "0: FullPOVMEffect with dimension 4\n",
            " 0.71   0   0 0.71\n",
            "\n",
            "1: FullPOVMEffect with dimension 4\n",
            " 0.71   0   0-0.71\n",
            "\n",
            "\n",
            "\n",
            "Gi = \n",
            "FullArbitraryOp with shape (4, 4)\n",
            " 1.00   0   0   0\n",
            "   0 1.00   0   0\n",
            "   0   0 1.00   0\n",
            "   0   0   0 1.00\n",
            "\n",
            "\n",
            "Gx = \n",
            "FullArbitraryOp with shape (4, 4)\n",
            " 1.00   0   0   0\n",
            "   0 1.00   0   0\n",
            "   0   0   0-1.00\n",
            "   0   0 1.00   0\n",
            "\n",
            "\n",
            "Gy = \n",
            "FullArbitraryOp with shape (4, 4)\n",
            " 1.00   0   0   0\n",
            "   0   0   0 1.00\n",
            "   0   0 1.00   0\n",
            "   0-1.00   0   0\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Automatic selection of fiducials and germs using \"laissez-faire\" method\n",
        "prepFiducials, measFiducials = fidsel.find_fiducials(ideal_target_model)\n",
        "germs = germsel.find_germs(ideal_target_model, seed = 1234)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F4rvB8JHcipz",
        "outputId": "c624f826-a0ee-4fdc-a474-7c98cd50f09c"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Length Available Fiducial List: 7\n",
            "Length Available Fiducial List Dropped Identities and Duplicates: 7\n",
            "Using GRASP algorithm.\n",
            "Preparation fiducials:\n",
            "['{}', 'Gx', 'Gy', 'GxGx']\n",
            "Score: 31.99999999999997\n",
            "Measurement fiducials:\n",
            "['{}', 'Gx', 'Gy']\n",
            "Score: 9.999999999999996\n",
            "Initial Length Available Germ List: 196\n",
            "Length Available Germ List After Deduping: 24\n",
            "Length Available Germ List After Dropping Random Fraction: 24\n",
            "Length Available Germ List After Adding Back In Forced Germs: 24\n",
            "Memory estimate of 0.0 GB for all-Jac mode.\n",
            "Memory estimate of 0.0 GB for single-Jac mode.\n",
            "Using greedy algorithm.\n",
            "Constructed germ set:\n",
            "['Gi', 'Gx', 'Gy', 'GxGy', 'GiGxGx', 'GiGyGyGx', 'GiGxGyGx', 'GiGyGxGx', 'GxGxGxGy', 'GiGxGyGxGx']\n",
            "Score: major=-34.0 minor=494007.94737039437, N: 34\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'prepFiducials: {prepFiducials} \\n measFiducials: {measFiducials} \\n germs: {germs}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dtc8CpOEjL9R",
        "outputId": "4817c3f6-d378-48fd-e487-1f5d96ab2213"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prepFiducials: [Circuit({}), Circuit(Gx), Circuit(Gy), Circuit(GxGx)] \n",
            " measFiducials: [Circuit({}), Circuit(Gx), Circuit(Gy)] \n",
            " germs: [Circuit(Gi), Circuit(Gx), Circuit(Gy), Circuit(GxGy), Circuit(GiGxGx), Circuit(GiGyGyGx), Circuit(GiGxGyGx), Circuit(GiGyGxGx), Circuit(GxGxGxGy), Circuit(GiGxGyGxGx)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate a list of circuits using the long-sequence gate set tomography (LSGST) method\n",
        "maxLengths = [2**n for n in range(5)]\n",
        "\n",
        "listOfExperiments = pygsti.circuits.create_lsgst_circuits(\n",
        "    target_model, prepFiducials, measFiducials, germs, maxLengths)\n",
        "\n",
        "# Simulate the probability outcomes of these circuits\n",
        "ds = pygsti.data.simulate_data(target_model, listOfExperiments, num_samples=1000,\n",
        "                                            sample_error=\"binomial\", seed=1234)\n",
        "# print(ds)\n",
        "\n",
        "pygsti.io.write_dataset(\"Custom_1Q_XYI_dataset_abc.txt\", ds, outcome_label_order=['0','1'])\n",
        "\n",
        "# Convert the probabilities to a DataFrame and save to a CSV file"
      ],
      "metadata": {
        "id": "uI80q6AIip7Z"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Lambda\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.layers import Concatenate\n",
        "from tensorflow.keras.models import Model\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.losses import MeanSquaredError\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Load the sorted data directly from the modified CSV\n",
        "df_sorted = pd.read_csv('Sorted_Encoded_Padded_Probabilities.csv')\n",
        "\n",
        "def prepare_data(df_part):\n",
        "    # Extracting features and labels\n",
        "    X = df_part['Padded'].apply(lambda x: [int(xi) for xi in x.strip('[]').split()]).to_list()\n",
        "    y = df_part[['Prob1', 'Prob2']].values\n",
        "\n",
        "    # Convert to numpy arrays\n",
        "    X = np.array(X)\n",
        "    y = np.array(y)\n",
        "\n",
        "    return X, y\n",
        "\n",
        "X, y = prepare_data(df_sorted)\n",
        "\n",
        "# Create new input data\n",
        "X_new = [X, y]\n",
        "\n",
        "# Split the original input (X[0]) and target labels (y) into training and test sets\n",
        "X_train_0, X_test_0, y_train, y_test = train_test_split(X_new[0], y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Manually combine the split y labels into the X data\n",
        "X_train = [np.array(X_train_0), np.array(y_train)]\n",
        "X_test = [np.array(X_test_0), np.array(y_test)]\n",
        "\n",
        "# Convert y data to numpy arrays\n",
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test)\n",
        "\n"
      ],
      "metadata": {
        "id": "6XTVMibkMAOZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ss1P_yEzuKGa",
        "outputId": "6199d20f-8891-4233-92f3-5354befe8b7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[2, 2, 2, ..., 0, 0, 0],\n",
              "       [2, 1, 0, ..., 0, 0, 0],\n",
              "       [3, 2, 2, ..., 0, 0, 0],\n",
              "       ...,\n",
              "       [2, 3, 2, ..., 0, 0, 0],\n",
              "       [2, 2, 3, ..., 0, 0, 0],\n",
              "       [2, 1, 1, ..., 0, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train[0][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nmZRwn4eppiP",
        "outputId": "1ed37778-bdb8-42e1-9430-64e1b630affc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eHAjtNHVuG-R",
        "outputId": "6be485e2-ec20-4707-e0db-e20efb7eb40d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.49 , 0.51 ],\n",
              "       [0.492, 0.508],\n",
              "       [0.526, 0.474],\n",
              "       [0.227, 0.773],\n",
              "       [0.   , 1.   ],\n",
              "       [0.922, 0.078],\n",
              "       [0.513, 0.487],\n",
              "       [0.519, 0.481],\n",
              "       [0.501, 0.499],\n",
              "       [0.715, 0.285],\n",
              "       [0.474, 0.526],\n",
              "       [0.969, 0.031],\n",
              "       [0.41 , 0.59 ],\n",
              "       [0.508, 0.492],\n",
              "       [0.519, 0.481],\n",
              "       [0.586, 0.414],\n",
              "       [0.95 , 0.05 ],\n",
              "       [0.498, 0.502],\n",
              "       [0.491, 0.509],\n",
              "       [0.644, 0.356],\n",
              "       [0.475, 0.525],\n",
              "       [0.491, 0.509],\n",
              "       [0.594, 0.406],\n",
              "       [0.294, 0.706],\n",
              "       [0.668, 0.332],\n",
              "       [1.   , 0.   ],\n",
              "       [0.525, 0.475],\n",
              "       [0.5  , 0.5  ],\n",
              "       [0.403, 0.597],\n",
              "       [0.   , 1.   ],\n",
              "       [0.501, 0.499],\n",
              "       [0.418, 0.582],\n",
              "       [0.511, 0.489],\n",
              "       [0.323, 0.677],\n",
              "       [0.   , 1.   ],\n",
              "       [0.383, 0.617],\n",
              "       [0.037, 0.963],\n",
              "       [0.983, 0.017],\n",
              "       [0.821, 0.179],\n",
              "       [0.036, 0.964],\n",
              "       [1.   , 0.   ],\n",
              "       [0.052, 0.948],\n",
              "       [0.486, 0.514],\n",
              "       [0.173, 0.827],\n",
              "       [0.466, 0.534],\n",
              "       [0.509, 0.491],\n",
              "       [0.103, 0.897],\n",
              "       [0.287, 0.713],\n",
              "       [0.835, 0.165],\n",
              "       [0.   , 1.   ],\n",
              "       [0.494, 0.506],\n",
              "       [0.894, 0.106],\n",
              "       [0.   , 1.   ],\n",
              "       [0.01 , 0.99 ],\n",
              "       [0.525, 0.475],\n",
              "       [0.485, 0.515],\n",
              "       [0.489, 0.511],\n",
              "       [0.579, 0.421],\n",
              "       [0.429, 0.571],\n",
              "       [0.48 , 0.52 ],\n",
              "       [0.481, 0.519],\n",
              "       [0.554, 0.446],\n",
              "       [0.493, 0.507],\n",
              "       [0.593, 0.407],\n",
              "       [0.937, 0.063],\n",
              "       [0.492, 0.508],\n",
              "       [0.034, 0.966],\n",
              "       [0.931, 0.069],\n",
              "       [1.   , 0.   ],\n",
              "       [0.754, 0.246],\n",
              "       [0.38 , 0.62 ],\n",
              "       [0.516, 0.484],\n",
              "       [0.386, 0.614],\n",
              "       [0.384, 0.616],\n",
              "       [0.512, 0.488],\n",
              "       [0.893, 0.107],\n",
              "       [0.517, 0.483],\n",
              "       [0.116, 0.884],\n",
              "       [0.346, 0.654],\n",
              "       [0.521, 0.479],\n",
              "       [0.778, 0.222],\n",
              "       [0.402, 0.598],\n",
              "       [0.396, 0.604],\n",
              "       [0.388, 0.612],\n",
              "       [0.326, 0.674],\n",
              "       [0.017, 0.983],\n",
              "       [0.504, 0.496],\n",
              "       [0.972, 0.028],\n",
              "       [0.271, 0.729],\n",
              "       [0.497, 0.503],\n",
              "       [0.517, 0.483],\n",
              "       [0.476, 0.524],\n",
              "       [0.829, 0.171],\n",
              "       [0.785, 0.215],\n",
              "       [0.602, 0.398],\n",
              "       [0.486, 0.514],\n",
              "       [0.598, 0.402],\n",
              "       [0.818, 0.182],\n",
              "       [0.51 , 0.49 ],\n",
              "       [0.398, 0.602],\n",
              "       [0.87 , 0.13 ],\n",
              "       [0.667, 0.333],\n",
              "       [0.533, 0.467],\n",
              "       [0.406, 0.594],\n",
              "       [0.606, 0.394],\n",
              "       [0.684, 0.316],\n",
              "       [0.48 , 0.52 ],\n",
              "       [0.   , 1.   ],\n",
              "       [0.34 , 0.66 ],\n",
              "       [0.04 , 0.96 ],\n",
              "       [0.491, 0.509],\n",
              "       [1.   , 0.   ],\n",
              "       [0.2  , 0.8  ],\n",
              "       [0.406, 0.594],\n",
              "       [0.67 , 0.33 ],\n",
              "       [0.523, 0.477],\n",
              "       [0.   , 1.   ],\n",
              "       [0.421, 0.579],\n",
              "       [0.79 , 0.21 ],\n",
              "       [0.581, 0.419],\n",
              "       [1.   , 0.   ],\n",
              "       [0.593, 0.407],\n",
              "       [1.   , 0.   ],\n",
              "       [0.52 , 0.48 ],\n",
              "       [0.124, 0.876],\n",
              "       [0.481, 0.519],\n",
              "       [0.532, 0.468],\n",
              "       [0.67 , 0.33 ],\n",
              "       [0.409, 0.591],\n",
              "       [0.496, 0.504],\n",
              "       [0.   , 1.   ],\n",
              "       [0.967, 0.033],\n",
              "       [0.081, 0.919],\n",
              "       [0.427, 0.573],\n",
              "       [0.342, 0.658],\n",
              "       [0.189, 0.811],\n",
              "       [0.39 , 0.61 ],\n",
              "       [0.   , 1.   ],\n",
              "       [0.484, 0.516],\n",
              "       [0.   , 1.   ],\n",
              "       [1.   , 0.   ],\n",
              "       [0.512, 0.488],\n",
              "       [0.409, 0.591],\n",
              "       [0.529, 0.471],\n",
              "       [0.322, 0.678],\n",
              "       [0.483, 0.517],\n",
              "       [1.   , 0.   ],\n",
              "       [0.716, 0.284],\n",
              "       [0.49 , 0.51 ],\n",
              "       [0.08 , 0.92 ],\n",
              "       [0.321, 0.679],\n",
              "       [0.513, 0.487],\n",
              "       [1.   , 0.   ],\n",
              "       [0.184, 0.816],\n",
              "       [0.435, 0.565],\n",
              "       [0.338, 0.662],\n",
              "       [1.   , 0.   ],\n",
              "       [0.472, 0.528],\n",
              "       [0.875, 0.125],\n",
              "       [0.494, 0.506],\n",
              "       [1.   , 0.   ],\n",
              "       [0.485, 0.515],\n",
              "       [0.007, 0.993],\n",
              "       [0.412, 0.588],\n",
              "       [0.919, 0.081],\n",
              "       [0.506, 0.494],\n",
              "       [0.631, 0.369],\n",
              "       [0.398, 0.602],\n",
              "       [0.993, 0.007],\n",
              "       [0.036, 0.964],\n",
              "       [0.496, 0.504],\n",
              "       [0.49 , 0.51 ],\n",
              "       [0.03 , 0.97 ],\n",
              "       [0.47 , 0.53 ],\n",
              "       [0.042, 0.958],\n",
              "       [0.517, 0.483],\n",
              "       [0.509, 0.491],\n",
              "       [0.223, 0.777],\n",
              "       [0.8  , 0.2  ],\n",
              "       [0.   , 1.   ],\n",
              "       [0.484, 0.516],\n",
              "       [0.917, 0.083],\n",
              "       [0.   , 1.   ],\n",
              "       [0.983, 0.017],\n",
              "       [0.941, 0.059],\n",
              "       [0.   , 1.   ],\n",
              "       [0.   , 1.   ],\n",
              "       [0.04 , 0.96 ],\n",
              "       [0.877, 0.123],\n",
              "       [0.03 , 0.97 ],\n",
              "       [0.32 , 0.68 ],\n",
              "       [0.499, 0.501],\n",
              "       [0.039, 0.961],\n",
              "       [0.5  , 0.5  ],\n",
              "       [0.067, 0.933],\n",
              "       [0.405, 0.595],\n",
              "       [0.502, 0.498],\n",
              "       [0.815, 0.185],\n",
              "       [0.58 , 0.42 ],\n",
              "       [0.48 , 0.52 ],\n",
              "       [0.395, 0.605],\n",
              "       [1.   , 0.   ],\n",
              "       [0.483, 0.517],\n",
              "       [0.508, 0.492],\n",
              "       [0.431, 0.569],\n",
              "       [0.   , 1.   ],\n",
              "       [0.525, 0.475],\n",
              "       [0.263, 0.737],\n",
              "       [0.961, 0.039],\n",
              "       [0.489, 0.511],\n",
              "       [0.089, 0.911],\n",
              "       [0.782, 0.218],\n",
              "       [0.861, 0.139],\n",
              "       [0.172, 0.828],\n",
              "       [0.504, 0.496],\n",
              "       [0.49 , 0.51 ],\n",
              "       [0.486, 0.514],\n",
              "       [0.116, 0.884],\n",
              "       [0.49 , 0.51 ],\n",
              "       [0.516, 0.484],\n",
              "       [0.196, 0.804],\n",
              "       [0.504, 0.496],\n",
              "       [0.788, 0.212],\n",
              "       [0.   , 1.   ],\n",
              "       [0.813, 0.187],\n",
              "       [0.374, 0.626],\n",
              "       [0.   , 1.   ],\n",
              "       [0.511, 0.489],\n",
              "       [0.999, 0.001],\n",
              "       [0.   , 1.   ],\n",
              "       [0.523, 0.477],\n",
              "       [0.   , 1.   ],\n",
              "       [0.499, 0.501],\n",
              "       [0.   , 1.   ],\n",
              "       [0.028, 0.972],\n",
              "       [0.974, 0.026],\n",
              "       [0.567, 0.433],\n",
              "       [0.376, 0.624],\n",
              "       [0.495, 0.505],\n",
              "       [0.52 , 0.48 ],\n",
              "       [0.329, 0.671],\n",
              "       [0.7  , 0.3  ],\n",
              "       [0.713, 0.287],\n",
              "       [0.   , 1.   ],\n",
              "       [0.   , 1.   ],\n",
              "       [0.651, 0.349],\n",
              "       [0.54 , 0.46 ],\n",
              "       [0.017, 0.983],\n",
              "       [0.986, 0.014],\n",
              "       [0.398, 0.602],\n",
              "       [0.492, 0.508],\n",
              "       [0.476, 0.524],\n",
              "       [0.178, 0.822],\n",
              "       [0.675, 0.325],\n",
              "       [0.001, 0.999],\n",
              "       [0.123, 0.877],\n",
              "       [0.   , 1.   ],\n",
              "       [0.04 , 0.96 ],\n",
              "       [0.518, 0.482],\n",
              "       [0.872, 0.128],\n",
              "       [0.81 , 0.19 ],\n",
              "       [0.607, 0.393],\n",
              "       [0.5  , 0.5  ],\n",
              "       [0.499, 0.501],\n",
              "       [0.498, 0.502],\n",
              "       [0.043, 0.957],\n",
              "       [0.502, 0.498],\n",
              "       [0.375, 0.625],\n",
              "       [0.499, 0.501],\n",
              "       [0.39 , 0.61 ],\n",
              "       [1.   , 0.   ],\n",
              "       [0.217, 0.783]])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train[1][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKV11cBruTr6",
        "outputId": "809134d7-9525-4f66-c81d-0184302a7118"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.49, 0.51])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomLearningRateScheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, initial_learning_rate, decay_steps, decay_rate):\n",
        "        super().__init__()\n",
        "        self.initial_learning_rate = initial_learning_rate\n",
        "        self.decay_steps = decay_steps\n",
        "        self.decay_rate = decay_rate\n",
        "\n",
        "    def __call__(self, step):\n",
        "        step = tf.cast(step, dtype=tf.float32)\n",
        "        return self.initial_learning_rate / (1 + self.decay_rate * step / self.decay_steps)\n",
        "\n",
        "    def get_config(self):\n",
        "        return {\n",
        "            \"initial_learning_rate\": self.initial_learning_rate,\n",
        "            \"decay_steps\": self.decay_steps,\n",
        "            \"decay_rate\": self.decay_rate\n",
        "        }\n",
        "\n",
        "\n",
        "# class CustomLearningRateScheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "\n",
        "#   def __init__(self, initial_learning_rate):\n",
        "#     self.initial_learning_rate = initial_learning_rate\n",
        "\n",
        "#   def __call__(self, step):\n",
        "#      return self.initial_learning_rate / (step + 1)\n",
        "\n",
        "# Define model\n",
        "# model = Sequential([\n",
        "#     Dense(20, activation='relu', input_shape=(len(X_train[0]),)),\n",
        "#     Dense(128, activation='relu'),\n",
        "#     Dense(64, activation='relu'),\n",
        "#     Dense(2, activation='sigmoid'),  # Sigmoid function outputs in the range [0, 1]\n",
        "# ])\n",
        "# model = Sequential([\n",
        "#     Dense(20, activation='relu', input_shape=(len(X_train[0]),)),\n",
        "#     Dense(128, activation='relu'),\n",
        "#     Dense(64, activation='relu'),\n",
        "#     Dense(2, activation='relu'),\n",
        "# ])\n",
        "\n",
        "# # Scale model output to range [0, 2pi]\n",
        "# scaling_layer = Lambda(lambda x: x * 2 * math.pi)\n",
        "# model.add(scaling_layer)\n",
        "\n",
        "# Define model architecture\n",
        "input_X = Input(shape=(len(X_train[0][0]),), name='input_X')\n",
        "input_y = Input(shape=(len(X_train[1][0]),), name='input_y')\n",
        "\n",
        "# Original branch\n",
        "x1 = Dense(20, activation='relu')(input_X)\n",
        "x1 = Dense(128, activation='relu')(x1)\n",
        "\n",
        "# New branch\n",
        "x2 = Dense(2, activation='relu')(input_y)\n",
        "x2 = Dense(64, activation='relu')(x2)\n",
        "\n",
        "# Concatenate branches\n",
        "merged = Concatenate()([x1, x2])\n",
        "merged = Dense(64, activation='relu')(merged)\n",
        "merged = Dense(2, activation='relu')(merged)\n",
        "\n",
        "# Create and compile model\n",
        "model = Model(inputs=[input_X, input_y], outputs=merged)\n",
        "\n",
        "# Compile model\n",
        "\n",
        "initial_learning_rate = 1e-3\n",
        "decay_steps = 1000\n",
        "decay_rate = 0.01\n",
        "lr_schedule = CustomLearningRateScheduler(initial_learning_rate, decay_steps, decay_rate)\n",
        "# lr_schedule = CustomLearningRateScheduler(initial_learning_rate)\n",
        "optimizer = Adam(learning_rate=lr_schedule)\n",
        "\n",
        "model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
        "\n",
        "# Custom X gate\n",
        "def custom_X(depol_amt, over_rotation):\n",
        "\n",
        "  # print(f'depol_amt: {self.depol_amt}, over_rotation: {self.over_rotation}')\n",
        "\n",
        "  theta = (math.pi/4 + over_rotation)/2\n",
        "  a = 1.0-depol_amt\n",
        "  b = a*2*tf.math.cos(theta)*tf.math.sin(theta)\n",
        "  c = a*(tf.math.sin(theta)**2 - tf.math.cos(theta)**2)\n",
        "\n",
        "  # print(f'a: {a}, b: {b}, c: {c}')\n",
        "\n",
        "  # ._ptr is a member of DenseOperator and is a numpy array that is\n",
        "  # the dense Pauli transfer matrix of this operator\n",
        "  # Technical note: use [:,:] instead of direct assignment so id of self._ptr doesn't change\n",
        "  custom_X_arr = tf.convert_to_tensor([[1,   0,   0,   0],\n",
        "                            [0,   a,   0,   0],\n",
        "                            [0,   0,   c,  -b],\n",
        "                            [0,   0,   b,   c]], dtype=tf.float32)\n",
        "\n",
        "  return custom_X_arr\n",
        "\n",
        "\n",
        "# Define gate application function\n",
        "def apply_gate(state, depol_amt, over_rotation, label):\n",
        "    # Construct arrays using NumPy\n",
        "    # Define gates in PTM form\n",
        "    I_np = np.array([[1, 0, 0, 0],\n",
        "                    [0, 1, 0, 0],\n",
        "                    [0, 0, 1, 0],\n",
        "                    [0, 0, 0, 1]], dtype=np.float32)  #Gi\n",
        "\n",
        "    Y_pi_2_np = np.array([[1, 0, 0, 0],\n",
        "                          [0, np.cos(np.pi/2), 0, np.sin(np.pi/2)],\n",
        "                          [0, 0, 1, 0],\n",
        "                          [0, -np.sin(np.pi/2), 0, np.cos(np.pi/2)]], dtype=np.float32) #Gy\n",
        "\n",
        "    # Normalized State corresponding to |0⟩ in Pauli basis\n",
        "    # state_np = np.array([1/np.sqrt(2), 0, 0, 1/np.sqrt(2)], dtype=np.float32)\n",
        "\n",
        "    # Convert NumPy arrays to TensorFlow tensors\n",
        "    I = tf.convert_to_tensor(I_np)\n",
        "    Y_pi_2 = tf.convert_to_tensor(Y_pi_2_np)\n",
        "    # state = tf.convert_to_tensor(state_np)\n",
        "\n",
        "    # X_theta = tf.convert_to_tensor([[1, 0, 0, 0], [0, tf.math.cos(theta_value), 0, tf.math.sin(theta_value)],\n",
        "    #                        [0, 0, 1, 0], [0, -tf.math.sin(theta_value), 0, tf.math.cos(theta_value)]], dtype=tf.float32)  # Gx\n",
        "\n",
        "    X_theta = custom_X(depol_amt, over_rotation)\n",
        "    # print('current label: ', label)\n",
        "\n",
        "    if label == 1:\n",
        "        return tf.linalg.matmul(X_theta, tf.reshape(state, [-1, 1]))\n",
        "    elif label == 2:\n",
        "        return tf.linalg.matmul(Y_pi_2, tf.reshape(state, [-1, 1]))\n",
        "    elif label == 3:\n",
        "        return tf.linalg.matmul(I, tf.reshape(state, [-1, 1]))\n",
        "    else:\n",
        "        return state  # If label is 0, don't apply any gate\n",
        "\n",
        "\n",
        "# def apply_gate_sequence(single_gate_sequence):\n",
        "#     # Initialize state in Pauli basis\n",
        "#     state = tf.convert_to_tensor([1/np.sqrt(2), 0, 0, 1/np.sqrt(2)], dtype=tf.float32)\n",
        "\n",
        "#   # Apply each gate in the sequence\n",
        "#     # print('model(single_gate_sequence[tf.newaxis, :]) ->', model(single_gate_sequence[tf.newaxis, :]))\n",
        "#     depol_amt, over_rotation = tf.squeeze(model(single_gate_sequence[tf.newaxis, :])) # Predict depolar_error, over_rotation for the current gate sequence\n",
        "#     # print('theta_value: ', theta_value)\n",
        "#     # depol_amt = tf.clip_by_value(tf.squeeze(depol_amt), 0, 0.1)\n",
        "#     # over_rotation = tf.clip_by_value(tf.squeeze(over_rotation), 0, 0.1)\n",
        "#     # print(f\"depol_amt: {depol_amt}, over_rotation: {over_rotation}\")\n",
        "#     # print('squeezed theta_value: ', theta_value)\n",
        "#     for i in range(tf.shape(single_gate_sequence)[0]):\n",
        "#       if single_gate_sequence[i] == 0:\n",
        "#         break\n",
        "#       # print('tf.shape(single_gate_sequence): ', tf.shape(single_gate_sequence))\n",
        "#       # print('tf.shape(single_gate_sequence[0]): ', tf.shape(single_gate_sequence)[0])\n",
        "#       # print('single_gate_sequence[i]: ', single_gate_sequence[i])\n",
        "#       state = apply_gate(state, depol_amt, over_rotation, single_gate_sequence[i])\n",
        "#       # print('current state: ', state)\n",
        "\n",
        "#     return state\n",
        "\n",
        "def apply_gate_sequence(single_gate_sequence, single_y_label):\n",
        "    # Initialize state in Pauli basis\n",
        "    state = tf.convert_to_tensor([1/np.sqrt(2), 0, 0, 1/np.sqrt(2)], dtype=tf.float32)\n",
        "\n",
        "    # print(\"Shape of single_gate_sequence:\", tf.shape(single_gate_sequence))\n",
        "    # print(\"Shape of single_y_label:\", tf.shape(single_y_label))\n",
        "    # print(\"Shape of model input single_gate_sequence:\", tf.shape(single_gate_sequence[tf.newaxis, :]))\n",
        "    # print(\"Shape of model input single_gate_sequence:\", tf.shape(single_y_label[tf.newaxis, :]))\n",
        "\n",
        "\n",
        "    # Apply each gate in the sequence\n",
        "    depol_amt, over_rotation = tf.squeeze(model([single_gate_sequence[tf.newaxis, :], single_y_label[tf.newaxis, :]])) # Predict depolar_error, over_rotation for the current gate sequence\n",
        "\n",
        "    for i in range(tf.shape(single_gate_sequence)[0]):\n",
        "        if single_gate_sequence[i] == 0:\n",
        "            break\n",
        "        state = apply_gate(state, depol_amt, over_rotation, single_gate_sequence[i])\n",
        "\n",
        "    return state\n",
        "\n",
        "\n",
        "\n",
        "def compute_probabilities(ptm_vector):\n",
        "    # PTM representations for |0> and |1> states\n",
        "    ptm_0 = tf.constant([1, 0, 0, 1], dtype=tf.float32)\n",
        "    ptm_1 = tf.constant([1, 0, 0, -1], dtype=tf.float32)\n",
        "    # ptm_0 = tf.convert_to_tensor([1, 0, 0, 1], dtype=tf.float32)\n",
        "    # ptm_1 = tf.convert_to_tensor([1, 0, 0, -1], dtype=tf.float32)\n",
        "\n",
        "    # Normalize the vectors\n",
        "    ptm_vector = tf.squeeze(tf.linalg.l2_normalize(ptm_vector))\n",
        "    ptm_0 = tf.linalg.l2_normalize(ptm_0)\n",
        "    ptm_1 = tf.linalg.l2_normalize(ptm_1)\n",
        "\n",
        "    # Compute dot products\n",
        "    prob_0 = tf.tensordot(ptm_vector, ptm_0, axes=1)\n",
        "    prob_1 = tf.tensordot(ptm_vector, ptm_1, axes=1)\n",
        "\n",
        "    return tf.stack([prob_0, prob_1])\n",
        "\n",
        "\n",
        "# Define loss function\n",
        "loss_fn = MeanSquaredError()\n",
        "\n",
        "# Define training loop\n",
        "# def train_step(X, y):\n",
        "#     with tf.GradientTape() as tape:\n",
        "#       batched_final_states = []\n",
        "#       batched_probabilities = []\n",
        "\n",
        "#       # Process each sequence in the batch individually\n",
        "#       for i in range(tf.shape(X)[0]):\n",
        "#           single_sequence = tf.gather(X, i, axis=0)\n",
        "#           final_state = apply_gate_sequence(single_sequence)\n",
        "#           probabilities = compute_probabilities(final_state)\n",
        "#           batched_final_states.append(final_state)\n",
        "#           # print('batched_final_states: ', batched_final_states)\n",
        "#           batched_probabilities.append(probabilities)\n",
        "#           # print('batched_probabilities: ', batched_probabilities)\n",
        "\n",
        "#       batched_final_states = tf.stack(batched_final_states)\n",
        "#       # print('batched_final_states: ', batched_final_states)\n",
        "#       # print('batched_final_states.shape: ', batched_final_states.shape)\n",
        "#       batched_probabilities = tf.stack(batched_probabilities)\n",
        "#       # print('batched_probabilities: ', batched_probabilities)\n",
        "#       # print('batched_probabilities.shape: ', batched_probabilities.shape)\n",
        "\n",
        "\n",
        "#       loss = loss_fn(y, batched_probabilities)\n",
        "#       # print('loss: ', loss)\n",
        "\n",
        "#     grads = tape.gradient(loss, model.trainable_weights)\n",
        "#     optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "#     return loss\n",
        "\n",
        "def train_step(X, y):\n",
        "    with tf.GradientTape() as tape:\n",
        "        batched_final_states = []\n",
        "        batched_probabilities = []\n",
        "\n",
        "        # print(\"Shape of X[0]:\", tf.shape(X[0]))\n",
        "        # print(\"Shape of X[1]:\", tf.shape(X[1]))\n",
        "        # print(\"Shape of y:\", tf.shape(y))\n",
        "\n",
        "        # Process each sequence in the batch individually\n",
        "        # print('tf.shape(X[0])[0]:', tf.shape(X[0])[0])\n",
        "        for i in range(tf.shape(X[0])[0]):\n",
        "            single_sequence = tf.gather(X[0], i, axis=0)\n",
        "            single_y_label = tf.gather(X[1], i, axis=0)\n",
        "            final_state = apply_gate_sequence(single_sequence, single_y_label)\n",
        "            probabilities = compute_probabilities(final_state)\n",
        "            batched_final_states.append(final_state)\n",
        "            batched_probabilities.append(probabilities)\n",
        "\n",
        "        batched_final_states = tf.stack(batched_final_states)\n",
        "        batched_probabilities = tf.stack(batched_probabilities)\n",
        "\n",
        "        loss = loss_fn(y, batched_probabilities)\n",
        "\n",
        "    grads = tape.gradient(loss, model.trainable_weights)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "    return loss\n",
        "\n",
        "\n",
        "# Define validation loop (similar to training loop but without gradients)\n",
        "# def validate_step(X, y, print_results = False):\n",
        "#     batched_final_states = []\n",
        "#     batched_probabilities = []\n",
        "#     # batched_thetas = []\n",
        "#     batched_errors = []\n",
        "\n",
        "#     for i in range(tf.shape(X)[0]):\n",
        "#         single_sequence = tf.gather(X, i, axis=0)\n",
        "#         final_state = apply_gate_sequence(single_sequence)\n",
        "#         probabilities = compute_probabilities(final_state)\n",
        "#         batched_final_states.append(final_state)\n",
        "#         batched_probabilities.append(probabilities)\n",
        "#         if print_results == True:\n",
        "#             theta_value = tf.squeeze(model(single_sequence[tf.newaxis, :])) # Predict theta for the current gate\n",
        "#             depol_amt, over_rotation = tf.squeeze(model(single_sequence[tf.newaxis, :])) # Predict depolar_error, over_rotation for the current gate sequence\n",
        "#             # batched_thetas.append(theta_value)\n",
        "#             batched_errors.append(tf.stack([depol_amt, over_rotation]))\n",
        "\n",
        "\n",
        "#     batched_final_states = tf.stack(batched_final_states)\n",
        "#     batched_probabilities = tf.stack(batched_probabilities)\n",
        "#     loss = loss_fn(y, batched_probabilities)\n",
        "#     if print_results == True:\n",
        "#       # print('batched_thetas: ', batched_thetas)\n",
        "#       print('batched_errors: ', batched_errors)\n",
        "#     return loss\n",
        "\n",
        "def validate_step(X, y, print_results = False):\n",
        "    batched_final_states = []\n",
        "    batched_probabilities = []\n",
        "    batched_errors = []\n",
        "\n",
        "    for i in range(tf.shape(X[0])[0]):\n",
        "        single_sequence = tf.gather(X[0], i, axis=0)\n",
        "        single_y_label = tf.gather(X[1], i, axis=0)\n",
        "        final_state = apply_gate_sequence(single_sequence, single_y_label)\n",
        "        probabilities = compute_probabilities(final_state)\n",
        "        batched_final_states.append(final_state)\n",
        "        batched_probabilities.append(probabilities)\n",
        "        if print_results == True:\n",
        "            depol_amt, over_rotation = tf.squeeze(model([single_sequence[tf.newaxis, :], single_y_label[tf.newaxis, :]])) # Predict depolar_error, over_rotation for the current gate sequence\n",
        "            batched_errors.append(tf.stack([depol_amt, over_rotation]))\n",
        "\n",
        "    batched_final_states = tf.stack(batched_final_states)\n",
        "    batched_probabilities = tf.stack(batched_probabilities)\n",
        "    loss = loss_fn(y, batched_probabilities)\n",
        "    if print_results == True:\n",
        "        print('batched_errors: ', batched_errors)\n",
        "    return loss\n"
      ],
      "metadata": {
        "id": "rSc7GUbxdydS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add these lines at the top of your code\n",
        "import os\n",
        "\n",
        "# Directory to save the model and weights\n",
        "model_save_dir = \"saved_models\"\n",
        "if not os.path.exists(model_save_dir):\n",
        "    os.makedirs(model_save_dir)"
      ],
      "metadata": {
        "id": "mUZNtlhbiL7n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "EPOCHS = 100\n",
        "BATCH_SIZE = 64\n",
        "num_parts = 3\n",
        "part_size = len(df_sorted) // num_parts\n",
        "\n",
        "total_epochs_elapsed = 0  # Counter for total number of epochs elapsed\n",
        "\n",
        "# Lists to store the mean train and validation losses for each epoch across all parts\n",
        "all_train_losses = []\n",
        "all_val_losses = []\n",
        "\n",
        "for part in range(num_parts):\n",
        "    # Determine the dataset subset for the current part\n",
        "    end_idx = (part + 1) * part_size\n",
        "    X_subset, y_subset = prepare_data(df_sorted.iloc[:end_idx])\n",
        "\n",
        "    # Convert X_subset and y_subset to a single numpy array\n",
        "    X_subset = np.array(X_subset)\n",
        "    X_subset = [X_subset, y_subset]\n",
        "    y_subset = np.array(y_subset)\n",
        "\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        train_losses_per_epoch = []\n",
        "        val_losses_per_epoch = []\n",
        "\n",
        "        for i in range(0, len(X_subset[0]), BATCH_SIZE):\n",
        "            # print('len of X_subset[0][i:i+BATCH_SIZE]: ', len(X_subset[0][i:i+BATCH_SIZE]))\n",
        "            X_batch = [X_subset[0][i:i+BATCH_SIZE], X_subset[1][i:i+BATCH_SIZE]]\n",
        "            y_batch = y_subset[i:i+BATCH_SIZE]\n",
        "\n",
        "            # Training and validation steps remain unchanged\n",
        "            train_loss = train_step(X_batch, y_batch)\n",
        "            val_loss = validate_step(X_test, y_test)\n",
        "            train_losses_per_epoch.append(train_loss)\n",
        "            val_losses_per_epoch.append(val_loss)\n",
        "\n",
        "        mean_train_loss = np.mean(train_losses_per_epoch)\n",
        "        mean_val_loss = np.mean(val_losses_per_epoch)\n",
        "\n",
        "        # Store the mean losses for this epoch\n",
        "        all_train_losses.append(mean_train_loss)\n",
        "        all_val_losses.append(mean_val_loss)\n",
        "\n",
        "        total_epochs_elapsed += 1\n",
        "        print(f\"Part: {part+1}/{num_parts}, Epoch: {epoch+1}/{EPOCHS}, Total Epochs: {total_epochs_elapsed}, Train Loss: {mean_train_loss}, Validation Loss: {mean_val_loss}\")\n",
        "\n",
        "        if total_epochs_elapsed % 50 == 0:\n",
        "          model_path = os.path.join(model_save_dir, f\"model_epoch_{total_epochs_elapsed}.h5\")\n",
        "          weights_path = os.path.join(model_save_dir, f\"weights_epoch_{total_epochs_elapsed}.h5\")\n",
        "          model.save(model_path)\n",
        "          model.save_weights(weights_path)\n",
        "\n",
        "          # model_path = os.path.join(model_save_dir, f\"model_epoch_{total_epochs_elapsed}\")\n",
        "          # weights_path = os.path.join(model_save_dir, f\"weights_epoch_{total_epochs_elapsed}\")\n",
        "\n",
        "          # # Save model in SavedModel format\n",
        "          # model.save(model_path, save_format=\"tf\")\n",
        "\n",
        "          # # Save weights in SavedModel format\n",
        "          # model.save_weights(weights_path, save_format=\"tf\")\n",
        "\n",
        "          print(f\"Model and weights saved at epoch {total_epochs_elapsed}\")\n",
        "\n",
        "        if mean_train_loss <= 1e-6 or mean_val_loss <= 1e-6:\n",
        "            print(f\"Train Loss: {mean_train_loss} <= 1e-6, skipping to next stage of training\")\n",
        "            break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w7buJL9YyU-b",
        "outputId": "0454bbd2-e940-40f6-cb11-08c353dd8401"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Part: 1/3, Epoch: 1/100, Total Epochs: 1, Train Loss: 0.0037216488271951675, Validation Loss: 0.007291092537343502\n",
            "Part: 1/3, Epoch: 2/100, Total Epochs: 2, Train Loss: 0.0036276611499488354, Validation Loss: 0.007020854856818914\n",
            "Part: 1/3, Epoch: 3/100, Total Epochs: 3, Train Loss: 0.003313925117254257, Validation Loss: 0.0040528541430830956\n",
            "Part: 1/3, Epoch: 4/100, Total Epochs: 4, Train Loss: 0.001822200370952487, Validation Loss: 0.005419028922915459\n",
            "Part: 1/3, Epoch: 5/100, Total Epochs: 5, Train Loss: 0.001263399375602603, Validation Loss: 0.0039192866533994675\n",
            "Part: 1/3, Epoch: 6/100, Total Epochs: 6, Train Loss: 0.000793521641753614, Validation Loss: 0.0038001658394932747\n",
            "Part: 1/3, Epoch: 7/100, Total Epochs: 7, Train Loss: 0.0007786608766764402, Validation Loss: 0.003943103365600109\n",
            "Part: 1/3, Epoch: 8/100, Total Epochs: 8, Train Loss: 0.00064138974994421, Validation Loss: 0.0033824550919234753\n",
            "Part: 1/3, Epoch: 9/100, Total Epochs: 9, Train Loss: 0.0005249122623354197, Validation Loss: 0.004094372503459454\n",
            "Part: 1/3, Epoch: 10/100, Total Epochs: 10, Train Loss: 0.00036097143311053514, Validation Loss: 0.0039998264983296394\n",
            "Part: 1/3, Epoch: 11/100, Total Epochs: 11, Train Loss: 0.00034764991141855717, Validation Loss: 0.00353429326787591\n",
            "Part: 1/3, Epoch: 12/100, Total Epochs: 12, Train Loss: 0.00027937215054407716, Validation Loss: 0.004478389397263527\n",
            "Part: 1/3, Epoch: 13/100, Total Epochs: 13, Train Loss: 0.0002649054513312876, Validation Loss: 0.004081500694155693\n",
            "Part: 1/3, Epoch: 14/100, Total Epochs: 14, Train Loss: 0.00027400621911510825, Validation Loss: 0.003430937882512808\n",
            "Part: 1/3, Epoch: 15/100, Total Epochs: 15, Train Loss: 0.000202561539481394, Validation Loss: 0.0038820034824311733\n",
            "Part: 1/3, Epoch: 16/100, Total Epochs: 16, Train Loss: 0.000176461529918015, Validation Loss: 0.0033736058976501226\n",
            "Part: 1/3, Epoch: 17/100, Total Epochs: 17, Train Loss: 0.00018312325119040906, Validation Loss: 0.002964599058032036\n",
            "Part: 1/3, Epoch: 18/100, Total Epochs: 18, Train Loss: 0.00017945151193998754, Validation Loss: 0.003230452071875334\n",
            "Part: 1/3, Epoch: 19/100, Total Epochs: 19, Train Loss: 0.00015570675896015018, Validation Loss: 0.0028110877610743046\n",
            "Part: 1/3, Epoch: 20/100, Total Epochs: 20, Train Loss: 0.00016182070248760283, Validation Loss: 0.0026656840927898884\n",
            "Part: 1/3, Epoch: 21/100, Total Epochs: 21, Train Loss: 0.00015946794883348048, Validation Loss: 0.0028289477340877056\n",
            "Part: 1/3, Epoch: 22/100, Total Epochs: 22, Train Loss: 0.00014994386583566666, Validation Loss: 0.002530711703002453\n",
            "Part: 1/3, Epoch: 23/100, Total Epochs: 23, Train Loss: 0.00014417825150303543, Validation Loss: 0.002569986507296562\n",
            "Part: 1/3, Epoch: 24/100, Total Epochs: 24, Train Loss: 0.0001446974347345531, Validation Loss: 0.0025716708041727543\n",
            "Part: 1/3, Epoch: 25/100, Total Epochs: 25, Train Loss: 0.00014268115046434104, Validation Loss: 0.002377364318817854\n",
            "Part: 1/3, Epoch: 26/100, Total Epochs: 26, Train Loss: 0.00013725187454838306, Validation Loss: 0.0024945198092609644\n",
            "Part: 1/3, Epoch: 27/100, Total Epochs: 27, Train Loss: 0.00013266220048535615, Validation Loss: 0.0023804185912013054\n",
            "Part: 1/3, Epoch: 28/100, Total Epochs: 28, Train Loss: 0.00013204282731749117, Validation Loss: 0.0024018471594899893\n",
            "Part: 1/3, Epoch: 29/100, Total Epochs: 29, Train Loss: 0.0001316852867603302, Validation Loss: 0.0024725832045078278\n",
            "Part: 1/3, Epoch: 30/100, Total Epochs: 30, Train Loss: 0.0001303600292885676, Validation Loss: 0.0024004741571843624\n",
            "Part: 1/3, Epoch: 31/100, Total Epochs: 31, Train Loss: 0.00012785049329977483, Validation Loss: 0.002522313967347145\n",
            "Part: 1/3, Epoch: 32/100, Total Epochs: 32, Train Loss: 0.00012590781261678785, Validation Loss: 0.002440183889120817\n",
            "Part: 1/3, Epoch: 33/100, Total Epochs: 33, Train Loss: 0.00012359002721495926, Validation Loss: 0.0025102212093770504\n",
            "Part: 1/3, Epoch: 34/100, Total Epochs: 34, Train Loss: 0.00012180508201709017, Validation Loss: 0.0024727797135710716\n",
            "Part: 1/3, Epoch: 35/100, Total Epochs: 35, Train Loss: 0.00012072594836354256, Validation Loss: 0.002488879021257162\n",
            "Part: 1/3, Epoch: 36/100, Total Epochs: 36, Train Loss: 0.00011993803491350263, Validation Loss: 0.002505711978301406\n",
            "Part: 1/3, Epoch: 37/100, Total Epochs: 37, Train Loss: 0.00011889751476701349, Validation Loss: 0.00249610748142004\n",
            "Part: 1/3, Epoch: 38/100, Total Epochs: 38, Train Loss: 0.00011880687088705599, Validation Loss: 0.0025506122037768364\n",
            "Part: 1/3, Epoch: 39/100, Total Epochs: 39, Train Loss: 0.00011871055176015943, Validation Loss: 0.0025157686322927475\n",
            "Part: 1/3, Epoch: 40/100, Total Epochs: 40, Train Loss: 0.00012108840746805072, Validation Loss: 0.0026045539416372776\n",
            "Part: 1/3, Epoch: 41/100, Total Epochs: 41, Train Loss: 0.00012746124411933124, Validation Loss: 0.002527127042412758\n",
            "Part: 1/3, Epoch: 42/100, Total Epochs: 42, Train Loss: 0.00014796393224969506, Validation Loss: 0.002724188845604658\n",
            "Part: 1/3, Epoch: 43/100, Total Epochs: 43, Train Loss: 0.00019779341528192163, Validation Loss: 0.0025281626731157303\n",
            "Part: 1/3, Epoch: 44/100, Total Epochs: 44, Train Loss: 0.0003127156523987651, Validation Loss: 0.003197498619556427\n",
            "Part: 1/3, Epoch: 45/100, Total Epochs: 45, Train Loss: 0.00044579163659363985, Validation Loss: 0.002403422724455595\n",
            "Part: 1/3, Epoch: 46/100, Total Epochs: 46, Train Loss: 0.0003857321571558714, Validation Loss: 0.0036556897684931755\n",
            "Part: 1/3, Epoch: 47/100, Total Epochs: 47, Train Loss: 0.0001613773056305945, Validation Loss: 0.002285386435687542\n",
            "Part: 1/3, Epoch: 48/100, Total Epochs: 48, Train Loss: 0.00014534269575960934, Validation Loss: 0.0026559822726994753\n",
            "Part: 1/3, Epoch: 49/100, Total Epochs: 49, Train Loss: 0.0002452107146382332, Validation Loss: 0.003178464248776436\n",
            "Part: 1/3, Epoch: 50/100, Total Epochs: 50, Train Loss: 0.00019348436035215855, Validation Loss: 0.0023223767057061195\n",
            "Model and weights saved at epoch 50\n",
            "Part: 1/3, Epoch: 51/100, Total Epochs: 51, Train Loss: 0.00011669279774650931, Validation Loss: 0.002853265032172203\n",
            "Part: 1/3, Epoch: 52/100, Total Epochs: 52, Train Loss: 0.00016110719298012555, Validation Loss: 0.002922655316069722\n",
            "Part: 1/3, Epoch: 53/100, Total Epochs: 53, Train Loss: 0.00017235004634130746, Validation Loss: 0.002405945211648941\n",
            "Part: 1/3, Epoch: 54/100, Total Epochs: 54, Train Loss: 0.00011941857519559562, Validation Loss: 0.0029362656641751528\n",
            "Part: 1/3, Epoch: 55/100, Total Epochs: 55, Train Loss: 0.0001249673223355785, Validation Loss: 0.00277915270999074\n",
            "Part: 1/3, Epoch: 56/100, Total Epochs: 56, Train Loss: 0.00014533670037053525, Validation Loss: 0.002502312883734703\n",
            "Part: 1/3, Epoch: 57/100, Total Epochs: 57, Train Loss: 0.0001234592346008867, Validation Loss: 0.002959816250950098\n",
            "Part: 1/3, Epoch: 58/100, Total Epochs: 58, Train Loss: 0.00011079513933509588, Validation Loss: 0.002686518244445324\n",
            "Part: 1/3, Epoch: 59/100, Total Epochs: 59, Train Loss: 0.00012425020395312458, Validation Loss: 0.002608762588351965\n",
            "Part: 1/3, Epoch: 60/100, Total Epochs: 60, Train Loss: 0.00012242449156474322, Validation Loss: 0.0029495684430003166\n",
            "Part: 1/3, Epoch: 61/100, Total Epochs: 61, Train Loss: 0.00010920634667854756, Validation Loss: 0.0026500322856009007\n",
            "Part: 1/3, Epoch: 62/100, Total Epochs: 62, Train Loss: 0.00011118782276753336, Validation Loss: 0.0027308138087391853\n",
            "Part: 1/3, Epoch: 63/100, Total Epochs: 63, Train Loss: 0.00011664319026749581, Validation Loss: 0.0029056710191071033\n",
            "Part: 1/3, Epoch: 64/100, Total Epochs: 64, Train Loss: 0.00011137394176330417, Validation Loss: 0.002660030033439398\n",
            "Part: 1/3, Epoch: 65/100, Total Epochs: 65, Train Loss: 0.0001073285675374791, Validation Loss: 0.0028353328816592693\n",
            "Part: 1/3, Epoch: 66/100, Total Epochs: 66, Train Loss: 0.00010857588495127857, Validation Loss: 0.002816895954310894\n",
            "Part: 1/3, Epoch: 67/100, Total Epochs: 67, Train Loss: 0.00011042591359000653, Validation Loss: 0.0026995523367077112\n",
            "Part: 1/3, Epoch: 68/100, Total Epochs: 68, Train Loss: 0.00010876254236791283, Validation Loss: 0.00286448048427701\n",
            "Part: 1/3, Epoch: 69/100, Total Epochs: 69, Train Loss: 0.00010610942263156176, Validation Loss: 0.002725426107645035\n",
            "Part: 1/3, Epoch: 70/100, Total Epochs: 70, Train Loss: 0.00010605746501823887, Validation Loss: 0.0027630641125142574\n",
            "Part: 1/3, Epoch: 71/100, Total Epochs: 71, Train Loss: 0.00010730131907621399, Validation Loss: 0.0028170368168503046\n",
            "Part: 1/3, Epoch: 72/100, Total Epochs: 72, Train Loss: 0.00010702411964302883, Validation Loss: 0.0027169904205948114\n",
            "Part: 1/3, Epoch: 73/100, Total Epochs: 73, Train Loss: 0.00010626007860992104, Validation Loss: 0.0028330162167549133\n",
            "Part: 1/3, Epoch: 74/100, Total Epochs: 74, Train Loss: 0.00010508621198823676, Validation Loss: 0.0027555832639336586\n",
            "Part: 1/3, Epoch: 75/100, Total Epochs: 75, Train Loss: 0.00010493981972103938, Validation Loss: 0.0027827622834593058\n",
            "Part: 1/3, Epoch: 76/100, Total Epochs: 76, Train Loss: 0.00010524869139771909, Validation Loss: 0.0028005270287394524\n",
            "Part: 1/3, Epoch: 77/100, Total Epochs: 77, Train Loss: 0.00010526621190365404, Validation Loss: 0.002741866046562791\n",
            "Part: 1/3, Epoch: 78/100, Total Epochs: 78, Train Loss: 0.00010517571354284883, Validation Loss: 0.0028072574641555548\n",
            "Part: 1/3, Epoch: 79/100, Total Epochs: 79, Train Loss: 0.00010450185800436884, Validation Loss: 0.002742441836744547\n",
            "Part: 1/3, Epoch: 80/100, Total Epochs: 80, Train Loss: 0.0001042130243149586, Validation Loss: 0.0027874032966792583\n",
            "Part: 1/3, Epoch: 81/100, Total Epochs: 81, Train Loss: 0.00010397445294074714, Validation Loss: 0.002767026424407959\n",
            "Part: 1/3, Epoch: 82/100, Total Epochs: 82, Train Loss: 0.00010391935938969254, Validation Loss: 0.0027675803285092115\n",
            "Part: 1/3, Epoch: 83/100, Total Epochs: 83, Train Loss: 0.00010399655729997903, Validation Loss: 0.002789911348372698\n",
            "Part: 1/3, Epoch: 84/100, Total Epochs: 84, Train Loss: 0.00010396451398264617, Validation Loss: 0.002761330921202898\n",
            "Part: 1/3, Epoch: 85/100, Total Epochs: 85, Train Loss: 0.00010405252396594733, Validation Loss: 0.002813436556607485\n",
            "Part: 1/3, Epoch: 86/100, Total Epochs: 86, Train Loss: 0.00010395760182291269, Validation Loss: 0.002769815269857645\n",
            "Part: 1/3, Epoch: 87/100, Total Epochs: 87, Train Loss: 0.00010406207002233714, Validation Loss: 0.0028318227268755436\n",
            "Part: 1/3, Epoch: 88/100, Total Epochs: 88, Train Loss: 0.00010392861440777779, Validation Loss: 0.0027756148483604193\n",
            "Part: 1/3, Epoch: 89/100, Total Epochs: 89, Train Loss: 0.0001041945070028305, Validation Loss: 0.002843935042619705\n",
            "Part: 1/3, Epoch: 90/100, Total Epochs: 90, Train Loss: 0.00010413442214485258, Validation Loss: 0.002772953361272812\n",
            "Part: 1/3, Epoch: 91/100, Total Epochs: 91, Train Loss: 0.00010456479503773153, Validation Loss: 0.002851705066859722\n",
            "Part: 1/3, Epoch: 92/100, Total Epochs: 92, Train Loss: 0.00010480356286279857, Validation Loss: 0.0027666473761200905\n",
            "Part: 1/3, Epoch: 93/100, Total Epochs: 93, Train Loss: 0.00010596074571367353, Validation Loss: 0.002865872345864773\n",
            "Part: 1/3, Epoch: 94/100, Total Epochs: 94, Train Loss: 0.00010774304973892868, Validation Loss: 0.0027518647257238626\n",
            "Part: 1/3, Epoch: 95/100, Total Epochs: 95, Train Loss: 0.00011147069744765759, Validation Loss: 0.002910459414124489\n",
            "Part: 1/3, Epoch: 96/100, Total Epochs: 96, Train Loss: 0.00011779019405366853, Validation Loss: 0.002727562328800559\n",
            "Part: 1/3, Epoch: 97/100, Total Epochs: 97, Train Loss: 0.0001305490150116384, Validation Loss: 0.0030065947212278843\n",
            "Part: 1/3, Epoch: 98/100, Total Epochs: 98, Train Loss: 0.0001517755736131221, Validation Loss: 0.0026921697426587343\n",
            "Part: 1/3, Epoch: 99/100, Total Epochs: 99, Train Loss: 0.00018926820484921336, Validation Loss: 0.003235830459743738\n",
            "Part: 1/3, Epoch: 100/100, Total Epochs: 100, Train Loss: 0.0002441583783365786, Validation Loss: 0.002617502585053444\n",
            "Model and weights saved at epoch 100\n",
            "Part: 2/3, Epoch: 1/100, Total Epochs: 101, Train Loss: 0.0027640671469271183, Validation Loss: 0.00408555381000042\n",
            "Part: 2/3, Epoch: 2/100, Total Epochs: 102, Train Loss: 0.009031656198203564, Validation Loss: 0.00707092834636569\n",
            "Part: 2/3, Epoch: 3/100, Total Epochs: 103, Train Loss: 0.00920173805207014, Validation Loss: 0.007072526961565018\n",
            "Part: 2/3, Epoch: 4/100, Total Epochs: 104, Train Loss: 0.009204814210534096, Validation Loss: 0.007072526961565018\n",
            "Part: 2/3, Epoch: 5/100, Total Epochs: 105, Train Loss: 0.009206829592585564, Validation Loss: 0.007072526961565018\n",
            "Part: 2/3, Epoch: 6/100, Total Epochs: 106, Train Loss: 0.009207267314195633, Validation Loss: 0.007072526961565018\n",
            "Part: 2/3, Epoch: 7/100, Total Epochs: 107, Train Loss: 0.00920669175684452, Validation Loss: 0.007064793258905411\n",
            "Part: 2/3, Epoch: 8/100, Total Epochs: 108, Train Loss: 0.009206218644976616, Validation Loss: 0.007063614204525948\n",
            "Part: 2/3, Epoch: 9/100, Total Epochs: 109, Train Loss: 0.009205853566527367, Validation Loss: 0.007087106816470623\n",
            "Part: 2/3, Epoch: 10/100, Total Epochs: 110, Train Loss: 0.009205585345625877, Validation Loss: 0.0071493759751319885\n",
            "Part: 2/3, Epoch: 11/100, Total Epochs: 111, Train Loss: 0.009205457754433155, Validation Loss: 0.007228419650346041\n",
            "Part: 2/3, Epoch: 12/100, Total Epochs: 112, Train Loss: 0.009205387905240059, Validation Loss: 0.007258733734488487\n",
            "Part: 2/3, Epoch: 13/100, Total Epochs: 113, Train Loss: 0.00920530129224062, Validation Loss: 0.007232602685689926\n",
            "Part: 2/3, Epoch: 14/100, Total Epochs: 114, Train Loss: 0.009205210953950882, Validation Loss: 0.007194408681243658\n",
            "Part: 2/3, Epoch: 15/100, Total Epochs: 115, Train Loss: 0.009205162525177002, Validation Loss: 0.0071740723215043545\n",
            "Part: 2/3, Epoch: 16/100, Total Epochs: 116, Train Loss: 0.009205138310790062, Validation Loss: 0.007172478828579187\n",
            "Part: 2/3, Epoch: 17/100, Total Epochs: 117, Train Loss: 0.009205115959048271, Validation Loss: 0.007182911969721317\n",
            "Part: 2/3, Epoch: 18/100, Total Epochs: 118, Train Loss: 0.009205097332596779, Validation Loss: 0.007200818508863449\n",
            "Part: 2/3, Epoch: 19/100, Total Epochs: 119, Train Loss: 0.009205082431435585, Validation Loss: 0.007223295979201794\n",
            "Part: 2/3, Epoch: 20/100, Total Epochs: 120, Train Loss: 0.009205073118209839, Validation Loss: 0.007239208091050386\n",
            "Part: 2/3, Epoch: 21/100, Total Epochs: 121, Train Loss: 0.00920507125556469, Validation Loss: 0.007235699333250523\n",
            "Part: 2/3, Epoch: 22/100, Total Epochs: 122, Train Loss: 0.009205065667629242, Validation Loss: 0.0072252387180924416\n",
            "Part: 2/3, Epoch: 23/100, Total Epochs: 123, Train Loss: 0.009205063804984093, Validation Loss: 0.007220398169010878\n",
            "Part: 2/3, Epoch: 24/100, Total Epochs: 124, Train Loss: 0.009205061942338943, Validation Loss: 0.007221039850264788\n",
            "Part: 2/3, Epoch: 25/100, Total Epochs: 125, Train Loss: 0.009205061942338943, Validation Loss: 0.007224647328257561\n",
            "Part: 2/3, Epoch: 26/100, Total Epochs: 126, Train Loss: 0.009205060079693794, Validation Loss: 0.007229621522128582\n",
            "Part: 2/3, Epoch: 27/100, Total Epochs: 127, Train Loss: 0.009205060079693794, Validation Loss: 0.007231610827147961\n",
            "Part: 2/3, Epoch: 28/100, Total Epochs: 128, Train Loss: 0.009205060079693794, Validation Loss: 0.007229498587548733\n",
            "Part: 2/3, Epoch: 29/100, Total Epochs: 129, Train Loss: 0.009205060079693794, Validation Loss: 0.007226986810564995\n",
            "Part: 2/3, Epoch: 30/100, Total Epochs: 130, Train Loss: 0.009205060079693794, Validation Loss: 0.007226071320474148\n",
            "Part: 2/3, Epoch: 31/100, Total Epochs: 131, Train Loss: 0.00920505914837122, Validation Loss: 0.007226404268294573\n",
            "Part: 2/3, Epoch: 32/100, Total Epochs: 132, Train Loss: 0.00920505914837122, Validation Loss: 0.007227381691336632\n",
            "Part: 2/3, Epoch: 33/100, Total Epochs: 133, Train Loss: 0.00920505914837122, Validation Loss: 0.0072286068461835384\n",
            "Part: 2/3, Epoch: 34/100, Total Epochs: 134, Train Loss: 0.009205058217048645, Validation Loss: 0.00722927413880825\n",
            "Part: 2/3, Epoch: 35/100, Total Epochs: 135, Train Loss: 0.009205058217048645, Validation Loss: 0.007228977046906948\n",
            "Part: 2/3, Epoch: 36/100, Total Epochs: 136, Train Loss: 0.009205058217048645, Validation Loss: 0.007228395435959101\n",
            "Part: 2/3, Epoch: 37/100, Total Epochs: 137, Train Loss: 0.009205058217048645, Validation Loss: 0.007228087168186903\n",
            "Part: 2/3, Epoch: 38/100, Total Epochs: 138, Train Loss: 0.009205058217048645, Validation Loss: 0.007228042930364609\n",
            "Part: 2/3, Epoch: 39/100, Total Epochs: 139, Train Loss: 0.009205058217048645, Validation Loss: 0.007228137459605932\n",
            "Part: 2/3, Epoch: 40/100, Total Epochs: 140, Train Loss: 0.009205058217048645, Validation Loss: 0.007228289730846882\n",
            "Part: 2/3, Epoch: 41/100, Total Epochs: 141, Train Loss: 0.009205058217048645, Validation Loss: 0.00722845084965229\n",
            "Part: 2/3, Epoch: 42/100, Total Epochs: 142, Train Loss: 0.009205058217048645, Validation Loss: 0.007228591945022345\n",
            "Part: 2/3, Epoch: 43/100, Total Epochs: 143, Train Loss: 0.009205058217048645, Validation Loss: 0.0072287023067474365\n",
            "Part: 2/3, Epoch: 44/100, Total Epochs: 144, Train Loss: 0.009205058217048645, Validation Loss: 0.007228745147585869\n",
            "Part: 2/3, Epoch: 45/100, Total Epochs: 145, Train Loss: 0.009205058217048645, Validation Loss: 0.007228711619973183\n",
            "Part: 2/3, Epoch: 46/100, Total Epochs: 146, Train Loss: 0.009205058217048645, Validation Loss: 0.007228660862892866\n",
            "Part: 2/3, Epoch: 47/100, Total Epochs: 147, Train Loss: 0.009205058217048645, Validation Loss: 0.007228618022054434\n",
            "Part: 2/3, Epoch: 48/100, Total Epochs: 148, Train Loss: 0.009205058217048645, Validation Loss: 0.007228580303490162\n",
            "Part: 2/3, Epoch: 49/100, Total Epochs: 149, Train Loss: 0.009205058217048645, Validation Loss: 0.007228548638522625\n",
            "Part: 2/3, Epoch: 50/100, Total Epochs: 150, Train Loss: 0.009205058217048645, Validation Loss: 0.007228522095829248\n",
            "Model and weights saved at epoch 150\n",
            "Part: 2/3, Epoch: 51/100, Total Epochs: 151, Train Loss: 0.009205058217048645, Validation Loss: 0.007228499744087458\n",
            "Part: 2/3, Epoch: 52/100, Total Epochs: 152, Train Loss: 0.009205058217048645, Validation Loss: 0.007228481583297253\n",
            "Part: 2/3, Epoch: 53/100, Total Epochs: 153, Train Loss: 0.009205058217048645, Validation Loss: 0.007228468079119921\n",
            "Part: 2/3, Epoch: 54/100, Total Epochs: 154, Train Loss: 0.009205058217048645, Validation Loss: 0.0072284578345716\n",
            "Part: 2/3, Epoch: 55/100, Total Epochs: 155, Train Loss: 0.009205058217048645, Validation Loss: 0.00722845084965229\n",
            "Part: 2/3, Epoch: 56/100, Total Epochs: 156, Train Loss: 0.009205058217048645, Validation Loss: 0.007228447590023279\n",
            "Part: 2/3, Epoch: 57/100, Total Epochs: 157, Train Loss: 0.009205058217048645, Validation Loss: 0.007228445261716843\n",
            "Part: 2/3, Epoch: 58/100, Total Epochs: 158, Train Loss: 0.009205058217048645, Validation Loss: 0.007228444330394268\n",
            "Part: 2/3, Epoch: 59/100, Total Epochs: 159, Train Loss: 0.009205058217048645, Validation Loss: 0.007228444330394268\n",
            "Part: 2/3, Epoch: 60/100, Total Epochs: 160, Train Loss: 0.009205058217048645, Validation Loss: 0.007228444796055555\n",
            "Part: 2/3, Epoch: 61/100, Total Epochs: 161, Train Loss: 0.009205058217048645, Validation Loss: 0.007228444796055555\n",
            "Part: 2/3, Epoch: 62/100, Total Epochs: 162, Train Loss: 0.009205058217048645, Validation Loss: 0.00722844572737813\n",
            "Part: 2/3, Epoch: 63/100, Total Epochs: 163, Train Loss: 0.009205058217048645, Validation Loss: 0.007228446193039417\n",
            "Part: 2/3, Epoch: 64/100, Total Epochs: 164, Train Loss: 0.009205058217048645, Validation Loss: 0.007228446193039417\n",
            "Part: 2/3, Epoch: 65/100, Total Epochs: 165, Train Loss: 0.009205058217048645, Validation Loss: 0.007228446658700705\n",
            "Part: 2/3, Epoch: 66/100, Total Epochs: 166, Train Loss: 0.009205058217048645, Validation Loss: 0.007228446658700705\n",
            "Part: 2/3, Epoch: 67/100, Total Epochs: 167, Train Loss: 0.009205058217048645, Validation Loss: 0.007228447124361992\n",
            "Part: 2/3, Epoch: 68/100, Total Epochs: 168, Train Loss: 0.009205058217048645, Validation Loss: 0.007228447124361992\n",
            "Part: 2/3, Epoch: 69/100, Total Epochs: 169, Train Loss: 0.009205058217048645, Validation Loss: 0.007228448521345854\n",
            "Part: 2/3, Epoch: 70/100, Total Epochs: 170, Train Loss: 0.009205058217048645, Validation Loss: 0.007228448521345854\n",
            "Part: 2/3, Epoch: 71/100, Total Epochs: 171, Train Loss: 0.009205058217048645, Validation Loss: 0.007228448521345854\n",
            "Part: 2/3, Epoch: 72/100, Total Epochs: 172, Train Loss: 0.009205058217048645, Validation Loss: 0.007228448521345854\n",
            "Part: 2/3, Epoch: 73/100, Total Epochs: 173, Train Loss: 0.009205058217048645, Validation Loss: 0.007228448521345854\n",
            "Part: 2/3, Epoch: 74/100, Total Epochs: 174, Train Loss: 0.009205058217048645, Validation Loss: 0.007228448521345854\n",
            "Part: 2/3, Epoch: 75/100, Total Epochs: 175, Train Loss: 0.009205058217048645, Validation Loss: 0.007228447590023279\n",
            "Part: 2/3, Epoch: 76/100, Total Epochs: 176, Train Loss: 0.009205058217048645, Validation Loss: 0.007228447124361992\n",
            "Part: 2/3, Epoch: 77/100, Total Epochs: 177, Train Loss: 0.009205058217048645, Validation Loss: 0.007228447124361992\n",
            "Part: 2/3, Epoch: 78/100, Total Epochs: 178, Train Loss: 0.009205058217048645, Validation Loss: 0.007228447124361992\n",
            "Part: 2/3, Epoch: 79/100, Total Epochs: 179, Train Loss: 0.009205058217048645, Validation Loss: 0.007228447124361992\n",
            "Part: 2/3, Epoch: 80/100, Total Epochs: 180, Train Loss: 0.009205058217048645, Validation Loss: 0.007228447590023279\n",
            "Part: 2/3, Epoch: 81/100, Total Epochs: 181, Train Loss: 0.009205058217048645, Validation Loss: 0.007228447590023279\n",
            "Part: 2/3, Epoch: 82/100, Total Epochs: 182, Train Loss: 0.009205058217048645, Validation Loss: 0.0072284480556845665\n",
            "Part: 2/3, Epoch: 83/100, Total Epochs: 183, Train Loss: 0.009205058217048645, Validation Loss: 0.007228448521345854\n",
            "Part: 2/3, Epoch: 84/100, Total Epochs: 184, Train Loss: 0.009205058217048645, Validation Loss: 0.007228448521345854\n",
            "Part: 2/3, Epoch: 85/100, Total Epochs: 185, Train Loss: 0.009205058217048645, Validation Loss: 0.007228448521345854\n",
            "Part: 2/3, Epoch: 86/100, Total Epochs: 186, Train Loss: 0.009205058217048645, Validation Loss: 0.0072284480556845665\n",
            "Part: 2/3, Epoch: 87/100, Total Epochs: 187, Train Loss: 0.009205058217048645, Validation Loss: 0.0072284480556845665\n",
            "Part: 2/3, Epoch: 88/100, Total Epochs: 188, Train Loss: 0.009205058217048645, Validation Loss: 0.0072284480556845665\n",
            "Part: 2/3, Epoch: 89/100, Total Epochs: 189, Train Loss: 0.009205058217048645, Validation Loss: 0.0072284480556845665\n",
            "Part: 2/3, Epoch: 90/100, Total Epochs: 190, Train Loss: 0.009205058217048645, Validation Loss: 0.0072284480556845665\n",
            "Part: 2/3, Epoch: 91/100, Total Epochs: 191, Train Loss: 0.009205058217048645, Validation Loss: 0.0072284480556845665\n",
            "Part: 2/3, Epoch: 92/100, Total Epochs: 192, Train Loss: 0.009205058217048645, Validation Loss: 0.0072284480556845665\n",
            "Part: 2/3, Epoch: 93/100, Total Epochs: 193, Train Loss: 0.009205058217048645, Validation Loss: 0.0072284480556845665\n",
            "Part: 2/3, Epoch: 94/100, Total Epochs: 194, Train Loss: 0.009205058217048645, Validation Loss: 0.0072284480556845665\n",
            "Part: 2/3, Epoch: 95/100, Total Epochs: 195, Train Loss: 0.009205058217048645, Validation Loss: 0.0072284480556845665\n",
            "Part: 2/3, Epoch: 96/100, Total Epochs: 196, Train Loss: 0.009205058217048645, Validation Loss: 0.0072284480556845665\n",
            "Part: 2/3, Epoch: 97/100, Total Epochs: 197, Train Loss: 0.009205058217048645, Validation Loss: 0.0072284480556845665\n",
            "Part: 2/3, Epoch: 98/100, Total Epochs: 198, Train Loss: 0.009205058217048645, Validation Loss: 0.0072284480556845665\n",
            "Part: 2/3, Epoch: 99/100, Total Epochs: 199, Train Loss: 0.009205058217048645, Validation Loss: 0.0072284480556845665\n",
            "Part: 2/3, Epoch: 100/100, Total Epochs: 200, Train Loss: 0.009205058217048645, Validation Loss: 0.0072284480556845665\n",
            "Model and weights saved at epoch 200\n",
            "Part: 3/3, Epoch: 1/100, Total Epochs: 201, Train Loss: 0.01611565239727497, Validation Loss: 0.007150487974286079\n",
            "Part: 3/3, Epoch: 2/100, Total Epochs: 202, Train Loss: 0.016090042889118195, Validation Loss: 0.007072526961565018\n",
            "Part: 3/3, Epoch: 3/100, Total Epochs: 203, Train Loss: 0.016090964898467064, Validation Loss: 0.007072526961565018\n",
            "Part: 3/3, Epoch: 4/100, Total Epochs: 204, Train Loss: 0.016091451048851013, Validation Loss: 0.007072526961565018\n",
            "Part: 3/3, Epoch: 5/100, Total Epochs: 205, Train Loss: 0.01609148643910885, Validation Loss: 0.007072526961565018\n",
            "Part: 3/3, Epoch: 6/100, Total Epochs: 206, Train Loss: 0.01609126105904579, Validation Loss: 0.007072526961565018\n",
            "Part: 3/3, Epoch: 7/100, Total Epochs: 207, Train Loss: 0.01609094627201557, Validation Loss: 0.007072526961565018\n",
            "Part: 3/3, Epoch: 8/100, Total Epochs: 208, Train Loss: 0.016090622171759605, Validation Loss: 0.007072526961565018\n",
            "Part: 3/3, Epoch: 9/100, Total Epochs: 209, Train Loss: 0.01609034091234207, Validation Loss: 0.007072526961565018\n",
            "Part: 3/3, Epoch: 10/100, Total Epochs: 210, Train Loss: 0.01609012298285961, Validation Loss: 0.007072526961565018\n",
            "Part: 3/3, Epoch: 11/100, Total Epochs: 211, Train Loss: 0.016089806333184242, Validation Loss: 0.007072526961565018\n",
            "Part: 3/3, Epoch: 12/100, Total Epochs: 212, Train Loss: 0.01608911342918873, Validation Loss: 0.007072526961565018\n",
            "Part: 3/3, Epoch: 13/100, Total Epochs: 213, Train Loss: 0.0160885751247406, Validation Loss: 0.007072526961565018\n",
            "Part: 3/3, Epoch: 14/100, Total Epochs: 214, Train Loss: 0.016088249161839485, Validation Loss: 0.007072526961565018\n",
            "Part: 3/3, Epoch: 15/100, Total Epochs: 215, Train Loss: 0.016088034957647324, Validation Loss: 0.007072526961565018\n",
            "Part: 3/3, Epoch: 16/100, Total Epochs: 216, Train Loss: 0.016087844967842102, Validation Loss: 0.007072526961565018\n",
            "Part: 3/3, Epoch: 17/100, Total Epochs: 217, Train Loss: 0.01608765684068203, Validation Loss: 0.007072526961565018\n",
            "Part: 3/3, Epoch: 18/100, Total Epochs: 218, Train Loss: 0.016087474301457405, Validation Loss: 0.007072526961565018\n",
            "Part: 3/3, Epoch: 19/100, Total Epochs: 219, Train Loss: 0.016087302938103676, Validation Loss: 0.007072526961565018\n",
            "Part: 3/3, Epoch: 20/100, Total Epochs: 220, Train Loss: 0.01608714647591114, Validation Loss: 0.007072526961565018\n",
            "Part: 3/3, Epoch: 21/100, Total Epochs: 221, Train Loss: 0.01608700305223465, Validation Loss: 0.007072526961565018\n",
            "Part: 3/3, Epoch: 22/100, Total Epochs: 222, Train Loss: 0.016086874529719353, Validation Loss: 0.007072526961565018\n",
            "Part: 3/3, Epoch: 23/100, Total Epochs: 223, Train Loss: 0.016086753457784653, Validation Loss: 0.007072526961565018\n",
            "Part: 3/3, Epoch: 24/100, Total Epochs: 224, Train Loss: 0.016086643561720848, Validation Loss: 0.007072526961565018\n",
            "Part: 3/3, Epoch: 25/100, Total Epochs: 225, Train Loss: 0.01608654111623764, Validation Loss: 0.007072526961565018\n",
            "Part: 3/3, Epoch: 26/100, Total Epochs: 226, Train Loss: 0.01608644798398018, Validation Loss: 0.007072526961565018\n",
            "Part: 3/3, Epoch: 27/100, Total Epochs: 227, Train Loss: 0.016086362302303314, Validation Loss: 0.007072526961565018\n",
            "Part: 3/3, Epoch: 28/100, Total Epochs: 228, Train Loss: 0.016086285933852196, Validation Loss: 0.007072526961565018\n",
            "Part: 3/3, Epoch: 29/100, Total Epochs: 229, Train Loss: 0.016086215153336525, Validation Loss: 0.007072526961565018\n",
            "Part: 3/3, Epoch: 30/100, Total Epochs: 230, Train Loss: 0.016086149960756302, Validation Loss: 0.007072526961565018\n",
            "Part: 3/3, Epoch: 31/100, Total Epochs: 231, Train Loss: 0.016086092218756676, Validation Loss: 0.007072526961565018\n",
            "Part: 3/3, Epoch: 32/100, Total Epochs: 232, Train Loss: 0.016086040064692497, Validation Loss: 0.007072526961565018\n",
            "Part: 3/3, Epoch: 33/100, Total Epochs: 233, Train Loss: 0.016085991635918617, Validation Loss: 0.007072526961565018\n",
            "Part: 3/3, Epoch: 34/100, Total Epochs: 234, Train Loss: 0.016085950657725334, Validation Loss: 0.007072526961565018\n",
            "Part: 3/3, Epoch: 35/100, Total Epochs: 235, Train Loss: 0.01608590967953205, Validation Loss: 0.007072526961565018\n",
            "Part: 3/3, Epoch: 36/100, Total Epochs: 236, Train Loss: 0.016085876151919365, Validation Loss: 0.007072526961565018\n",
            "Part: 3/3, Epoch: 37/100, Total Epochs: 237, Train Loss: 0.016085846349596977, Validation Loss: 0.007072526961565018\n",
            "Part: 3/3, Epoch: 38/100, Total Epochs: 238, Train Loss: 0.016085820272564888, Validation Loss: 0.007072526961565018\n",
            "Part: 3/3, Epoch: 39/100, Total Epochs: 239, Train Loss: 0.016085796058177948, Validation Loss: 0.007072526961565018\n",
            "Part: 3/3, Epoch: 40/100, Total Epochs: 240, Train Loss: 0.016085775569081306, Validation Loss: 0.007072526961565018\n",
            "Part: 3/3, Epoch: 41/100, Total Epochs: 241, Train Loss: 0.016085756942629814, Validation Loss: 0.007072526961565018\n",
            "Part: 3/3, Epoch: 42/100, Total Epochs: 242, Train Loss: 0.01608574204146862, Validation Loss: 0.007072526961565018\n",
            "Part: 3/3, Epoch: 43/100, Total Epochs: 243, Train Loss: 0.016085729002952576, Validation Loss: 0.007072526961565018\n",
            "Part: 3/3, Epoch: 44/100, Total Epochs: 244, Train Loss: 0.01608571596443653, Validation Loss: 0.007072526961565018\n",
            "Part: 3/3, Epoch: 45/100, Total Epochs: 245, Train Loss: 0.016085706651210785, Validation Loss: 0.007072526961565018\n",
            "Part: 3/3, Epoch: 46/100, Total Epochs: 246, Train Loss: 0.01608569733798504, Validation Loss: 0.007072526961565018\n",
            "Part: 3/3, Epoch: 47/100, Total Epochs: 247, Train Loss: 0.016085689887404442, Validation Loss: 0.007072526961565018\n",
            "Part: 3/3, Epoch: 48/100, Total Epochs: 248, Train Loss: 0.016085682436823845, Validation Loss: 0.007072526961565018\n",
            "Part: 3/3, Epoch: 49/100, Total Epochs: 249, Train Loss: 0.016085676848888397, Validation Loss: 0.007072526961565018\n",
            "Part: 3/3, Epoch: 50/100, Total Epochs: 250, Train Loss: 0.01608567126095295, Validation Loss: 0.007072526961565018\n",
            "Model and weights saved at epoch 250\n",
            "Part: 3/3, Epoch: 51/100, Total Epochs: 251, Train Loss: 0.0160856693983078, Validation Loss: 0.007072526961565018\n",
            "Part: 3/3, Epoch: 52/100, Total Epochs: 252, Train Loss: 0.016085665673017502, Validation Loss: 0.007072526961565018\n",
            "Part: 3/3, Epoch: 53/100, Total Epochs: 253, Train Loss: 0.016085661947727203, Validation Loss: 0.007072526961565018\n",
            "Part: 3/3, Epoch: 54/100, Total Epochs: 254, Train Loss: 0.016085660085082054, Validation Loss: 0.007072526961565018\n",
            "Part: 3/3, Epoch: 55/100, Total Epochs: 255, Train Loss: 0.016085658222436905, Validation Loss: 0.007072526961565018\n",
            "Part: 3/3, Epoch: 56/100, Total Epochs: 256, Train Loss: 0.016085656359791756, Validation Loss: 0.007072526961565018\n",
            "Part: 3/3, Epoch: 57/100, Total Epochs: 257, Train Loss: 0.016085654497146606, Validation Loss: 0.007072526961565018\n",
            "Part: 3/3, Epoch: 58/100, Total Epochs: 258, Train Loss: 0.016085654497146606, Validation Loss: 0.007072526961565018\n",
            "Part: 3/3, Epoch: 59/100, Total Epochs: 259, Train Loss: 0.016085652634501457, Validation Loss: 0.007072526961565018\n",
            "Part: 3/3, Epoch: 60/100, Total Epochs: 260, Train Loss: 0.016085652634501457, Validation Loss: 0.007072526961565018\n",
            "Part: 3/3, Epoch: 61/100, Total Epochs: 261, Train Loss: 0.016085652634501457, Validation Loss: 0.007072526961565018\n",
            "Part: 3/3, Epoch: 62/100, Total Epochs: 262, Train Loss: 0.016085652634501457, Validation Loss: 0.007072526961565018\n",
            "Part: 3/3, Epoch: 63/100, Total Epochs: 263, Train Loss: 0.016085650771856308, Validation Loss: 0.007072526961565018\n",
            "Part: 3/3, Epoch: 64/100, Total Epochs: 264, Train Loss: 0.016085650771856308, Validation Loss: 0.007072526961565018\n",
            "Part: 3/3, Epoch: 65/100, Total Epochs: 265, Train Loss: 0.01608564890921116, Validation Loss: 0.007072526961565018\n",
            "Part: 3/3, Epoch: 66/100, Total Epochs: 266, Train Loss: 0.01608564890921116, Validation Loss: 0.007072526961565018\n",
            "Part: 3/3, Epoch: 67/100, Total Epochs: 267, Train Loss: 0.01608564890921116, Validation Loss: 0.007072526961565018\n",
            "Part: 3/3, Epoch: 68/100, Total Epochs: 268, Train Loss: 0.01608564890921116, Validation Loss: 0.007072526961565018\n",
            "Part: 3/3, Epoch: 69/100, Total Epochs: 269, Train Loss: 0.01608564890921116, Validation Loss: 0.007072526961565018\n",
            "Part: 3/3, Epoch: 70/100, Total Epochs: 270, Train Loss: 0.01608564890921116, Validation Loss: 0.007072526961565018\n",
            "Part: 3/3, Epoch: 71/100, Total Epochs: 271, Train Loss: 0.01608564890921116, Validation Loss: 0.007072526961565018\n",
            "Part: 3/3, Epoch: 72/100, Total Epochs: 272, Train Loss: 0.01608564890921116, Validation Loss: 0.007072526961565018\n",
            "Part: 3/3, Epoch: 73/100, Total Epochs: 273, Train Loss: 0.01608564890921116, Validation Loss: 0.007072526961565018\n",
            "Part: 3/3, Epoch: 74/100, Total Epochs: 274, Train Loss: 0.01608564890921116, Validation Loss: 0.007072526961565018\n",
            "Part: 3/3, Epoch: 75/100, Total Epochs: 275, Train Loss: 0.01608564890921116, Validation Loss: 0.007072526961565018\n",
            "Part: 3/3, Epoch: 76/100, Total Epochs: 276, Train Loss: 0.01608564890921116, Validation Loss: 0.007072526961565018\n",
            "Part: 3/3, Epoch: 77/100, Total Epochs: 277, Train Loss: 0.01608564890921116, Validation Loss: 0.007072526961565018\n",
            "Part: 3/3, Epoch: 78/100, Total Epochs: 278, Train Loss: 0.01608564890921116, Validation Loss: 0.007072526961565018\n",
            "Part: 3/3, Epoch: 79/100, Total Epochs: 279, Train Loss: 0.01608564890921116, Validation Loss: 0.007072526961565018\n",
            "Part: 3/3, Epoch: 80/100, Total Epochs: 280, Train Loss: 0.01608564890921116, Validation Loss: 0.007072526961565018\n",
            "Part: 3/3, Epoch: 81/100, Total Epochs: 281, Train Loss: 0.01608564890921116, Validation Loss: 0.007072526961565018\n",
            "Part: 3/3, Epoch: 82/100, Total Epochs: 282, Train Loss: 0.01608564890921116, Validation Loss: 0.007072526961565018\n",
            "Part: 3/3, Epoch: 83/100, Total Epochs: 283, Train Loss: 0.01608564890921116, Validation Loss: 0.007072526961565018\n",
            "Part: 3/3, Epoch: 84/100, Total Epochs: 284, Train Loss: 0.01608564890921116, Validation Loss: 0.007072526961565018\n",
            "Part: 3/3, Epoch: 85/100, Total Epochs: 285, Train Loss: 0.01608564890921116, Validation Loss: 0.007072526961565018\n",
            "Part: 3/3, Epoch: 86/100, Total Epochs: 286, Train Loss: 0.01608564890921116, Validation Loss: 0.007072526961565018\n",
            "Part: 3/3, Epoch: 87/100, Total Epochs: 287, Train Loss: 0.01608564890921116, Validation Loss: 0.007072526961565018\n",
            "Part: 3/3, Epoch: 88/100, Total Epochs: 288, Train Loss: 0.01608564890921116, Validation Loss: 0.007072526961565018\n",
            "Part: 3/3, Epoch: 89/100, Total Epochs: 289, Train Loss: 0.01608564890921116, Validation Loss: 0.007072526961565018\n",
            "Part: 3/3, Epoch: 90/100, Total Epochs: 290, Train Loss: 0.01608564890921116, Validation Loss: 0.007072526961565018\n",
            "Part: 3/3, Epoch: 91/100, Total Epochs: 291, Train Loss: 0.01608564890921116, Validation Loss: 0.007072526961565018\n",
            "Part: 3/3, Epoch: 92/100, Total Epochs: 292, Train Loss: 0.01608564890921116, Validation Loss: 0.007072526961565018\n",
            "Part: 3/3, Epoch: 93/100, Total Epochs: 293, Train Loss: 0.01608564890921116, Validation Loss: 0.007072526961565018\n",
            "Part: 3/3, Epoch: 94/100, Total Epochs: 294, Train Loss: 0.01608564890921116, Validation Loss: 0.007072526961565018\n",
            "Part: 3/3, Epoch: 95/100, Total Epochs: 295, Train Loss: 0.01608564890921116, Validation Loss: 0.007072526961565018\n",
            "Part: 3/3, Epoch: 96/100, Total Epochs: 296, Train Loss: 0.01608564890921116, Validation Loss: 0.007072526961565018\n",
            "Part: 3/3, Epoch: 97/100, Total Epochs: 297, Train Loss: 0.01608564890921116, Validation Loss: 0.007072526961565018\n",
            "Part: 3/3, Epoch: 98/100, Total Epochs: 298, Train Loss: 0.01608564890921116, Validation Loss: 0.007072526961565018\n",
            "Part: 3/3, Epoch: 99/100, Total Epochs: 299, Train Loss: 0.01608564890921116, Validation Loss: 0.007072526961565018\n",
            "Part: 3/3, Epoch: 100/100, Total Epochs: 300, Train Loss: 0.01608564890921116, Validation Loss: 0.007072526961565018\n",
            "Model and weights saved at epoch 300\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_weights('./checkpoints/my_checkpoint')"
      ],
      "metadata": {
        "id": "pcaEGqWGvJVE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "outputId": "cbda9378-502b-4ba6-d2c4-15ea13b6d7e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-c1987622b305>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./checkpoints/my_checkpoint'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('./saved_model')"
      ],
      "metadata": {
        "id": "ozjhzlLFJc9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_weights('./saved_models/weights_epoch_200.h5')"
      ],
      "metadata": {
        "id": "VtLX_vklvPwH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.load_model('./saved_models/model_epoch_200.h5')"
      ],
      "metadata": {
        "id": "RZbr3DldMg5L",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        },
        "outputId": "deb8cbea-ee7d-4834-cc73-8de9138f0996"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-8572f61f3fcc>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./saved_models/model_epoch_200.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/saving/saving_api.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;31m# Legacy case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m     return legacy_sm_saving_lib.load_model(\n\u001b[0m\u001b[1;32m    213\u001b[0m         \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/saving/legacy/serialization.py\u001b[0m in \u001b[0;36mclass_and_config_for_serialized_keras_object\u001b[0;34m(config, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[1;32m    366\u001b[0m     )\n\u001b[1;32m    367\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 368\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    369\u001b[0m             \u001b[0;34mf\"Unknown {printable_module_name}: '{class_name}'. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m             \u001b[0;34m\"Please ensure you are using a `keras.utils.custom_object_scope` \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Unknown decay: 'CustomLearningRateScheduler'. Please ensure you are using a `keras.utils.custom_object_scope` and that this object is included in the scope. See https://www.tensorflow.org/guide/keras/save_and_serialize#registering_the_custom_object for details."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting training and validation losses\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(all_train_losses, label='Training Loss')\n",
        "plt.plot(all_val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.title('Training and Validation Losses')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        },
        "id": "aL6iMAOhH2nj",
        "outputId": "31d27df0-8b1e-4084-cd8b-c30e48a9a5e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-bd3d01746888>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_train_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Training Loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_val_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Validation Loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epochs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'all_train_losses' is not defined"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x600 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the recorded losses into a DataFrame\n",
        "loss_df = pd.DataFrame({\n",
        "    'Epoch': list(range(1, len(all_train_losses) + 1)),\n",
        "    'Training_Loss': all_train_losses,\n",
        "    'Validation_Loss': all_val_losses\n",
        "})\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "loss_df.to_csv('losses.csv', index=False)\n"
      ],
      "metadata": {
        "id": "f0vf6jwkNtYD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to sample part of the training data and evaluate model's performance\n",
        "def sample_and_evaluate(num_samples=20):\n",
        "    indices = np.random.choice(len(X_train[0]), size=num_samples, replace=False)\n",
        "    # indices = [i for i in range(10)]\n",
        "    sampled_X = [X_train[0][indices], X_train[1][indices]]\n",
        "    sampled_y = y_train[indices]\n",
        "    loss = validate_step(sampled_X, sampled_y, print_results = True)\n",
        "    print(f\"Loss on sampled data: {loss.numpy()}\")\n",
        "    return loss.numpy()"
      ],
      "metadata": {
        "id": "Vd-mE6MJH48G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train[0][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wUOTaXO3Mhvq",
        "outputId": "541c2f4a-8e69-48e8-bfc9-a83471375282"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_and_evaluate()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aurMt4AwNa40",
        "outputId": "550fcb81-85ed-40a6-fd52-de67b6154c83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batched_errors:  [<tf.Tensor: shape=(2,), dtype=float32, numpy=array([0., 0.], dtype=float32)>, <tf.Tensor: shape=(2,), dtype=float32, numpy=array([0., 0.], dtype=float32)>, <tf.Tensor: shape=(2,), dtype=float32, numpy=array([0., 0.], dtype=float32)>, <tf.Tensor: shape=(2,), dtype=float32, numpy=array([0., 0.], dtype=float32)>, <tf.Tensor: shape=(2,), dtype=float32, numpy=array([0.09584745, 0.        ], dtype=float32)>, <tf.Tensor: shape=(2,), dtype=float32, numpy=array([0., 0.], dtype=float32)>, <tf.Tensor: shape=(2,), dtype=float32, numpy=array([0., 0.], dtype=float32)>, <tf.Tensor: shape=(2,), dtype=float32, numpy=array([0., 0.], dtype=float32)>, <tf.Tensor: shape=(2,), dtype=float32, numpy=array([0., 0.], dtype=float32)>, <tf.Tensor: shape=(2,), dtype=float32, numpy=array([0., 0.], dtype=float32)>, <tf.Tensor: shape=(2,), dtype=float32, numpy=array([0., 0.], dtype=float32)>, <tf.Tensor: shape=(2,), dtype=float32, numpy=array([0., 0.], dtype=float32)>, <tf.Tensor: shape=(2,), dtype=float32, numpy=array([0., 0.], dtype=float32)>, <tf.Tensor: shape=(2,), dtype=float32, numpy=array([0., 0.], dtype=float32)>, <tf.Tensor: shape=(2,), dtype=float32, numpy=array([0., 0.], dtype=float32)>, <tf.Tensor: shape=(2,), dtype=float32, numpy=array([0., 0.], dtype=float32)>, <tf.Tensor: shape=(2,), dtype=float32, numpy=array([0.05111224, 0.        ], dtype=float32)>, <tf.Tensor: shape=(2,), dtype=float32, numpy=array([0., 0.], dtype=float32)>, <tf.Tensor: shape=(2,), dtype=float32, numpy=array([0., 0.], dtype=float32)>, <tf.Tensor: shape=(2,), dtype=float32, numpy=array([0., 0.], dtype=float32)>]\n",
            "Loss on sampled data: 0.03371768444776535\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.033717684"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.shape(X_train)[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fCP-Ov4QcTyl",
        "outputId": "82feba0f-61ce-4c1e-a2ab-e6ca7c2d3d37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=int32, numpy=272>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "apply_gate(np.array([1/np.sqrt(2), 0, 0, 1/np.sqrt(2)], dtype=np.float32), 0.5, 0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ntIlL0UZeYVi",
        "outputId": "8623848e-72c1-4ce1-dd79-7db74a79d2bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "current label:  0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.70710677, 0.        , 0.        , 0.70710677], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_vector = np.array(\n",
        "    [[ 0.70710677],\n",
        "     [-0.7064972 ],\n",
        "     [ 0.        ],\n",
        "     [-0.02935636]], dtype=np.float32)"
      ],
      "metadata": {
        "id": "Jw0N66NJk3IN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_vector.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zp_bRrqOlnbm",
        "outputId": "b2cfafa3-bc67-4983-d253-52fa87ddaeb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.squeeze(test_vector).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LacrNsyblsfq",
        "outputId": "a81f37b0-73b8-4afb-b3d7-e897e6889fe5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([4])"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "compute_probabilities(tf.constant([1, 0, 0, 1], dtype=tf.float32))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lmMaS4Crkokb",
        "outputId": "15ccd2cd-df57-40f9-b457-a5828f2f5e9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor: shape=(), dtype=float32, numpy=0.99999994>,\n",
              " <tf.Tensor: shape=(), dtype=float32, numpy=0.0>)"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "compute_probabilities(tf.constant([1, 0, 0, -1], dtype=tf.float32))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dTnDDEUNkbg-",
        "outputId": "a2706368-ea03-42a4-d950-faad34fa70b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor: shape=(), dtype=float32, numpy=0.0>,\n",
              " <tf.Tensor: shape=(), dtype=float32, numpy=0.99999994>)"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for i in range (tf.shape(X_train[:1])[0]):\n",
        "#   single_sequence = tf.gather(X_train, i, axis=0)\n",
        "#   print('single_sequence: ', single_sequence)\n",
        "#   final_state = apply_gate_sequence(single_sequence)\n",
        "#   print('final_state: ', final_state)\n",
        "#   probabilities = compute_probabilities(final_state)\n",
        "#   print('probabilities: ',probabilities )"
      ],
      "metadata": {
        "id": "ASCr3ie5SjJN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(X_train[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bF4T9A-dOFuD",
        "outputId": "492d9a19-85de-4169-f308-73d24b015811"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "debug = tf.convert_to_tensor([[1, 0, 0, 0], [0, tf.math.cos(0.5), 0, tf.math.sin(0.5)],\n",
        "                           [0, 0, 1, 0], [0, -tf.math.sin(0.5), 0, tf.math.cos(0.5)]], dtype=tf.float32)"
      ],
      "metadata": {
        "id": "7EpcfNO0PMaL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "debug"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DNKTrgdkRHhi",
        "outputId": "7f3f7c76-53de-476b-c10c-9999a25d3189"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(4, 4), dtype=float32, numpy=\n",
              "array([[ 1.        ,  0.        ,  0.        ,  0.        ],\n",
              "       [ 0.        ,  0.87758255,  0.        ,  0.47942555],\n",
              "       [ 0.        ,  0.        ,  1.        ,  0.        ],\n",
              "       [ 0.        , -0.47942555,  0.        ,  0.87758255]],\n",
              "      dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    }
  ]
}