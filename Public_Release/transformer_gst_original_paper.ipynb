{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zd1H8cDcvaHr",
    "outputId": "f39b22da-e568-4310-c94a-93de13ab5e55"
   },
   "outputs": [],
   "source": [
    "# Google Colab specific code for mounting Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "# Define the directory path on your Google Drive\n",
    "# Replace 'Your_directory' with the actual directory\n",
    "directory = '/content/drive/My Drive/Your_directory'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0RJyTecyWOVa",
    "outputId": "852b7777-2a72-44cb-8d44-81a04f25f58d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Change the working directory to the desired path\n",
    "os.chdir(directory)\n",
    "\n",
    "# Verify that the working directory has been changed\n",
    "print(\"Current working directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6XTVMibkMAOZ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Lambda\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "# Load the sorted data directly from the modified CSV\n",
    "df_sorted = pd.read_csv('Sorted_Encoded_Padded_Probabilities.csv')\n",
    "\n",
    "def prepare_data(df_part):\n",
    "    # Extracting features and labels\n",
    "    X = df_part['Padded'].apply(lambda x: [int(xi) for xi in x.strip('[]').split()]).to_list()\n",
    "    y = df_part[['Prob1', 'Prob2']].values\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_integers_to_gates(int_sequences, gate_matrices):\n",
    "    \"\"\"\n",
    "    Map each integer in the sequences to its corresponding quantum gate matrix.\n",
    "    This version also maps zeros to a 4x4 zero matrix.\n",
    "    \n",
    "    Args:\n",
    "    - int_sequences (np.array): An array of shape (batch_size, seq_length) containing integer sequences.\n",
    "    - gate_matrices (list): A list of 4x4 matrices representing quantum gates.\n",
    "    \n",
    "    Returns:\n",
    "    - np.array: An array of shape (batch_size, seq_length, 4, 4) containing the mapped sequences.\n",
    "    \"\"\"\n",
    "    # Initialize an empty array to store the mapped sequences\n",
    "    mapped_sequences = np.zeros((int_sequences.shape[0], int_sequences.shape[1], 4, 4))\n",
    "    \n",
    "    # Loop through each sequence\n",
    "    for i in range(int_sequences.shape[0]):\n",
    "        # Map the integers to their corresponding quantum gate matrices\n",
    "        for j in range(int_sequences.shape[1]):\n",
    "            mapped_sequences[i, j] = gate_matrices[int_sequences[i, j]]\n",
    "            \n",
    "    return mapped_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = prepare_data(df_sorted)\n",
    "\n",
    "print('X type: ', type(X))\n",
    "print('y type: ', type(y))\n",
    "\n",
    "# Create new input data\n",
    "X_new = [X, y]\n",
    "\n",
    "X_train_0 = X\n",
    "y_train = y \n",
    "\n",
    "# Run the mapping function to convert integer sequences to gate matrices\n",
    "X_train_mapped = map_integers_to_gates(X_train_0, gate_matrices)\n",
    "# New input data after mapping\n",
    "X_train = [X_train_mapped, y_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters (adjustable)\n",
    "num_layers = 4  # Number of Transformer layers\n",
    "num_heads = 4  # Number of attention heads\n",
    "d_model = 8  # Feature dimension\n",
    "dff = 512  # Dimension of feed-forward network\n",
    "group_size = 30  # Example group size\n",
    "input_length = len(X_train[0][0])  # Example input length\n",
    "vocab_size = 4\n",
    "\n",
    "# Positional Encoding Layer (Serializable)\n",
    "@tf.keras.utils.register_keras_serializable()\n",
    "class PositionalEncodingLayer(layers.Layer):\n",
    "    def __init__(self, d_model, **kwargs):\n",
    "        super(PositionalEncodingLayer, self).__init__(**kwargs)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Calculate positional encoding for the combined group_size and input_length\n",
    "        seq_len = input_shape[1] * input_shape[2]  # group_size * input_length\n",
    "        self.pos_encoding = self.get_positional_encoding(seq_len, self.d_model)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Reshape inputs to (batch_size, group_size * input_length, d_model) for adding positional encoding\n",
    "        seq_len = tf.shape(inputs)[1] * tf.shape(inputs)[2]\n",
    "        reshaped_inputs = tf.reshape(inputs, [-1, seq_len, self.d_model])\n",
    "        output = reshaped_inputs + self.pos_encoding\n",
    "        return tf.reshape(output, [-1, tf.shape(inputs)[1], tf.shape(inputs)[2], self.d_model])\n",
    "\n",
    "    def get_positional_encoding(self, seq_len, d_model):\n",
    "        angles = np.arange(seq_len)[:, np.newaxis] / np.power(10000, (2 * (np.arange(d_model)[np.newaxis, :] // 2)) / np.float32(d_model))\n",
    "        sines = np.sin(angles[:, 0::2])\n",
    "        cosines = np.cos(angles[:, 1::2])\n",
    "\n",
    "        pos_encoding = np.concatenate([sines, cosines], axis=-1)\n",
    "        pos_encoding = pos_encoding[np.newaxis, ...]\n",
    "\n",
    "        return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(PositionalEncodingLayer, self).get_config()\n",
    "        config.update({\"d_model\": self.d_model})\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)\n",
    "\n",
    "\n",
    "# Transformer Encoder Layer (Serializable)\n",
    "@tf.keras.utils.register_keras_serializable()\n",
    "class TransformerEncoderLayer(models.Model):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1, **kwargs):\n",
    "        super(TransformerEncoderLayer, self).__init__(**kwargs)\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.dff = dff\n",
    "        self.rate = rate\n",
    "\n",
    "        self.attention = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.norm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.ffn = models.Sequential([\n",
    "            layers.Dense(dff, activation='gelu'),\n",
    "            layers.Dense(d_model)\n",
    "        ])\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "        self.norm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        attn_output = self.attention(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output)\n",
    "        out1 = self.norm1(inputs + attn_output)\n",
    "\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "        return self.norm2(out1 + ffn_output)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(TransformerEncoderLayer, self).get_config()\n",
    "        config.update({\n",
    "            'd_model': self.d_model,\n",
    "            'num_heads': self.num_heads,\n",
    "            'dff': self.dff,\n",
    "            'rate': self.rate\n",
    "        })\n",
    "        return config\n",
    "\n",
    "# Transformer Encoder (Serializable)\n",
    "@tf.keras.utils.register_keras_serializable()\n",
    "class TransformerEncoder(models.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, rate=0.1, **kwargs):\n",
    "        super(TransformerEncoder, self).__init__(**kwargs)\n",
    "        self.num_layers = num_layers\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.dff = dff\n",
    "        self.rate = rate\n",
    "\n",
    "        self.enc_layers = [TransformerEncoderLayer(d_model, num_heads, dff, rate)\n",
    "                           for _ in range(num_layers)]\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = inputs\n",
    "        for layer in self.enc_layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(TransformerEncoder, self).get_config()\n",
    "        config.update({\n",
    "            'num_layers': self.num_layers,\n",
    "            'd_model': self.d_model,\n",
    "            'num_heads': self.num_heads,\n",
    "            'dff': self.dff,\n",
    "            'rate': self.rate\n",
    "        })\n",
    "        return config\n",
    "\n",
    "\n",
    "# Inputs\n",
    "input_y_transformer = layers.Input(shape=(group_size, 2), name='input_y_transformer')\n",
    "input_gate_transformer = layers.Input(shape=(group_size, input_length), name='input_gate_transformer')\n",
    "\n",
    "\n",
    "# Embedding and Positional Encoding for gate sequences\n",
    "gate_embedding_layer = layers.Embedding(output_dim=d_model, input_dim=vocab_size)\n",
    "gate_embeddings = gate_embedding_layer(input_gate_transformer)\n",
    "gate_pos_encoding_layer = PositionalEncodingLayer(d_model=d_model)\n",
    "gate_pos_encoding = gate_pos_encoding_layer(gate_embeddings)\n",
    "\n",
    "\n",
    "# Embedding-like transformation for probabilities\n",
    "prob_transform_layer = layers.Dense(2 * d_model, activation='gelu')\n",
    "prob_transformed = prob_transform_layer(input_y_transformer)\n",
    "\n",
    "# Reshape to match the shape of gate sequence embeddings\n",
    "prob_reshaped = layers.Reshape((group_size, 2, d_model))(prob_transformed)\n",
    "prob_pos_encoding_layer = PositionalEncodingLayer(d_model=d_model)\n",
    "prob_pos_encoding = prob_pos_encoding_layer(prob_reshaped)\n",
    "\n",
    "# Cross-Attention between Dataset 1 and Dataset 2\n",
    "cross_attention = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n",
    "cross_attention_output = cross_attention(gate_pos_encoding, prob_pos_encoding)\n",
    "\n",
    "# Applying Transformer Encoder to the output of cross-attention\n",
    "transformer_block = TransformerEncoder(num_layers, d_model, num_heads, dff)\n",
    "encoded_output = transformer_block(cross_attention_output)\n",
    "\n",
    "# Regression Prediction\n",
    "flattened = layers.Flatten()(encoded_output)\n",
    "dense_layer = layers.Dense(2048, activation='gelu')(flattened)\n",
    "dense_layer = layers.Dropout(0.2)(dense_layer)\n",
    "dense_layer = layers.Dense(1024, activation='gelu')(dense_layer)\n",
    "dense_layer = layers.Dropout(0.2)(dense_layer)\n",
    "\n",
    "\n",
    "depolar_error_pred = layers.Dense(256, activation='gelu')(dense_layer)\n",
    "depolar_error_pred = layers.Dropout(0.2)(depolar_error_pred)\n",
    "depolar_error_pred = layers.Dense(128, activation='gelu')(depolar_error_pred)\n",
    "depolar_error_pred = layers.Dense(2, activation='tanh')(depolar_error_pred)\n",
    "\n",
    "\n",
    "over_rotation_pred = layers.Dense(256, activation='gelu')(dense_layer)\n",
    "over_rotation_pred = layers.Dropout(0.2)(over_rotation_pred)\n",
    "over_rotation_pred = layers.Dense(128, activation='gelu')(over_rotation_pred)\n",
    "over_rotation_pred = layers.Dense(2, activation='tanh')(over_rotation_pred)\n",
    "\n",
    "output = layers.Concatenate()([depolar_error_pred, over_rotation_pred])\n",
    "\n",
    "# Full Model\n",
    "model = models.Model(inputs=[input_gate_transformer, input_y_transformer], outputs=output)\n",
    "\n",
    "\n",
    "initial_learning_rate = 1e-6\n",
    "decay_steps = 200\n",
    "decay_rate = 0.95\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate, decay_steps, decay_rate, staircase=True\n",
    ")\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "# Compile the Model\n",
    "model.compile(optimizer=optimizer, loss='mse')\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def easy_PTM_depol_channel(depol_mat):\n",
    "    identity = tf.eye(4, dtype=tf.float32)\n",
    "    PTM_depol = (1 - tf.math.abs(depol_mat)) * identity\n",
    "    \n",
    "    # Update the value at index [0,0] to 1\n",
    "    # We use Tensorflow operations to ensure gradient computation\n",
    "    PTM_depol = tf.tensor_scatter_nd_update(PTM_depol, [[0,0]], [1])\n",
    "    PTM_depol = tf.cast(PTM_depol, dtype=tf.complex64)\n",
    "    return PTM_depol\n",
    "\n",
    "\n",
    "\n",
    "def pauli_matrices():\n",
    "    \"\"\"Return the Pauli matrices including identity.\"\"\"\n",
    "    I = tf.eye(2, dtype=tf.complex64)\n",
    "    X = tf.constant([[0, 1], [1, 0]], dtype=tf.complex64)\n",
    "    Y_imag = tf.constant([[0, -1], [1, 0]], dtype=tf.float32)\n",
    "    Y_real = tf.constant([[0, 0], [0, 0]], dtype=tf.float32)\n",
    "    Y = tf.complex(Y_real, Y_imag)\n",
    "    Z = tf.constant([[1, 0], [0, -1]], dtype=tf.complex64)\n",
    "    \n",
    "    return [I, X, Y, Z]\n",
    "\n",
    "def compute_ideal_ptm(unitary):\n",
    "    \"\"\"Compute the ideal PTM from a given unitary.\"\"\"\n",
    "    paulis = pauli_matrices()\n",
    "    ptm_ideal = tf.zeros((4, 4), dtype=tf.complex64)\n",
    "\n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            term = tf.matmul(unitary, tf.matmul(paulis[j], tf.linalg.adjoint(unitary)))\n",
    "            trace_value = 0.5 * tf.linalg.trace(tf.matmul(paulis[i], term))\n",
    "            \n",
    "            # Update ptm_ideal at position [i, j] with the calculated trace_value\n",
    "            indices = tf.constant([[i, j]])\n",
    "            ptm_ideal = tf.tensor_scatter_nd_add(ptm_ideal, indices, [trace_value])\n",
    "            \n",
    "    return ptm_ideal\n",
    "\n",
    "\n",
    "def general_custom_gate(theta, delta, depol_amt, gate):\n",
    "    # Compute real and imaginary parts as real numbers initially\n",
    "    real_part = tf.cos((theta + delta) / 2)\n",
    "    imag_part = tf.sin((theta + delta) / 2)\n",
    "    \n",
    "    # Cast them to complex numbers only when necessary\n",
    "    unitary_rx_adjusted = tf.cast(real_part, dtype=tf.complex64) * tf.eye(2, dtype=tf.complex64) - 1j * tf.cast(imag_part, dtype=tf.complex64) * pauli_matrices()[gate]\n",
    "    \n",
    "    ptm_adjusted_rx = compute_ideal_ptm(unitary_rx_adjusted)\n",
    "    ptm = tf.matmul(easy_PTM_depol_channel(depol_amt), ptm_adjusted_rx)\n",
    "    \n",
    "    return tf.math.real(ptm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def custom_X(depol_amt, over_rotation):\n",
    "    return general_custom_gate(theta=math.pi/2, delta=over_rotation, depol_amt=depol_amt, gate=1)\n",
    "\n",
    "def custom_Y(depol_amt, over_rotation):\n",
    "    return general_custom_gate(theta=math.pi/2, delta=over_rotation, depol_amt=depol_amt, gate=2)\n",
    "\n",
    "\n",
    "def custom_gate_set(depol_amt_X, over_rotation_X, depol_amt_Y, over_rotation_Y):\n",
    "    # Define gates in PTM form\n",
    "\n",
    "    I = tf.constant([[1, 0, 0, 0],\n",
    "                  [0, 1, 0, 0],\n",
    "                  [0, 0, 1, 0],\n",
    "                  [0, 0, 0, 1]], dtype=tf.float32)  #Gi\n",
    "\n",
    "    X_theta = custom_X(depol_amt_X, over_rotation_X)\n",
    "    Y_theta = custom_Y(depol_amt_Y, over_rotation_Y)\n",
    "    return [X_theta, Y_theta, I]\n",
    "  \n",
    "# Define gate application function\n",
    "def apply_gate(state, gate_set, label):   \n",
    "    X_theta = gate_set[0]\n",
    "    Y_theta = gate_set[1]\n",
    "    I = gate_set[2]\n",
    "      \n",
    "    if label == 1:\n",
    "        return tf.linalg.matmul(X_theta, tf.reshape(state, [-1, 1]))\n",
    "    elif label == 2:\n",
    "        return tf.linalg.matmul(Y_theta, tf.reshape(state, [-1, 1]))\n",
    "    elif label == 3:\n",
    "        return tf.linalg.matmul(I, tf.reshape(state, [-1, 1]))\n",
    "    else:\n",
    "        return state  # If label is 0, don't apply any gate  \n",
    "\n",
    "\n",
    "def apply_gate_sequence(grouped_gate_sequence, grouped_y_label, grouped_gate_matrix_sequence, grouped_model_output):\n",
    "    # Initialize a list to collect final states for each data point in the group\n",
    "    grouped_final_states = []\n",
    "       \n",
    "    # Iterate through each data point within the group\n",
    "    for i in range(tf.shape(grouped_gate_sequence)[0]):\n",
    "        # print(f\"current gate sequence number: {i}\")\n",
    "        single_gate_sequence = tf.gather(grouped_gate_sequence, i, axis=0)\n",
    "        \n",
    "        single_y_label = tf.gather(grouped_y_label, i, axis=0)\n",
    "        single_gate_matrix_sequence = tf.gather(grouped_gate_matrix_sequence, i, axis=0)\n",
    "        \n",
    "        # Initialize state in Pauli basis\n",
    "        state = tf.constant([1/math.sqrt(2), 0, 0, 1/math.sqrt(2)], dtype=tf.float32)\n",
    "        \n",
    "        # Extract the model output corresponding to this data point\n",
    "        depol_amt_X, depol_amt_Y, over_rotation_X, over_rotation_Y = grouped_model_output\n",
    "       \n",
    "        # Reconstruct the gate set using the predicted depolarization and over-rotation\n",
    "        reconstructed_gate_set = custom_gate_set(depol_amt_X, over_rotation_X, depol_amt_Y, over_rotation_Y)\n",
    "        \n",
    "        \n",
    "\n",
    "        # Apply each gate in the sequence\n",
    "        debug_label_arr = []\n",
    "        for j in range(tf.shape(single_gate_sequence)[0]):\n",
    "            debug_label_arr.append(single_gate_sequence[j])\n",
    "            if single_gate_sequence[j] == 0:\n",
    "                break\n",
    "            state = apply_gate(state, reconstructed_gate_set, single_gate_sequence[j])\n",
    "\n",
    "        # Append the final state for this data point to the list of final states\n",
    "        grouped_final_states.append(state)\n",
    "\n",
    "    # Convert the list of final states into a tensor\n",
    "    grouped_final_states = tf.stack(grouped_final_states)   \n",
    "    return grouped_final_states\n",
    "\n",
    "\n",
    "def compute_probabilities(grouped_ptm_vector):    \n",
    "    # Remove singleton dimensions if any (like the last '1' in [10, 4, 1])\n",
    "    grouped_ptm_vector = tf.squeeze(grouped_ptm_vector, axis=-1)    \n",
    "    ptm_0 = tf.constant([1/math.sqrt(2), 0, 0, 1/math.sqrt(2)], dtype=tf.float32)\n",
    "    ptm_1 = tf.constant([1/math.sqrt(2), 0, 0, -1/math.sqrt(2)], dtype=tf.float32)\n",
    "    \n",
    "    # Compute dot products in a batched manner using broadcasting\n",
    "    prob_0 = tf.reduce_sum(grouped_ptm_vector * ptm_0, axis=-1)\n",
    "    prob_1 = tf.reduce_sum(grouped_ptm_vector * ptm_1, axis=-1)\n",
    "\n",
    "    final_probabilities = tf.stack([prob_0, prob_1], axis=-1)\n",
    "    \n",
    "    return final_probabilities\n",
    "\n",
    "\n",
    "\n",
    "# Define loss function\n",
    "loss_fn = MeanSquaredError()\n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "def train_step(X, y, X_ms):\n",
    "    train_losses = []  # Initialize a list to collect the losses for each group in the mini-batch\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        # Compute model output for the entire mini-batch here\n",
    "        full_batch_model_output = model([X_ms[0], X[1]])\n",
    "        \n",
    "        # Loop over each group in the mini-batch\n",
    "        for i in range(tf.shape(X[0])[0]):\n",
    "            single_group_gate_sequence = tf.gather(X[0], i, axis=0)\n",
    "            single_group_y_label = tf.gather(X[1], i, axis=0)\n",
    "            single_group_gate_matrix_sequence = tf.gather(X_ms[0], i, axis=0)\n",
    "\n",
    "            # Gather the model's output for the i-th group from the full batch output\n",
    "            single_group_model_output = tf.gather(full_batch_model_output, i, axis=0)\n",
    "\n",
    "            # Process a single group\n",
    "            final_states = apply_gate_sequence(single_group_gate_sequence, single_group_y_label, single_group_gate_matrix_sequence, single_group_model_output)\n",
    "            probabilities = compute_probabilities(final_states)\n",
    "            \n",
    "            # Calculate loss for this group\n",
    "            loss = loss_fn(tf.gather(y, i, axis=0), probabilities)\n",
    "            train_losses.append(loss)\n",
    "            \n",
    "        # Average the losses across the mini-batch\n",
    "        average_loss = tf.reduce_mean(train_losses)\n",
    "\n",
    "    # Compute the gradients for the entire mini-batch and update model parameters\n",
    "    grads = tape.gradient(average_loss, model.trainable_weights)\n",
    "       \n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "    \n",
    "    return average_loss, tf.squeeze(full_batch_model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mUZNtlhbiL7n",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Directory to save the model and weights\n",
    "model_save_dir = \"saved_models\"\n",
    "if not os.path.exists(model_save_dir):\n",
    "    os.makedirs(model_save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_for_grouping(data, group_size):\n",
    "    \"\"\"\n",
    "    Reshape the data for grouping with repeating the last indices in case of remainder.\n",
    "    \n",
    "    Parameters:\n",
    "        data (numpy.ndarray): The data to be reshaped.\n",
    "        group_size (int): The desired group size.\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: The reshaped data.\n",
    "    \"\"\"\n",
    "    n_samples = data.shape[0]\n",
    "    n_batches = n_samples // group_size\n",
    "    remainder = n_samples % group_size\n",
    "    \n",
    "    # If the remainder exists, pad the data by repeating the last indices\n",
    "    if remainder:\n",
    "        padding_count = group_size - remainder\n",
    "        padding_indices = [-i % n_samples - 1 for i in range(padding_count)]\n",
    "        padding = data[padding_indices]\n",
    "        data = np.concatenate([data, padding], axis=0)\n",
    "        \n",
    "    return data.reshape(-1, group_size, *data.shape[1:])\n",
    "\n",
    "X_train_0_grouped = reshape_for_grouping(X_train_0, group_size)\n",
    "y_train_grouped = reshape_for_grouping(y_train, group_size)\n",
    "X_train_grouped = [reshape_for_grouping(X, group_size) for X in X_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Training loop\n",
    "EPOCHS = 100\n",
    "num_parts = 1 # num_parts = 1 means no curriculum learning\n",
    "part_size = len(X_train_0_grouped) // num_parts\n",
    "print(f\"Length of X_train_0_grouped: {len(X_train_0_grouped)}\")\n",
    "print(f\"part size: {part_size}\")\n",
    "\n",
    "# Lists to store the mean train and validation losses for each epoch\n",
    "all_train_losses = []\n",
    "all_train_predicted_values = []\n",
    "all_train_X_depolar = []\n",
    "all_train_Y_depolar = []\n",
    "all_train_predicted_X_overrotation = []\n",
    "all_train_predicted_Y_overrotation = []\n",
    "\n",
    "\n",
    "# List to store learning rates\n",
    "learning_rates = []\n",
    "\n",
    "total_epochs_elapsed = 0  # Counter for the total number of epochs elapsed\n",
    "\n",
    "end_idx = 0  # Initialize end_idx to zero\n",
    "\n",
    "\n",
    "for part in range(num_parts):\n",
    "    # Determine the dataset subset for the current part\n",
    "    start_idx = part * part_size\n",
    "    end_idx = (part + 1) * part_size  # Exclusive\n",
    "    \n",
    "    print(f\"start_idx: {start_idx}\")\n",
    "    print(f\"end_idx: {end_idx}\")\n",
    "    \n",
    "\n",
    "    X_subset = X_train_0_grouped[start_idx:end_idx]\n",
    "    print(f\"Shape of X_subset: {X_subset.shape}\")\n",
    "\n",
    "    y_subset = y_train_grouped[start_idx:end_idx]\n",
    "    print(f\"Shape of y_subset: {y_subset.shape}\")\n",
    "\n",
    "    X_subset_ms = [X_train_grouped[0][start_idx:end_idx], X_train_grouped[1][start_idx:end_idx]]\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_losses_per_epoch = []\n",
    "        predicted_values_per_epoch = np.zeros(4) # Initialize zeros array, number of zeros corresponds to model output dimension \n",
    "        predicted_values_counter_per_epoch = 0 \n",
    "\n",
    "        for i in range(len(X_subset)):\n",
    "            X_batch = [X_subset[i:i + 1], y_subset[i:i + 1]]\n",
    "            X_batch_ms = [X_subset_ms[0][i:i + 1], X_subset_ms[1][i:i + 1]]\n",
    "            y_batch = y_subset[i:i + 1]\n",
    "\n",
    "            train_loss, train_predicted_values = train_step(X_batch, y_batch, X_batch)\n",
    "                        \n",
    "            train_losses_per_epoch.append(train_loss)\n",
    "            predicted_values_per_epoch = predicted_values_per_epoch + np.array(train_predicted_values)\n",
    "            predicted_values_counter_per_epoch += 1\n",
    "                      \n",
    "\n",
    "        mean_train_loss = np.mean(train_losses_per_epoch)\n",
    "        mean_predicted_values = predicted_values_per_epoch/predicted_values_counter_per_epoch\n",
    "\n",
    "        # Store the mean losses for this epoch\n",
    "        all_train_losses.append(mean_train_loss)\n",
    "        all_train_predicted_values.append(mean_predicted_values)\n",
    "        \n",
    "        all_train_X_depolar.append(mean_predicted_values[0])\n",
    "        all_train_Y_depolar.append(mean_predicted_values[1])\n",
    "        all_train_predicted_X_overrotation.append(mean_predicted_values[2])\n",
    "        all_train_predicted_Y_overrotation.append(mean_predicted_values[3])\n",
    "            \n",
    "\n",
    "        # Record the learning rate at this epoch\n",
    "        current_step = model.optimizer.iterations\n",
    "        current_lr = lr_schedule(current_step).numpy()\n",
    "        learning_rates.append(current_lr)\n",
    "\n",
    "        total_epochs_elapsed += 1\n",
    "        print(f\"Part: {part+1}/{num_parts}, Epoch: {epoch+1}/{EPOCHS}, Total Epochs: {total_epochs_elapsed}, Train Loss: {mean_train_loss}, Learning Rate: {current_lr}\")\n",
    "        if (total_epochs_elapsed % 10 == 0) or (total_epochs_elapsed == 1) or (epoch+1 == EPOCHS):\n",
    "            model_path = os.path.join(model_save_dir, f\"model_epoch_{total_epochs_elapsed}\")\n",
    "            weights_path = os.path.join(model_save_dir, f\"weights_epoch_{total_epochs_elapsed}\")\n",
    "            if (total_epochs_elapsed % 100 == 0) or (epoch+1 == EPOCHS):\n",
    "                model.save(f\"{model_path}.keras\", save_format='keras')\n",
    "                print(f\"Model and weights saved at epoch {total_epochs_elapsed}\")          \n",
    "\n",
    "            loss_lr_df = pd.DataFrame({\n",
    "                'Epoch': list(range(1, len(all_train_losses) + 1)),\n",
    "                'Training_Loss': all_train_losses,\n",
    "                'Learning_Rate': learning_rates # Include learning rates\n",
    "            })\n",
    "            \n",
    "            predicted_values_df = pd.DataFrame({\n",
    "                'Epoch': list(range(1, len(all_train_predicted_values) + 1)),\n",
    "                'Predicted_Values': all_train_predicted_values,\n",
    "                'X_Depolarizing_Error': all_train_X_depolar,\n",
    "                'Y_Depolarizing_Error': all_train_Y_depolar,\n",
    "                'X_Over_Rotation_Error': all_train_predicted_X_overrotation,\n",
    "                'Y_Over_Rotation_Error':all_train_predicted_Y_overrotation\n",
    "            })\n",
    "            \n",
    "            # Save the DataFrame to a CSV file\n",
    "            loss_lr_df.to_csv(f'losses_and_lr_epoch_{total_epochs_elapsed}.csv', index=False)\n",
    "            predicted_values_df.to_csv(f'predicted_values_epoch_{total_epochs_elapsed}.csv', index=False)\n",
    "\n",
    "            print(f\"Losses and learning rates saved at epoch {total_epochs_elapsed}\")\n",
    "            \n",
    "\n",
    "        if (part+1) != num_parts:\n",
    "            if mean_train_loss <= 2.5e-5:\n",
    "                print(f\"mean_train_loss: {mean_train_loss} <= 2.5e-5, skip to next stage.\")\n",
    "                break\n",
    "                "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
